Not using distributed mode
[09:50:29.972656] job dir: /home/wsj/mae
[09:50:29.972705] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=8,
ema_alpha=0.999,
enable_ema=True,
enable_bootstrap=True,
ema_warmup_epochs=8,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae_warmup8',
log_dir='./output_dir/bmae_warmup8',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[09:50:30.503095] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[09:50:30.503250] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7b4729cfab90>
[09:50:31.009878] use bootstrapped MAE!
[09:50:31.009913] ema enabled!
[09:50:31.025836] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[09:50:31.025864] base lr: 5.00e-04
[09:50:31.025870] actual lr: 5.00e-04
[09:50:31.025875] accumulate grad iterations: 1
[09:50:31.025879] effective batch size: 256
[09:50:31.026935] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[09:50:31.027017] Start training for 200 epochs
[09:50:31.028024] log_dir: ./output_dir/bmae_warmup8
Not using distributed mode
[09:50:42.340160] job dir: /home/wsj/mae
[09:50:42.340204] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=16,
ema_alpha=0.999,
enable_ema=True,
enable_bootstrap=True,
ema_warmup_epochs=16,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae_warmup16',
log_dir='./output_dir/bmae_warmup16',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[09:50:42.866700] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[09:50:42.866846] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x70df837065d0>
[09:50:43.323577] use bootstrapped MAE!
[09:50:43.323604] ema enabled!
[09:50:43.340924] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[09:50:43.340956] base lr: 5.00e-04
[09:50:43.340963] actual lr: 5.00e-04
[09:50:43.340969] accumulate grad iterations: 1
[09:50:43.340975] effective batch size: 256
[09:50:43.342389] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[09:50:43.342474] Start training for 200 epochs
[09:50:43.343790] log_dir: ./output_dir/bmae_warmup16
Not using distributed mode
[09:50:48.764325] job dir: /home/wsj/mae
[09:50:48.764372] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=32,
ema_alpha=0.999,
enable_ema=True,
enable_bootstrap=True,
ema_warmup_epochs=32,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae_warmup32',
log_dir='./output_dir/bmae_warmup32',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[09:50:49.291837] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[09:50:49.291990] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x79cd67e33510>
[09:50:49.748953] use bootstrapped MAE!
[09:50:49.748982] ema enabled!
[09:50:49.762558] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[09:50:49.762588] base lr: 5.00e-04
[09:50:49.762594] actual lr: 5.00e-04
[09:50:49.762599] accumulate grad iterations: 1
[09:50:49.762603] effective batch size: 256
[09:50:49.763666] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[09:50:49.763746] Start training for 200 epochs
[09:50:49.764762] log_dir: ./output_dir/bmae_warmup32
[09:50:44.491564] Epoch: [0]  [  0/195]  eta: 0:03:43  lr: 0.000000  loss: 2.5151 (2.5151)  time: 1.1468  data: 0.4696  max mem: 2091
[09:50:45.897060] Epoch: [0]  [ 20/195]  eta: 0:00:21  lr: 0.000001  loss: 2.4594 (2.4379)  time: 0.0702  data: 0.0001  max mem: 2159
[09:50:47.317340] Epoch: [0]  [ 40/195]  eta: 0:00:15  lr: 0.000003  loss: 1.9374 (2.2089)  time: 0.0710  data: 0.0001  max mem: 2159
[09:50:48.889691] Epoch: [0]  [ 60/195]  eta: 0:00:12  lr: 0.000004  loss: 1.4184 (1.9575)  time: 0.0786  data: 0.0002  max mem: 2159
[09:50:50.532526] Epoch: [0]  [ 80/195]  eta: 0:00:10  lr: 0.000005  loss: 1.1293 (1.7559)  time: 0.0821  data: 0.0008  max mem: 2159
[09:50:51.941695] Epoch: [0]  [100/195]  eta: 0:00:08  lr: 0.000006  loss: 0.9976 (1.6066)  time: 0.0704  data: 0.0002  max mem: 2159
[09:50:53.411625] Epoch: [0]  [120/195]  eta: 0:00:06  lr: 0.000008  loss: 0.9240 (1.4949)  time: 0.0734  data: 0.0002  max mem: 2159
[09:50:54.880874] Epoch: [0]  [140/195]  eta: 0:00:04  lr: 0.000009  loss: 0.8550 (1.4052)  time: 0.0734  data: 0.0002  max mem: 2159
[09:50:56.375293] Epoch: [0]  [160/195]  eta: 0:00:02  lr: 0.000010  loss: 0.8223 (1.3331)  time: 0.0747  data: 0.0003  max mem: 2159
[09:50:57.829465] Epoch: [0]  [180/195]  eta: 0:00:01  lr: 0.000012  loss: 0.7954 (1.2738)  time: 0.0727  data: 0.0002  max mem: 2159
[09:50:58.781458] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.000012  loss: 0.7869 (1.2387)  time: 0.0680  data: 0.0001  max mem: 2159
[09:50:58.832370] Epoch: [0] Total time: 0:00:15 (0.0794 s / it)
[09:50:58.832450] Averaged stats: lr: 0.000012  loss: 0.7869 (1.2387)
[09:50:58.995112] log_dir: ./output_dir/bmae_warmup16
[09:50:51.092802] Epoch: [0]  [  0/195]  eta: 0:04:18  lr: 0.000000  loss: 2.5151 (2.5151)  time: 1.3268  data: 0.4621  max mem: 2091
[09:50:52.584245] Epoch: [0]  [ 20/195]  eta: 0:00:23  lr: 0.000001  loss: 2.4594 (2.4379)  time: 0.0745  data: 0.0002  max mem: 2159
[09:50:54.106790] Epoch: [0]  [ 40/195]  eta: 0:00:16  lr: 0.000003  loss: 1.9374 (2.2089)  time: 0.0761  data: 0.0002  max mem: 2159
[09:50:55.626508] Epoch: [0]  [ 60/195]  eta: 0:00:12  lr: 0.000004  loss: 1.4184 (1.9575)  time: 0.0759  data: 0.0002  max mem: 2159
[09:50:57.140502] Epoch: [0]  [ 80/195]  eta: 0:00:10  lr: 0.000005  loss: 1.1293 (1.7559)  time: 0.0757  data: 0.0001  max mem: 2159
[09:50:58.653038] Epoch: [0]  [100/195]  eta: 0:00:08  lr: 0.000006  loss: 0.9976 (1.6066)  time: 0.0756  data: 0.0002  max mem: 2159
[09:51:00.091578] Epoch: [0]  [120/195]  eta: 0:00:06  lr: 0.000008  loss: 0.9240 (1.4949)  time: 0.0719  data: 0.0002  max mem: 2159
[09:51:01.519097] Epoch: [0]  [140/195]  eta: 0:00:04  lr: 0.000009  loss: 0.8550 (1.4052)  time: 0.0713  data: 0.0002  max mem: 2159
[09:51:02.927704] Epoch: [0]  [160/195]  eta: 0:00:02  lr: 0.000010  loss: 0.8223 (1.3331)  time: 0.0704  data: 0.0002  max mem: 2159
[09:51:04.307362] Epoch: [0]  [180/195]  eta: 0:00:01  lr: 0.000012  loss: 0.7954 (1.2738)  time: 0.0689  data: 0.0002  max mem: 2159
[09:51:05.250345] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.000012  loss: 0.7869 (1.2387)  time: 0.0672  data: 0.0001  max mem: 2159
[09:51:05.307178] Epoch: [0] Total time: 0:00:15 (0.0797 s / it)
[09:51:05.307271] Averaged stats: lr: 0.000012  loss: 0.7869 (1.2387)
[09:51:05.485524] log_dir: ./output_dir/bmae_warmup32
Not using distributed mode
[09:51:10.145665] job dir: /home/wsj/mae
[09:51:10.145710] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=64,
ema_alpha=0.999,
enable_ema=True,
enable_bootstrap=True,
ema_warmup_epochs=64,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae_warmup64',
log_dir='./output_dir/bmae_warmup64',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[09:51:10.685232] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[09:51:10.685411] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x71ed688f0d50>
[09:51:12.203771] use bootstrapped MAE!
[09:51:12.203807] ema enabled!
[09:51:12.221292] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[09:51:12.221332] base lr: 5.00e-04
[09:51:12.221341] actual lr: 5.00e-04
[09:51:12.221347] accumulate grad iterations: 1
[09:51:12.221353] effective batch size: 256
[09:51:12.234497] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[09:51:12.234775] Start training for 200 epochs
[09:51:12.237079] log_dir: ./output_dir/bmae_warmup64
[09:50:59.516311] Epoch: [1]  [  0/195]  eta: 0:01:41  lr: 0.000013  loss: 0.8015 (0.8015)  time: 0.5202  data: 0.4339  max mem: 2159
[09:51:00.919087] Epoch: [1]  [ 20/195]  eta: 0:00:16  lr: 0.000014  loss: 0.7691 (0.7724)  time: 0.0701  data: 0.0001  max mem: 2159
[09:51:02.324953] Epoch: [1]  [ 40/195]  eta: 0:00:12  lr: 0.000015  loss: 0.7630 (0.7685)  time: 0.0702  data: 0.0001  max mem: 2159
[09:51:03.747508] Epoch: [1]  [ 60/195]  eta: 0:00:10  lr: 0.000016  loss: 0.7609 (0.7666)  time: 0.0711  data: 0.0001  max mem: 2159
[09:51:05.122041] Epoch: [1]  [ 80/195]  eta: 0:00:08  lr: 0.000018  loss: 0.7709 (0.7666)  time: 0.0687  data: 0.0001  max mem: 2159
[09:51:06.497060] Epoch: [1]  [100/195]  eta: 0:00:07  lr: 0.000019  loss: 0.7641 (0.7657)  time: 0.0687  data: 0.0002  max mem: 2159
[09:51:07.900881] Epoch: [1]  [120/195]  eta: 0:00:05  lr: 0.000020  loss: 0.7579 (0.7643)  time: 0.0702  data: 0.0002  max mem: 2159
[09:51:09.304919] Epoch: [1]  [140/195]  eta: 0:00:04  lr: 0.000021  loss: 0.7526 (0.7636)  time: 0.0702  data: 0.0002  max mem: 2159
[09:51:10.703106] Epoch: [1]  [160/195]  eta: 0:00:02  lr: 0.000023  loss: 0.7600 (0.7630)  time: 0.0699  data: 0.0002  max mem: 2159
[09:51:12.087313] Epoch: [1]  [180/195]  eta: 0:00:01  lr: 0.000024  loss: 0.7463 (0.7619)  time: 0.0692  data: 0.0002  max mem: 2159
[09:51:13.097718] Epoch: [1]  [194/195]  eta: 0:00:00  lr: 0.000025  loss: 0.7478 (0.7612)  time: 0.0710  data: 0.0001  max mem: 2159
[09:51:13.171485] Epoch: [1] Total time: 0:00:14 (0.0727 s / it)
[09:51:13.171597] Averaged stats: lr: 0.000025  loss: 0.7478 (0.7612)
[09:51:13.176631] log_dir: ./output_dir/bmae_warmup16
[09:51:05.974735] Epoch: [1]  [  0/195]  eta: 0:01:35  lr: 0.000013  loss: 0.8015 (0.8015)  time: 0.4879  data: 0.3995  max mem: 2159
[09:51:07.560100] Epoch: [1]  [ 20/195]  eta: 0:00:17  lr: 0.000014  loss: 0.7691 (0.7724)  time: 0.0792  data: 0.0002  max mem: 2159
[09:51:09.050098] Epoch: [1]  [ 40/195]  eta: 0:00:13  lr: 0.000015  loss: 0.7630 (0.7685)  time: 0.0745  data: 0.0001  max mem: 2159
[09:51:10.503900] Epoch: [1]  [ 60/195]  eta: 0:00:11  lr: 0.000016  loss: 0.7609 (0.7666)  time: 0.0726  data: 0.0001  max mem: 2159
[09:51:11.891342] Epoch: [1]  [ 80/195]  eta: 0:00:09  lr: 0.000018  loss: 0.7709 (0.7666)  time: 0.0693  data: 0.0002  max mem: 2159
[09:51:13.404482] Epoch: [1]  [100/195]  eta: 0:00:07  lr: 0.000019  loss: 0.7641 (0.7657)  time: 0.0756  data: 0.0008  max mem: 2159
[09:51:14.829628] Epoch: [1]  [120/195]  eta: 0:00:05  lr: 0.000020  loss: 0.7579 (0.7643)  time: 0.0712  data: 0.0002  max mem: 2159
[09:51:16.242356] Epoch: [1]  [140/195]  eta: 0:00:04  lr: 0.000021  loss: 0.7526 (0.7636)  time: 0.0706  data: 0.0001  max mem: 2159
[09:51:17.639737] Epoch: [1]  [160/195]  eta: 0:00:02  lr: 0.000023  loss: 0.7600 (0.7630)  time: 0.0698  data: 0.0001  max mem: 2159
[09:51:19.095974] Epoch: [1]  [180/195]  eta: 0:00:01  lr: 0.000024  loss: 0.7463 (0.7619)  time: 0.0728  data: 0.0001  max mem: 2159
[09:51:20.032829] Epoch: [1]  [194/195]  eta: 0:00:00  lr: 0.000025  loss: 0.7478 (0.7612)  time: 0.0669  data: 0.0001  max mem: 2159
[09:51:20.091709] Epoch: [1] Total time: 0:00:14 (0.0749 s / it)
[09:51:20.091781] Averaged stats: lr: 0.000025  loss: 0.7478 (0.7612)
[09:51:20.094402] log_dir: ./output_dir/bmae_warmup32
[09:50:32.663228] Epoch: [0]  [  0/195]  eta: 0:05:18  lr: 0.000000  loss: 2.5151 (2.5151)  time: 1.6338  data: 0.4811  max mem: 2091
[09:50:38.242892] Epoch: [0]  [ 20/195]  eta: 0:01:00  lr: 0.000001  loss: 2.4594 (2.4379)  time: 0.2789  data: 0.0002  max mem: 2159
[09:50:43.793405] Epoch: [0]  [ 40/195]  eta: 0:00:48  lr: 0.000003  loss: 1.9374 (2.2089)  time: 0.2775  data: 0.0001  max mem: 2159
[09:50:49.345502] Epoch: [0]  [ 60/195]  eta: 0:00:40  lr: 0.000004  loss: 1.4184 (1.9575)  time: 0.2776  data: 0.0001  max mem: 2159
[09:50:54.923095] Epoch: [0]  [ 80/195]  eta: 0:00:33  lr: 0.000005  loss: 1.1293 (1.7559)  time: 0.2788  data: 0.0002  max mem: 2159
[09:51:00.472852] Epoch: [0]  [100/195]  eta: 0:00:27  lr: 0.000006  loss: 0.9976 (1.6066)  time: 0.2774  data: 0.0002  max mem: 2159
[09:51:06.020903] Epoch: [0]  [120/195]  eta: 0:00:21  lr: 0.000008  loss: 0.9240 (1.4949)  time: 0.2774  data: 0.0001  max mem: 2159
[09:51:11.566438] Epoch: [0]  [140/195]  eta: 0:00:15  lr: 0.000009  loss: 0.8550 (1.4052)  time: 0.2772  data: 0.0002  max mem: 2159
[09:51:17.183082] Epoch: [0]  [160/195]  eta: 0:00:10  lr: 0.000010  loss: 0.8223 (1.3331)  time: 0.2808  data: 0.0002  max mem: 2159
[09:51:22.755037] Epoch: [0]  [180/195]  eta: 0:00:04  lr: 0.000012  loss: 0.7954 (1.2738)  time: 0.2786  data: 0.0002  max mem: 2159
[09:51:26.637217] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.000012  loss: 0.7869 (1.2387)  time: 0.2774  data: 0.0001  max mem: 2159
[09:51:26.691412] Epoch: [0] Total time: 0:00:55 (0.2855 s / it)
[09:51:26.691488] Averaged stats: lr: 0.000012  loss: 0.7869 (1.2387)
[09:51:26.851947] log_dir: ./output_dir/bmae_warmup8
[09:51:13.713805] Epoch: [2]  [  0/195]  eta: 0:01:44  lr: 0.000025  loss: 0.7753 (0.7753)  time: 0.5362  data: 0.4566  max mem: 2159
[09:51:15.197229] Epoch: [2]  [ 20/195]  eta: 0:00:16  lr: 0.000026  loss: 0.7487 (0.7510)  time: 0.0741  data: 0.0002  max mem: 2159
[09:51:16.682685] Epoch: [2]  [ 40/195]  eta: 0:00:13  lr: 0.000028  loss: 0.7403 (0.7472)  time: 0.0742  data: 0.0002  max mem: 2159
[09:51:18.093688] Epoch: [2]  [ 60/195]  eta: 0:00:10  lr: 0.000029  loss: 0.7464 (0.7477)  time: 0.0705  data: 0.0002  max mem: 2159
[09:51:19.584369] Epoch: [2]  [ 80/195]  eta: 0:00:09  lr: 0.000030  loss: 0.7519 (0.7483)  time: 0.0745  data: 0.0002  max mem: 2159
[09:51:21.141625] Epoch: [2]  [100/195]  eta: 0:00:07  lr: 0.000031  loss: 0.7501 (0.7479)  time: 0.0778  data: 0.0002  max mem: 2159
[09:51:22.619281] Epoch: [2]  [120/195]  eta: 0:00:05  lr: 0.000033  loss: 0.7447 (0.7472)  time: 0.0738  data: 0.0002  max mem: 2159
[09:51:23.991199] Epoch: [2]  [140/195]  eta: 0:00:04  lr: 0.000034  loss: 0.7401 (0.7463)  time: 0.0686  data: 0.0002  max mem: 2159
[09:51:25.359102] Epoch: [2]  [160/195]  eta: 0:00:02  lr: 0.000035  loss: 0.7350 (0.7451)  time: 0.0684  data: 0.0002  max mem: 2159
[09:51:26.722565] Epoch: [2]  [180/195]  eta: 0:00:01  lr: 0.000037  loss: 0.7238 (0.7431)  time: 0.0681  data: 0.0002  max mem: 2159
[09:51:27.723088] Epoch: [2]  [194/195]  eta: 0:00:00  lr: 0.000037  loss: 0.7280 (0.7421)  time: 0.0708  data: 0.0001  max mem: 2159
[09:51:27.796218] Epoch: [2] Total time: 0:00:14 (0.0750 s / it)
[09:51:27.796336] Averaged stats: lr: 0.000037  loss: 0.7280 (0.7421)
[09:51:27.800626] log_dir: ./output_dir/bmae_warmup16
[09:51:13.479703] Epoch: [0]  [  0/195]  eta: 0:04:02  lr: 0.000000  loss: 2.5151 (2.5151)  time: 1.2413  data: 0.4859  max mem: 2091
[09:51:15.046243] Epoch: [0]  [ 20/195]  eta: 0:00:23  lr: 0.000001  loss: 2.4594 (2.4379)  time: 0.0783  data: 0.0002  max mem: 2159
[09:51:16.503593] Epoch: [0]  [ 40/195]  eta: 0:00:16  lr: 0.000003  loss: 1.9374 (2.2089)  time: 0.0728  data: 0.0002  max mem: 2159
[09:51:18.009240] Epoch: [0]  [ 60/195]  eta: 0:00:12  lr: 0.000004  loss: 1.4184 (1.9575)  time: 0.0752  data: 0.0002  max mem: 2159
[09:51:19.610284] Epoch: [0]  [ 80/195]  eta: 0:00:10  lr: 0.000005  loss: 1.1293 (1.7559)  time: 0.0800  data: 0.0002  max mem: 2159
[09:51:21.198069] Epoch: [0]  [100/195]  eta: 0:00:08  lr: 0.000006  loss: 0.9976 (1.6066)  time: 0.0793  data: 0.0002  max mem: 2159
[09:51:22.703342] Epoch: [0]  [120/195]  eta: 0:00:06  lr: 0.000008  loss: 0.9240 (1.4949)  time: 0.0752  data: 0.0003  max mem: 2159
[09:51:24.234242] Epoch: [0]  [140/195]  eta: 0:00:04  lr: 0.000009  loss: 0.8550 (1.4052)  time: 0.0765  data: 0.0003  max mem: 2159
[09:51:25.707005] Epoch: [0]  [160/195]  eta: 0:00:02  lr: 0.000010  loss: 0.8223 (1.3331)  time: 0.0736  data: 0.0002  max mem: 2159
[09:51:27.193989] Epoch: [0]  [180/195]  eta: 0:00:01  lr: 0.000012  loss: 0.7954 (1.2738)  time: 0.0743  data: 0.0002  max mem: 2159
[09:51:28.293626] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.000012  loss: 0.7869 (1.2387)  time: 0.0771  data: 0.0001  max mem: 2159
[09:51:28.342881] Epoch: [0] Total time: 0:00:16 (0.0826 s / it)
[09:51:28.342958] Averaged stats: lr: 0.000012  loss: 0.7869 (1.2387)
[09:51:28.537658] log_dir: ./output_dir/bmae_warmup64
[09:51:20.634353] Epoch: [2]  [  0/195]  eta: 0:01:44  lr: 0.000025  loss: 0.7753 (0.7753)  time: 0.5382  data: 0.4268  max mem: 2159
[09:51:22.159256] Epoch: [2]  [ 20/195]  eta: 0:00:17  lr: 0.000026  loss: 0.7487 (0.7510)  time: 0.0762  data: 0.0002  max mem: 2159
[09:51:23.669638] Epoch: [2]  [ 40/195]  eta: 0:00:13  lr: 0.000028  loss: 0.7403 (0.7472)  time: 0.0755  data: 0.0002  max mem: 2159
[09:51:25.167186] Epoch: [2]  [ 60/195]  eta: 0:00:11  lr: 0.000029  loss: 0.7464 (0.7477)  time: 0.0748  data: 0.0002  max mem: 2159
[09:51:26.687323] Epoch: [2]  [ 80/195]  eta: 0:00:09  lr: 0.000030  loss: 0.7519 (0.7483)  time: 0.0760  data: 0.0002  max mem: 2159
[09:51:28.191104] Epoch: [2]  [100/195]  eta: 0:00:07  lr: 0.000031  loss: 0.7501 (0.7479)  time: 0.0751  data: 0.0002  max mem: 2159
[09:51:29.690261] Epoch: [2]  [120/195]  eta: 0:00:05  lr: 0.000033  loss: 0.7447 (0.7472)  time: 0.0749  data: 0.0002  max mem: 2159
[09:51:31.181907] Epoch: [2]  [140/195]  eta: 0:00:04  lr: 0.000034  loss: 0.7401 (0.7463)  time: 0.0745  data: 0.0002  max mem: 2159
[09:51:32.736921] Epoch: [2]  [160/195]  eta: 0:00:02  lr: 0.000035  loss: 0.7350 (0.7451)  time: 0.0777  data: 0.0002  max mem: 2159
[09:51:34.219068] Epoch: [2]  [180/195]  eta: 0:00:01  lr: 0.000037  loss: 0.7238 (0.7431)  time: 0.0741  data: 0.0002  max mem: 2159
[09:51:35.175910] Epoch: [2]  [194/195]  eta: 0:00:00  lr: 0.000037  loss: 0.7280 (0.7421)  time: 0.0699  data: 0.0001  max mem: 2159
[09:51:35.254647] Epoch: [2] Total time: 0:00:15 (0.0777 s / it)
[09:51:35.254828] Averaged stats: lr: 0.000037  loss: 0.7280 (0.7421)
[09:51:35.257217] log_dir: ./output_dir/bmae_warmup32
[09:51:28.332092] Epoch: [3]  [  0/195]  eta: 0:01:43  lr: 0.000038  loss: 0.7568 (0.7568)  time: 0.5305  data: 0.4488  max mem: 2159
[09:51:29.754358] Epoch: [3]  [ 20/195]  eta: 0:00:16  lr: 0.000039  loss: 0.7184 (0.7239)  time: 0.0711  data: 0.0001  max mem: 2159
[09:51:31.143395] Epoch: [3]  [ 40/195]  eta: 0:00:12  lr: 0.000040  loss: 0.7138 (0.7214)  time: 0.0694  data: 0.0001  max mem: 2159
[09:51:32.610707] Epoch: [3]  [ 60/195]  eta: 0:00:10  lr: 0.000041  loss: 0.7225 (0.7220)  time: 0.0733  data: 0.0002  max mem: 2159
[09:51:34.079921] Epoch: [3]  [ 80/195]  eta: 0:00:08  lr: 0.000043  loss: 0.7221 (0.7222)  time: 0.0734  data: 0.0002  max mem: 2159
[09:51:35.486302] Epoch: [3]  [100/195]  eta: 0:00:07  lr: 0.000044  loss: 0.7228 (0.7218)  time: 0.0703  data: 0.0002  max mem: 2159
[09:51:36.908373] Epoch: [3]  [120/195]  eta: 0:00:05  lr: 0.000045  loss: 0.7192 (0.7215)  time: 0.0711  data: 0.0002  max mem: 2159
[09:51:38.314489] Epoch: [3]  [140/195]  eta: 0:00:04  lr: 0.000046  loss: 0.7177 (0.7210)  time: 0.0703  data: 0.0001  max mem: 2159
[09:51:39.712906] Epoch: [3]  [160/195]  eta: 0:00:02  lr: 0.000048  loss: 0.7207 (0.7206)  time: 0.0699  data: 0.0001  max mem: 2159
[09:51:41.095641] Epoch: [3]  [180/195]  eta: 0:00:01  lr: 0.000049  loss: 0.7015 (0.7189)  time: 0.0691  data: 0.0002  max mem: 2159
[09:51:42.005035] Epoch: [3]  [194/195]  eta: 0:00:00  lr: 0.000050  loss: 0.6999 (0.7178)  time: 0.0654  data: 0.0001  max mem: 2159
[09:51:42.063760] Epoch: [3] Total time: 0:00:14 (0.0731 s / it)
[09:51:42.063841] Averaged stats: lr: 0.000050  loss: 0.6999 (0.7178)
[09:51:42.066532] log_dir: ./output_dir/bmae_warmup16
[09:51:29.032438] Epoch: [1]  [  0/195]  eta: 0:01:36  lr: 0.000013  loss: 0.8015 (0.8015)  time: 0.4936  data: 0.4077  max mem: 2159
[09:51:30.523992] Epoch: [1]  [ 20/195]  eta: 0:00:16  lr: 0.000014  loss: 0.7691 (0.7724)  time: 0.0745  data: 0.0002  max mem: 2159
[09:51:31.996601] Epoch: [1]  [ 40/195]  eta: 0:00:13  lr: 0.000015  loss: 0.7630 (0.7685)  time: 0.0736  data: 0.0002  max mem: 2159
[09:51:33.501772] Epoch: [1]  [ 60/195]  eta: 0:00:10  lr: 0.000016  loss: 0.7609 (0.7666)  time: 0.0752  data: 0.0002  max mem: 2159
[09:51:35.085396] Epoch: [1]  [ 80/195]  eta: 0:00:09  lr: 0.000018  loss: 0.7709 (0.7666)  time: 0.0791  data: 0.0002  max mem: 2159
[09:51:36.536592] Epoch: [1]  [100/195]  eta: 0:00:07  lr: 0.000019  loss: 0.7641 (0.7657)  time: 0.0725  data: 0.0002  max mem: 2159
[09:51:37.910818] Epoch: [1]  [120/195]  eta: 0:00:05  lr: 0.000020  loss: 0.7579 (0.7643)  time: 0.0687  data: 0.0002  max mem: 2159
[09:51:39.258876] Epoch: [1]  [140/195]  eta: 0:00:04  lr: 0.000021  loss: 0.7526 (0.7636)  time: 0.0674  data: 0.0002  max mem: 2159
[09:51:40.593847] Epoch: [1]  [160/195]  eta: 0:00:02  lr: 0.000023  loss: 0.7600 (0.7630)  time: 0.0667  data: 0.0002  max mem: 2159
[09:51:41.899172] Epoch: [1]  [180/195]  eta: 0:00:01  lr: 0.000024  loss: 0.7463 (0.7619)  time: 0.0652  data: 0.0002  max mem: 2159
[09:51:42.835672] Epoch: [1]  [194/195]  eta: 0:00:00  lr: 0.000025  loss: 0.7478 (0.7612)  time: 0.0664  data: 0.0001  max mem: 2159
[09:51:42.912713] Epoch: [1] Total time: 0:00:14 (0.0737 s / it)
[09:51:42.912787] Averaged stats: lr: 0.000025  loss: 0.7478 (0.7612)
[09:51:42.915190] log_dir: ./output_dir/bmae_warmup64
[09:51:35.721758] Epoch: [3]  [  0/195]  eta: 0:01:30  lr: 0.000038  loss: 0.7568 (0.7568)  time: 0.4630  data: 0.3724  max mem: 2159
[09:51:37.232964] Epoch: [3]  [ 20/195]  eta: 0:00:16  lr: 0.000039  loss: 0.7184 (0.7239)  time: 0.0755  data: 0.0002  max mem: 2159
[09:51:38.740901] Epoch: [3]  [ 40/195]  eta: 0:00:13  lr: 0.000040  loss: 0.7138 (0.7214)  time: 0.0754  data: 0.0001  max mem: 2159
[09:51:40.140115] Epoch: [3]  [ 60/195]  eta: 0:00:10  lr: 0.000041  loss: 0.7225 (0.7220)  time: 0.0699  data: 0.0001  max mem: 2159
[09:51:41.537166] Epoch: [3]  [ 80/195]  eta: 0:00:08  lr: 0.000043  loss: 0.7221 (0.7222)  time: 0.0698  data: 0.0001  max mem: 2159
[09:51:43.015507] Epoch: [3]  [100/195]  eta: 0:00:07  lr: 0.000044  loss: 0.7228 (0.7218)  time: 0.0739  data: 0.0002  max mem: 2159
[09:51:44.462215] Epoch: [3]  [120/195]  eta: 0:00:05  lr: 0.000045  loss: 0.7192 (0.7215)  time: 0.0723  data: 0.0002  max mem: 2159
[09:51:45.848770] Epoch: [3]  [140/195]  eta: 0:00:04  lr: 0.000046  loss: 0.7177 (0.7210)  time: 0.0693  data: 0.0001  max mem: 2159
[09:51:47.320316] Epoch: [3]  [160/195]  eta: 0:00:02  lr: 0.000048  loss: 0.7207 (0.7206)  time: 0.0735  data: 0.0001  max mem: 2159
[09:51:48.800695] Epoch: [3]  [180/195]  eta: 0:00:01  lr: 0.000049  loss: 0.7015 (0.7189)  time: 0.0740  data: 0.0002  max mem: 2159
[09:51:49.810608] Epoch: [3]  [194/195]  eta: 0:00:00  lr: 0.000050  loss: 0.6999 (0.7178)  time: 0.0720  data: 0.0001  max mem: 2159
[09:51:49.884729] Epoch: [3] Total time: 0:00:14 (0.0750 s / it)
[09:51:49.884835] Averaged stats: lr: 0.000050  loss: 0.6999 (0.7178)
[09:51:49.889059] log_dir: ./output_dir/bmae_warmup32
[09:51:42.581790] Epoch: [4]  [  0/195]  eta: 0:01:40  lr: 0.000050  loss: 0.7166 (0.7166)  time: 0.5141  data: 0.4208  max mem: 2159
[09:51:43.961097] Epoch: [4]  [ 20/195]  eta: 0:00:15  lr: 0.000051  loss: 0.6953 (0.6972)  time: 0.0689  data: 0.0002  max mem: 2159
[09:51:45.388596] Epoch: [4]  [ 40/195]  eta: 0:00:12  lr: 0.000053  loss: 0.6858 (0.6930)  time: 0.0713  data: 0.0002  max mem: 2159
[09:51:46.818415] Epoch: [4]  [ 60/195]  eta: 0:00:10  lr: 0.000054  loss: 0.6907 (0.6930)  time: 0.0714  data: 0.0002  max mem: 2159
[09:51:48.222567] Epoch: [4]  [ 80/195]  eta: 0:00:08  lr: 0.000055  loss: 0.6941 (0.6934)  time: 0.0702  data: 0.0002  max mem: 2159
[09:51:49.593157] Epoch: [4]  [100/195]  eta: 0:00:07  lr: 0.000056  loss: 0.6905 (0.6931)  time: 0.0685  data: 0.0001  max mem: 2159
[09:51:51.085735] Epoch: [4]  [120/195]  eta: 0:00:05  lr: 0.000058  loss: 0.6939 (0.6928)  time: 0.0746  data: 0.0002  max mem: 2159
[09:51:52.640937] Epoch: [4]  [140/195]  eta: 0:00:04  lr: 0.000059  loss: 0.6858 (0.6924)  time: 0.0777  data: 0.0002  max mem: 2159
[09:51:54.115789] Epoch: [4]  [160/195]  eta: 0:00:02  lr: 0.000060  loss: 0.6950 (0.6924)  time: 0.0737  data: 0.0002  max mem: 2159
[09:51:55.595071] Epoch: [4]  [180/195]  eta: 0:00:01  lr: 0.000062  loss: 0.6847 (0.6916)  time: 0.0739  data: 0.0002  max mem: 2159
[09:51:56.634996] Epoch: [4]  [194/195]  eta: 0:00:00  lr: 0.000062  loss: 0.6847 (0.6911)  time: 0.0734  data: 0.0001  max mem: 2159
[09:51:56.689394] Epoch: [4] Total time: 0:00:14 (0.0750 s / it)
[09:51:56.689488] Averaged stats: lr: 0.000062  loss: 0.6847 (0.6911)
[09:51:56.692555] log_dir: ./output_dir/bmae_warmup16
[09:51:43.395556] Epoch: [2]  [  0/195]  eta: 0:01:33  lr: 0.000025  loss: 0.7753 (0.7753)  time: 0.4791  data: 0.3870  max mem: 2159
[09:51:44.844416] Epoch: [2]  [ 20/195]  eta: 0:00:16  lr: 0.000026  loss: 0.7487 (0.7510)  time: 0.0724  data: 0.0002  max mem: 2159
[09:51:46.308144] Epoch: [2]  [ 40/195]  eta: 0:00:12  lr: 0.000028  loss: 0.7403 (0.7472)  time: 0.0731  data: 0.0002  max mem: 2159
[09:51:47.776442] Epoch: [2]  [ 60/195]  eta: 0:00:10  lr: 0.000029  loss: 0.7464 (0.7477)  time: 0.0734  data: 0.0002  max mem: 2159
[09:51:49.314741] Epoch: [2]  [ 80/195]  eta: 0:00:09  lr: 0.000030  loss: 0.7519 (0.7483)  time: 0.0769  data: 0.0002  max mem: 2159
[09:51:50.853024] Epoch: [2]  [100/195]  eta: 0:00:07  lr: 0.000031  loss: 0.7501 (0.7479)  time: 0.0769  data: 0.0002  max mem: 2159
[09:51:52.326461] Epoch: [2]  [120/195]  eta: 0:00:05  lr: 0.000033  loss: 0.7447 (0.7472)  time: 0.0736  data: 0.0002  max mem: 2159
[09:51:53.785583] Epoch: [2]  [140/195]  eta: 0:00:04  lr: 0.000034  loss: 0.7401 (0.7463)  time: 0.0729  data: 0.0002  max mem: 2159
[09:51:55.261734] Epoch: [2]  [160/195]  eta: 0:00:02  lr: 0.000035  loss: 0.7350 (0.7451)  time: 0.0738  data: 0.0002  max mem: 2159
[09:51:56.736129] Epoch: [2]  [180/195]  eta: 0:00:01  lr: 0.000037  loss: 0.7238 (0.7431)  time: 0.0737  data: 0.0002  max mem: 2159
[09:51:57.728937] Epoch: [2]  [194/195]  eta: 0:00:00  lr: 0.000037  loss: 0.7280 (0.7421)  time: 0.0717  data: 0.0001  max mem: 2159
[09:51:57.803890] Epoch: [2] Total time: 0:00:14 (0.0764 s / it)
[09:51:57.803970] Averaged stats: lr: 0.000037  loss: 0.7280 (0.7421)
[09:51:57.806617] log_dir: ./output_dir/bmae_warmup64

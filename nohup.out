Traceback (most recent call last):
  File "/home/wsj/mae/main_pretrain.py", line 22, in <module>
    import torchvision.transforms as transforms
  File "/home/wsj/anaconda3/envs/pagedATTN/lib/python3.11/site-packages/torchvision/__init__.py", line 6, in <module>
    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils
  File "/home/wsj/anaconda3/envs/pagedATTN/lib/python3.11/site-packages/torchvision/models/__init__.py", line 2, in <module>
    from .convnext import *
  File "/home/wsj/anaconda3/envs/pagedATTN/lib/python3.11/site-packages/torchvision/models/convnext.py", line 8, in <module>
    from ..ops.misc import Conv2dNormActivation, Permute
  File "/home/wsj/anaconda3/envs/pagedATTN/lib/python3.11/site-packages/torchvision/ops/__init__.py", line 1, in <module>
    from ._register_onnx_ops import _register_custom_op
  File "/home/wsj/anaconda3/envs/pagedATTN/lib/python3.11/site-packages/torchvision/ops/_register_onnx_ops.py", line 5, in <module>
    from torch.onnx import symbolic_opset11 as opset11
  File "/home/wsj/anaconda3/envs/pagedATTN/lib/python3.11/site-packages/torch/onnx/__init__.py", line 46, in <module>
    from ._internal.exporter import (  # usort:skip. needs to be last to avoid circular import
  File "/home/wsj/anaconda3/envs/pagedATTN/lib/python3.11/site-packages/torch/onnx/_internal/exporter.py", line 44, in <module>
    from torch.onnx._internal.fx import (
  File "/home/wsj/anaconda3/envs/pagedATTN/lib/python3.11/site-packages/torch/onnx/_internal/fx/__init__.py", line 1, in <module>
    from .patcher import ONNXTorchPatcher
  File "/home/wsj/anaconda3/envs/pagedATTN/lib/python3.11/site-packages/torch/onnx/_internal/fx/patcher.py", line 11, in <module>
    import transformers  # type: ignore[import]
    ^^^^^^^^^^^^^^^^^^^
  File "/home/wsj/anaconda3/envs/pagedATTN/lib/python3.11/site-packages/transformers/__init__.py", line 26, in <module>
    from . import dependency_versions_check
  File "/home/wsj/anaconda3/envs/pagedATTN/lib/python3.11/site-packages/transformers/dependency_versions_check.py", line 16, in <module>
    from .utils.versions import require_version, require_version_core
  File "/home/wsj/anaconda3/envs/pagedATTN/lib/python3.11/site-packages/transformers/utils/__init__.py", line 18, in <module>
    from huggingface_hub import get_full_repo_name  # for backward compatibility
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1229, in _handle_fromlist
  File "/home/wsj/anaconda3/envs/pagedATTN/lib/python3.11/site-packages/huggingface_hub/__init__.py", line 503, in __getattr__
    submod = importlib.import_module(submod_path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wsj/anaconda3/envs/pagedATTN/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wsj/anaconda3/envs/pagedATTN/lib/python3.11/site-packages/huggingface_hub/hf_api.py", line 363, in <module>
    @dataclass
     ^^^^^^^^^
  File "/home/wsj/anaconda3/envs/pagedATTN/lib/python3.11/dataclasses.py", line 1232, in dataclass
    return wrap(cls)
           ^^^^^^^^^
  File "/home/wsj/anaconda3/envs/pagedATTN/lib/python3.11/dataclasses.py", line 1222, in wrap
    return _process_class(cls, init, repr, eq, order, unsafe_hash,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wsj/anaconda3/envs/pagedATTN/lib/python3.11/dataclasses.py", line 1047, in _process_class
    _set_new_attribute(cls, '__repr__', _repr_fn(flds, globals))
                                        ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wsj/anaconda3/envs/pagedATTN/lib/python3.11/dataclasses.py", line 589, in _repr_fn
    fn = _create_fn('__repr__',
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wsj/anaconda3/envs/pagedATTN/lib/python3.11/dataclasses.py", line 433, in _create_fn
    exec(txt, globals, ns)
  File "<string>", line 0, in <module>
KeyboardInterrupt
Not using distributed mode
[13:51:19.291819] job dir: /home/wsj/mae
[13:51:19.291859] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.3,
norm_pix_loss=True,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/mae0.3',
log_dir='./output_dir/mae0.3',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[13:51:19.793466] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[13:51:19.793605] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x71e67629f0d0>
[13:51:20.250317] Model = MaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
)
[13:51:20.250342] base lr: 5.00e-04
[13:51:20.250346] actual lr: 5.00e-04
[13:51:20.250352] accumulate grad iterations: 1
[13:51:20.250357] effective batch size: 256
[13:51:20.251265] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[13:51:20.251332] Start training for 200 epochs
[13:51:20.252210] log_dir: ./output_dir/mae0.3
[13:51:21.506897] Epoch: [0]  [  0/195]  eta: 0:04:04  lr: 0.000000  loss: 2.3719 (2.3719)  time: 1.2536  data: 0.3329  max mem: 2522
[13:51:22.920137] Epoch: [0]  [ 20/195]  eta: 0:00:22  lr: 0.000001  loss: 2.3131 (2.2957)  time: 0.0706  data: 0.0002  max mem: 2598
[13:51:24.330202] Epoch: [0]  [ 40/195]  eta: 0:00:15  lr: 0.000003  loss: 1.8285 (2.0807)  time: 0.0704  data: 0.0002  max mem: 2598
[13:51:25.754808] Epoch: [0]  [ 60/195]  eta: 0:00:12  lr: 0.000004  loss: 1.3690 (1.8539)  time: 0.0712  data: 0.0002  max mem: 2598
[13:51:27.188773] Epoch: [0]  [ 80/195]  eta: 0:00:09  lr: 0.000005  loss: 1.0989 (1.6706)  time: 0.0717  data: 0.0002  max mem: 2598
[13:51:28.617987] Epoch: [0]  [100/195]  eta: 0:00:07  lr: 0.000006  loss: 0.9737 (1.5329)  time: 0.0714  data: 0.0002  max mem: 2598
[13:51:29.988643] Epoch: [0]  [120/195]  eta: 0:00:06  lr: 0.000008  loss: 0.8966 (1.4280)  time: 0.0685  data: 0.0001  max mem: 2598
[13:51:31.355876] Epoch: [0]  [140/195]  eta: 0:00:04  lr: 0.000009  loss: 0.8422 (1.3452)  time: 0.0683  data: 0.0001  max mem: 2598
[13:51:32.740838] Epoch: [0]  [160/195]  eta: 0:00:02  lr: 0.000010  loss: 0.8119 (1.2791)  time: 0.0692  data: 0.0001  max mem: 2598
[13:51:34.108628] Epoch: [0]  [180/195]  eta: 0:00:01  lr: 0.000012  loss: 0.7930 (1.2251)  time: 0.0684  data: 0.0001  max mem: 2598
[13:51:35.067599] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.000012  loss: 0.7739 (1.1931)  time: 0.0683  data: 0.0001  max mem: 2598
[13:51:35.116990] Epoch: [0] Total time: 0:00:14 (0.0762 s / it)
[13:51:35.117070] Averaged stats: lr: 0.000012  loss: 0.7739 (1.1931)
[13:51:35.358657] log_dir: ./output_dir/mae0.3
[13:51:35.792390] Epoch: [1]  [  0/195]  eta: 0:01:24  lr: 0.000013  loss: 0.8070 (0.8070)  time: 0.4329  data: 0.3433  max mem: 2598
[13:51:37.219721] Epoch: [1]  [ 20/195]  eta: 0:00:15  lr: 0.000014  loss: 0.7675 (0.7706)  time: 0.0713  data: 0.0001  max mem: 2598
[13:51:38.657123] Epoch: [1]  [ 40/195]  eta: 0:00:12  lr: 0.000015  loss: 0.7597 (0.7660)  time: 0.0718  data: 0.0001  max mem: 2598
[13:51:40.075076] Epoch: [1]  [ 60/195]  eta: 0:00:10  lr: 0.000016  loss: 0.7569 (0.7640)  time: 0.0709  data: 0.0002  max mem: 2598
[13:51:41.465792] Epoch: [1]  [ 80/195]  eta: 0:00:08  lr: 0.000018  loss: 0.7556 (0.7630)  time: 0.0695  data: 0.0002  max mem: 2598
[13:51:42.853462] Epoch: [1]  [100/195]  eta: 0:00:07  lr: 0.000019  loss: 0.7616 (0.7621)  time: 0.0693  data: 0.0001  max mem: 2598
[13:51:44.231895] Epoch: [1]  [120/195]  eta: 0:00:05  lr: 0.000020  loss: 0.7557 (0.7612)  time: 0.0689  data: 0.0001  max mem: 2598
[13:51:45.618294] Epoch: [1]  [140/195]  eta: 0:00:03  lr: 0.000021  loss: 0.7558 (0.7605)  time: 0.0693  data: 0.0002  max mem: 2598
[13:51:47.000316] Epoch: [1]  [160/195]  eta: 0:00:02  lr: 0.000023  loss: 0.7618 (0.7604)  time: 0.0691  data: 0.0001  max mem: 2598
[13:51:48.367581] Epoch: [1]  [180/195]  eta: 0:00:01  lr: 0.000024  loss: 0.7506 (0.7591)  time: 0.0683  data: 0.0001  max mem: 2598
[13:51:49.320894] Epoch: [1]  [194/195]  eta: 0:00:00  lr: 0.000025  loss: 0.7502 (0.7587)  time: 0.0680  data: 0.0001  max mem: 2598
[13:51:49.355510] Epoch: [1] Total time: 0:00:13 (0.0718 s / it)
[13:51:49.355577] Averaged stats: lr: 0.000025  loss: 0.7502 (0.7587)
[13:51:49.357834] log_dir: ./output_dir/mae0.3
[13:51:49.730523] Epoch: [2]  [  0/195]  eta: 0:01:12  lr: 0.000025  loss: 0.7842 (0.7842)  time: 0.3712  data: 0.2636  max mem: 2598
[13:51:51.160235] Epoch: [2]  [ 20/195]  eta: 0:00:14  lr: 0.000026  loss: 0.7444 (0.7510)  time: 0.0714  data: 0.0001  max mem: 2598
[13:51:52.560199] Epoch: [2]  [ 40/195]  eta: 0:00:12  lr: 0.000028  loss: 0.7391 (0.7468)  time: 0.0700  data: 0.0001  max mem: 2598
[13:51:53.952694] Epoch: [2]  [ 60/195]  eta: 0:00:10  lr: 0.000029  loss: 0.7458 (0.7467)  time: 0.0696  data: 0.0001  max mem: 2598
[13:51:55.323514] Epoch: [2]  [ 80/195]  eta: 0:00:08  lr: 0.000030  loss: 0.7485 (0.7469)  time: 0.0685  data: 0.0001  max mem: 2598
[13:51:56.698573] Epoch: [2]  [100/195]  eta: 0:00:06  lr: 0.000031  loss: 0.7523 (0.7474)  time: 0.0687  data: 0.0001  max mem: 2598
[13:51:58.073029] Epoch: [2]  [120/195]  eta: 0:00:05  lr: 0.000033  loss: 0.7455 (0.7475)  time: 0.0687  data: 0.0001  max mem: 2598
[13:51:59.452319] Epoch: [2]  [140/195]  eta: 0:00:03  lr: 0.000034  loss: 0.7409 (0.7471)  time: 0.0689  data: 0.0001  max mem: 2598
[13:52:00.832728] Epoch: [2]  [160/195]  eta: 0:00:02  lr: 0.000035  loss: 0.7425 (0.7468)  time: 0.0690  data: 0.0001  max mem: 2598
[13:52:02.203094] Epoch: [2]  [180/195]  eta: 0:00:01  lr: 0.000037  loss: 0.7323 (0.7455)  time: 0.0685  data: 0.0001  max mem: 2598
[13:52:03.151445] Epoch: [2]  [194/195]  eta: 0:00:00  lr: 0.000037  loss: 0.7300 (0.7446)  time: 0.0678  data: 0.0001  max mem: 2598
[13:52:03.198721] Epoch: [2] Total time: 0:00:13 (0.0710 s / it)
[13:52:03.198797] Averaged stats: lr: 0.000037  loss: 0.7300 (0.7446)
[13:52:03.201304] log_dir: ./output_dir/mae0.3
[13:52:03.600943] Epoch: [3]  [  0/195]  eta: 0:01:17  lr: 0.000038  loss: 0.7555 (0.7555)  time: 0.3986  data: 0.3092  max mem: 2598
[13:52:05.062792] Epoch: [3]  [ 20/195]  eta: 0:00:15  lr: 0.000039  loss: 0.7258 (0.7270)  time: 0.0730  data: 0.0002  max mem: 2598
[13:52:06.497137] Epoch: [3]  [ 40/195]  eta: 0:00:12  lr: 0.000040  loss: 0.7131 (0.7221)  time: 0.0717  data: 0.0002  max mem: 2598
[13:52:07.930947] Epoch: [3]  [ 60/195]  eta: 0:00:10  lr: 0.000041  loss: 0.7227 (0.7223)  time: 0.0717  data: 0.0002  max mem: 2598
[13:52:09.363529] Epoch: [3]  [ 80/195]  eta: 0:00:08  lr: 0.000043  loss: 0.7214 (0.7223)  time: 0.0716  data: 0.0002  max mem: 2598
[13:52:10.752192] Epoch: [3]  [100/195]  eta: 0:00:07  lr: 0.000044  loss: 0.7207 (0.7219)  time: 0.0694  data: 0.0001  max mem: 2598
[13:52:12.119671] Epoch: [3]  [120/195]  eta: 0:00:05  lr: 0.000045  loss: 0.7169 (0.7211)  time: 0.0683  data: 0.0001  max mem: 2598
[13:52:13.494924] Epoch: [3]  [140/195]  eta: 0:00:04  lr: 0.000046  loss: 0.7177 (0.7206)  time: 0.0687  data: 0.0001  max mem: 2598
[13:52:14.878869] Epoch: [3]  [160/195]  eta: 0:00:02  lr: 0.000048  loss: 0.7206 (0.7204)  time: 0.0692  data: 0.0001  max mem: 2598
[13:52:16.251472] Epoch: [3]  [180/195]  eta: 0:00:01  lr: 0.000049  loss: 0.7113 (0.7197)  time: 0.0686  data: 0.0001  max mem: 2598
[13:52:17.201465] Epoch: [3]  [194/195]  eta: 0:00:00  lr: 0.000050  loss: 0.7129 (0.7191)  time: 0.0679  data: 0.0001  max mem: 2598
[13:52:17.249900] Epoch: [3] Total time: 0:00:14 (0.0720 s / it)
[13:52:17.249993] Averaged stats: lr: 0.000050  loss: 0.7129 (0.7191)
[13:52:17.253838] log_dir: ./output_dir/mae0.3
[13:52:17.716642] Epoch: [4]  [  0/195]  eta: 0:01:29  lr: 0.000050  loss: 0.7321 (0.7321)  time: 0.4612  data: 0.3305  max mem: 2598
[13:52:19.175668] Epoch: [4]  [ 20/195]  eta: 0:00:15  lr: 0.000051  loss: 0.7060 (0.7063)  time: 0.0729  data: 0.0002  max mem: 2598
[13:52:20.586764] Epoch: [4]  [ 40/195]  eta: 0:00:12  lr: 0.000053  loss: 0.6933 (0.7016)  time: 0.0705  data: 0.0001  max mem: 2598
[13:52:22.008635] Epoch: [4]  [ 60/195]  eta: 0:00:10  lr: 0.000054  loss: 0.6950 (0.7007)  time: 0.0710  data: 0.0001  max mem: 2598
[13:52:23.427970] Epoch: [4]  [ 80/195]  eta: 0:00:08  lr: 0.000055  loss: 0.6929 (0.6987)  time: 0.0709  data: 0.0001  max mem: 2598
[13:52:24.862252] Epoch: [4]  [100/195]  eta: 0:00:07  lr: 0.000056  loss: 0.6877 (0.6966)  time: 0.0717  data: 0.0002  max mem: 2598
[13:52:26.296826] Epoch: [4]  [120/195]  eta: 0:00:05  lr: 0.000058  loss: 0.6856 (0.6949)  time: 0.0717  data: 0.0002  max mem: 2598
[13:52:27.719297] Epoch: [4]  [140/195]  eta: 0:00:04  lr: 0.000059  loss: 0.6804 (0.6931)  time: 0.0711  data: 0.0001  max mem: 2598
[13:52:29.140335] Epoch: [4]  [160/195]  eta: 0:00:02  lr: 0.000060  loss: 0.6812 (0.6917)  time: 0.0710  data: 0.0002  max mem: 2598
[13:52:30.598707] Epoch: [4]  [180/195]  eta: 0:00:01  lr: 0.000062  loss: 0.6728 (0.6898)  time: 0.0729  data: 0.0001  max mem: 2598
[13:52:31.573920] Epoch: [4]  [194/195]  eta: 0:00:00  lr: 0.000062  loss: 0.6696 (0.6883)  time: 0.0711  data: 0.0001  max mem: 2598
[13:52:31.628871] Epoch: [4] Total time: 0:00:14 (0.0737 s / it)
[13:52:31.628974] Averaged stats: lr: 0.000062  loss: 0.6696 (0.6883)
[13:52:31.632302] log_dir: ./output_dir/mae0.3
[13:52:32.121129] Epoch: [5]  [  0/195]  eta: 0:01:35  lr: 0.000063  loss: 0.6806 (0.6806)  time: 0.4872  data: 0.3618  max mem: 2598
[13:52:33.577779] Epoch: [5]  [ 20/195]  eta: 0:00:16  lr: 0.000064  loss: 0.6632 (0.6635)  time: 0.0728  data: 0.0002  max mem: 2598
[13:52:34.963227] Epoch: [5]  [ 40/195]  eta: 0:00:12  lr: 0.000065  loss: 0.6551 (0.6609)  time: 0.0692  data: 0.0001  max mem: 2598
[13:52:36.334822] Epoch: [5]  [ 60/195]  eta: 0:00:10  lr: 0.000066  loss: 0.6570 (0.6599)  time: 0.0685  data: 0.0001  max mem: 2598
[13:52:37.703477] Epoch: [5]  [ 80/195]  eta: 0:00:08  lr: 0.000068  loss: 0.6515 (0.6586)  time: 0.0684  data: 0.0001  max mem: 2598
[13:52:39.070603] Epoch: [5]  [100/195]  eta: 0:00:06  lr: 0.000069  loss: 0.6463 (0.6562)  time: 0.0683  data: 0.0001  max mem: 2598
[13:52:40.439098] Epoch: [5]  [120/195]  eta: 0:00:05  lr: 0.000070  loss: 0.6368 (0.6537)  time: 0.0684  data: 0.0001  max mem: 2598
[13:52:41.806265] Epoch: [5]  [140/195]  eta: 0:00:03  lr: 0.000071  loss: 0.6341 (0.6508)  time: 0.0683  data: 0.0001  max mem: 2598
[13:52:43.175063] Epoch: [5]  [160/195]  eta: 0:00:02  lr: 0.000073  loss: 0.6242 (0.6475)  time: 0.0684  data: 0.0001  max mem: 2598
[13:52:44.540970] Epoch: [5]  [180/195]  eta: 0:00:01  lr: 0.000074  loss: 0.6121 (0.6434)  time: 0.0683  data: 0.0001  max mem: 2598
[13:52:45.489868] Epoch: [5]  [194/195]  eta: 0:00:00  lr: 0.000075  loss: 0.6090 (0.6412)  time: 0.0678  data: 0.0001  max mem: 2598
[13:52:45.544684] Epoch: [5] Total time: 0:00:13 (0.0713 s / it)
[13:52:45.544741] Averaged stats: lr: 0.000075  loss: 0.6090 (0.6412)
[13:52:45.546914] log_dir: ./output_dir/mae0.3
[13:52:45.914347] Epoch: [6]  [  0/195]  eta: 0:01:11  lr: 0.000075  loss: 0.6098 (0.6098)  time: 0.3663  data: 0.2759  max mem: 2598
[13:52:47.330423] Epoch: [6]  [ 20/195]  eta: 0:00:14  lr: 0.000076  loss: 0.5965 (0.5971)  time: 0.0708  data: 0.0001  max mem: 2598
[13:52:48.733010] Epoch: [6]  [ 40/195]  eta: 0:00:12  lr: 0.000078  loss: 0.5870 (0.5935)  time: 0.0701  data: 0.0001  max mem: 2598
[13:52:50.121761] Epoch: [6]  [ 60/195]  eta: 0:00:10  lr: 0.000079  loss: 0.5881 (0.5908)  time: 0.0694  data: 0.0001  max mem: 2598
[13:52:51.503496] Epoch: [6]  [ 80/195]  eta: 0:00:08  lr: 0.000080  loss: 0.5774 (0.5878)  time: 0.0690  data: 0.0001  max mem: 2598
[13:52:52.884029] Epoch: [6]  [100/195]  eta: 0:00:06  lr: 0.000081  loss: 0.5693 (0.5840)  time: 0.0690  data: 0.0001  max mem: 2598
[13:52:54.261794] Epoch: [6]  [120/195]  eta: 0:00:05  lr: 0.000083  loss: 0.5612 (0.5804)  time: 0.0689  data: 0.0001  max mem: 2598
[13:52:55.641850] Epoch: [6]  [140/195]  eta: 0:00:03  lr: 0.000084  loss: 0.5536 (0.5765)  time: 0.0690  data: 0.0001  max mem: 2598
[13:52:57.034497] Epoch: [6]  [160/195]  eta: 0:00:02  lr: 0.000085  loss: 0.5448 (0.5726)  time: 0.0696  data: 0.0001  max mem: 2598
[13:52:58.405523] Epoch: [6]  [180/195]  eta: 0:00:01  lr: 0.000087  loss: 0.5374 (0.5687)  time: 0.0685  data: 0.0002  max mem: 2598
[13:52:59.358881] Epoch: [6]  [194/195]  eta: 0:00:00  lr: 0.000087  loss: 0.5360 (0.5663)  time: 0.0681  data: 0.0001  max mem: 2598
[13:52:59.407854] Epoch: [6] Total time: 0:00:13 (0.0711 s / it)
[13:52:59.407893] Averaged stats: lr: 0.000087  loss: 0.5360 (0.5663)
[13:52:59.409911] log_dir: ./output_dir/mae0.3
[13:52:59.876934] Epoch: [7]  [  0/195]  eta: 0:01:30  lr: 0.000087  loss: 0.5350 (0.5350)  time: 0.4659  data: 0.3681  max mem: 2598
[13:53:01.308057] Epoch: [7]  [ 20/195]  eta: 0:00:15  lr: 0.000089  loss: 0.5208 (0.5212)  time: 0.0715  data: 0.0002  max mem: 2598
[13:53:02.723087] Epoch: [7]  [ 40/195]  eta: 0:00:12  lr: 0.000090  loss: 0.5079 (0.5177)  time: 0.0707  data: 0.0001  max mem: 2598
[13:53:04.115825] Epoch: [7]  [ 60/195]  eta: 0:00:10  lr: 0.000091  loss: 0.5151 (0.5167)  time: 0.0696  data: 0.0001  max mem: 2598
[13:53:05.489342] Epoch: [7]  [ 80/195]  eta: 0:00:08  lr: 0.000093  loss: 0.5096 (0.5152)  time: 0.0686  data: 0.0001  max mem: 2598
[13:53:06.866221] Epoch: [7]  [100/195]  eta: 0:00:07  lr: 0.000094  loss: 0.5088 (0.5135)  time: 0.0688  data: 0.0001  max mem: 2598
[13:53:08.245226] Epoch: [7]  [120/195]  eta: 0:00:05  lr: 0.000095  loss: 0.5036 (0.5118)  time: 0.0689  data: 0.0001  max mem: 2598
[13:53:09.620682] Epoch: [7]  [140/195]  eta: 0:00:03  lr: 0.000096  loss: 0.4966 (0.5098)  time: 0.0687  data: 0.0001  max mem: 2598
[13:53:11.002765] Epoch: [7]  [160/195]  eta: 0:00:02  lr: 0.000098  loss: 0.4902 (0.5076)  time: 0.0691  data: 0.0001  max mem: 2598
[13:53:12.381681] Epoch: [7]  [180/195]  eta: 0:00:01  lr: 0.000099  loss: 0.4815 (0.5050)  time: 0.0689  data: 0.0001  max mem: 2598
[13:53:13.334885] Epoch: [7]  [194/195]  eta: 0:00:00  lr: 0.000100  loss: 0.4824 (0.5034)  time: 0.0681  data: 0.0001  max mem: 2598
[13:53:13.399145] Epoch: [7] Total time: 0:00:13 (0.0717 s / it)
[13:53:13.399233] Averaged stats: lr: 0.000100  loss: 0.4824 (0.5034)
[13:53:13.402406] log_dir: ./output_dir/mae0.3
[13:53:13.860233] Epoch: [8]  [  0/195]  eta: 0:01:29  lr: 0.000100  loss: 0.4824 (0.4824)  time: 0.4565  data: 0.3396  max mem: 2598
[13:53:15.302779] Epoch: [8]  [ 20/195]  eta: 0:00:15  lr: 0.000101  loss: 0.4748 (0.4742)  time: 0.0721  data: 0.0002  max mem: 2598
[13:53:16.730543] Epoch: [8]  [ 40/195]  eta: 0:00:12  lr: 0.000103  loss: 0.4707 (0.4732)  time: 0.0713  data: 0.0002  max mem: 2598
[13:53:18.150890] Epoch: [8]  [ 60/195]  eta: 0:00:10  lr: 0.000104  loss: 0.4700 (0.4720)  time: 0.0710  data: 0.0002  max mem: 2598
[13:53:19.588571] Epoch: [8]  [ 80/195]  eta: 0:00:08  lr: 0.000105  loss: 0.4671 (0.4710)  time: 0.0718  data: 0.0002  max mem: 2598
[13:53:21.017817] Epoch: [8]  [100/195]  eta: 0:00:07  lr: 0.000106  loss: 0.4644 (0.4699)  time: 0.0714  data: 0.0002  max mem: 2598
[13:53:22.442775] Epoch: [8]  [120/195]  eta: 0:00:05  lr: 0.000108  loss: 0.4620 (0.4684)  time: 0.0712  data: 0.0002  max mem: 2598
[13:53:23.861913] Epoch: [8]  [140/195]  eta: 0:00:04  lr: 0.000109  loss: 0.4551 (0.4665)  time: 0.0709  data: 0.0002  max mem: 2598
[13:53:25.288920] Epoch: [8]  [160/195]  eta: 0:00:02  lr: 0.000110  loss: 0.4551 (0.4651)  time: 0.0713  data: 0.0002  max mem: 2598
[13:53:26.698496] Epoch: [8]  [180/195]  eta: 0:00:01  lr: 0.000112  loss: 0.4458 (0.4630)  time: 0.0704  data: 0.0001  max mem: 2598
[13:53:27.671621] Epoch: [8]  [194/195]  eta: 0:00:00  lr: 0.000112  loss: 0.4483 (0.4621)  time: 0.0696  data: 0.0001  max mem: 2598
[13:53:27.742616] Epoch: [8] Total time: 0:00:14 (0.0735 s / it)
[13:53:27.742719] Averaged stats: lr: 0.000112  loss: 0.4483 (0.4621)
[13:53:27.745872] log_dir: ./output_dir/mae0.3
[13:53:28.219378] Epoch: [9]  [  0/195]  eta: 0:01:32  lr: 0.000113  loss: 0.4527 (0.4527)  time: 0.4724  data: 0.3772  max mem: 2598
[13:53:29.650775] Epoch: [9]  [ 20/195]  eta: 0:00:15  lr: 0.000114  loss: 0.4396 (0.4400)  time: 0.0715  data: 0.0002  max mem: 2598
[13:53:31.063519] Epoch: [9]  [ 40/195]  eta: 0:00:12  lr: 0.000115  loss: 0.4358 (0.4386)  time: 0.0706  data: 0.0001  max mem: 2598
[13:53:32.487474] Epoch: [9]  [ 60/195]  eta: 0:00:10  lr: 0.000116  loss: 0.4333 (0.4377)  time: 0.0712  data: 0.0002  max mem: 2598
[13:53:33.881558] Epoch: [9]  [ 80/195]  eta: 0:00:08  lr: 0.000118  loss: 0.4316 (0.4372)  time: 0.0697  data: 0.0001  max mem: 2598
[13:53:35.275221] Epoch: [9]  [100/195]  eta: 0:00:07  lr: 0.000119  loss: 0.4243 (0.4354)  time: 0.0696  data: 0.0001  max mem: 2598
[13:53:36.675964] Epoch: [9]  [120/195]  eta: 0:00:05  lr: 0.000120  loss: 0.4313 (0.4343)  time: 0.0700  data: 0.0001  max mem: 2598
[13:53:38.075285] Epoch: [9]  [140/195]  eta: 0:00:04  lr: 0.000121  loss: 0.4252 (0.4331)  time: 0.0699  data: 0.0002  max mem: 2598
[13:53:39.479538] Epoch: [9]  [160/195]  eta: 0:00:02  lr: 0.000123  loss: 0.4229 (0.4321)  time: 0.0702  data: 0.0001  max mem: 2598
[13:53:40.872627] Epoch: [9]  [180/195]  eta: 0:00:01  lr: 0.000124  loss: 0.4130 (0.4305)  time: 0.0696  data: 0.0001  max mem: 2598
[13:53:41.837605] Epoch: [9]  [194/195]  eta: 0:00:00  lr: 0.000125  loss: 0.4179 (0.4298)  time: 0.0690  data: 0.0001  max mem: 2598
[13:53:41.922291] Epoch: [9] Total time: 0:00:14 (0.0727 s / it)
[13:53:41.922365] Averaged stats: lr: 0.000125  loss: 0.4179 (0.4298)
[13:53:41.924939] log_dir: ./output_dir/mae0.3
[13:53:42.384724] Epoch: [10]  [  0/195]  eta: 0:01:29  lr: 0.000125  loss: 0.4236 (0.4236)  time: 0.4585  data: 0.3331  max mem: 2598
[13:53:43.837670] Epoch: [10]  [ 20/195]  eta: 0:00:15  lr: 0.000126  loss: 0.4151 (0.4154)  time: 0.0726  data: 0.0001  max mem: 2598
[13:53:45.239729] Epoch: [10]  [ 40/195]  eta: 0:00:12  lr: 0.000128  loss: 0.4120 (0.4133)  time: 0.0701  data: 0.0001  max mem: 2598
[13:53:46.647837] Epoch: [10]  [ 60/195]  eta: 0:00:10  lr: 0.000129  loss: 0.4116 (0.4126)  time: 0.0704  data: 0.0001  max mem: 2598
[13:53:48.049460] Epoch: [10]  [ 80/195]  eta: 0:00:08  lr: 0.000130  loss: 0.4045 (0.4112)  time: 0.0700  data: 0.0002  max mem: 2598
[13:53:49.455028] Epoch: [10]  [100/195]  eta: 0:00:07  lr: 0.000131  loss: 0.4045 (0.4100)  time: 0.0702  data: 0.0002  max mem: 2598
[13:53:50.861140] Epoch: [10]  [120/195]  eta: 0:00:05  lr: 0.000133  loss: 0.4051 (0.4087)  time: 0.0703  data: 0.0001  max mem: 2598
[13:53:52.264774] Epoch: [10]  [140/195]  eta: 0:00:04  lr: 0.000134  loss: 0.3972 (0.4071)  time: 0.0701  data: 0.0001  max mem: 2598
[13:53:53.670465] Epoch: [10]  [160/195]  eta: 0:00:02  lr: 0.000135  loss: 0.4013 (0.4062)  time: 0.0703  data: 0.0001  max mem: 2598
[13:53:55.068032] Epoch: [10]  [180/195]  eta: 0:00:01  lr: 0.000137  loss: 0.3916 (0.4048)  time: 0.0698  data: 0.0001  max mem: 2598
[13:53:56.034494] Epoch: [10]  [194/195]  eta: 0:00:00  lr: 0.000137  loss: 0.3916 (0.4041)  time: 0.0691  data: 0.0001  max mem: 2598
[13:53:56.088667] Epoch: [10] Total time: 0:00:14 (0.0726 s / it)
[13:53:56.088741] Averaged stats: lr: 0.000137  loss: 0.3916 (0.4041)
[13:53:56.091385] log_dir: ./output_dir/mae0.3
[13:53:56.513325] Epoch: [11]  [  0/195]  eta: 0:01:22  lr: 0.000137  loss: 0.4001 (0.4001)  time: 0.4210  data: 0.3155  max mem: 2598
[13:53:57.961522] Epoch: [11]  [ 20/195]  eta: 0:00:15  lr: 0.000139  loss: 0.3868 (0.3902)  time: 0.0724  data: 0.0002  max mem: 2598
[13:53:59.401328] Epoch: [11]  [ 40/195]  eta: 0:00:12  lr: 0.000140  loss: 0.3825 (0.3881)  time: 0.0719  data: 0.0002  max mem: 2598
[13:54:00.784567] Epoch: [11]  [ 60/195]  eta: 0:00:10  lr: 0.000141  loss: 0.3815 (0.3864)  time: 0.0691  data: 0.0001  max mem: 2598
[13:54:02.160973] Epoch: [11]  [ 80/195]  eta: 0:00:08  lr: 0.000143  loss: 0.3809 (0.3851)  time: 0.0688  data: 0.0001  max mem: 2598
[13:54:03.533578] Epoch: [11]  [100/195]  eta: 0:00:06  lr: 0.000144  loss: 0.3819 (0.3844)  time: 0.0686  data: 0.0001  max mem: 2598
[13:54:04.907852] Epoch: [11]  [120/195]  eta: 0:00:05  lr: 0.000145  loss: 0.3808 (0.3837)  time: 0.0687  data: 0.0001  max mem: 2598
[13:54:06.285949] Epoch: [11]  [140/195]  eta: 0:00:03  lr: 0.000146  loss: 0.3726 (0.3825)  time: 0.0689  data: 0.0001  max mem: 2598
[13:54:07.678402] Epoch: [11]  [160/195]  eta: 0:00:02  lr: 0.000148  loss: 0.3827 (0.3823)  time: 0.0696  data: 0.0001  max mem: 2598
[13:54:09.059252] Epoch: [11]  [180/195]  eta: 0:00:01  lr: 0.000149  loss: 0.3691 (0.3812)  time: 0.0690  data: 0.0001  max mem: 2598
[13:54:10.017995] Epoch: [11]  [194/195]  eta: 0:00:00  lr: 0.000150  loss: 0.3704 (0.3806)  time: 0.0683  data: 0.0001  max mem: 2598
[13:54:10.075293] Epoch: [11] Total time: 0:00:13 (0.0717 s / it)
[13:54:10.075378] Averaged stats: lr: 0.000150  loss: 0.3704 (0.3806)
[13:54:10.078790] log_dir: ./output_dir/mae0.3
[13:54:10.465933] Epoch: [12]  [  0/195]  eta: 0:01:15  lr: 0.000150  loss: 0.3764 (0.3764)  time: 0.3859  data: 0.3077  max mem: 2598
[13:54:11.894946] Epoch: [12]  [ 20/195]  eta: 0:00:15  lr: 0.000151  loss: 0.3650 (0.3653)  time: 0.0714  data: 0.0001  max mem: 2598
[13:54:13.308637] Epoch: [12]  [ 40/195]  eta: 0:00:12  lr: 0.000153  loss: 0.3607 (0.3642)  time: 0.0706  data: 0.0001  max mem: 2598
[13:54:14.710413] Epoch: [12]  [ 60/195]  eta: 0:00:10  lr: 0.000154  loss: 0.3596 (0.3630)  time: 0.0700  data: 0.0001  max mem: 2598
[13:54:16.110093] Epoch: [12]  [ 80/195]  eta: 0:00:08  lr: 0.000155  loss: 0.3648 (0.3631)  time: 0.0699  data: 0.0001  max mem: 2598
[13:54:17.512380] Epoch: [12]  [100/195]  eta: 0:00:06  lr: 0.000156  loss: 0.3555 (0.3619)  time: 0.0701  data: 0.0001  max mem: 2598
[13:54:18.910922] Epoch: [12]  [120/195]  eta: 0:00:05  lr: 0.000158  loss: 0.3565 (0.3612)  time: 0.0699  data: 0.0001  max mem: 2598
[13:54:20.309864] Epoch: [12]  [140/195]  eta: 0:00:03  lr: 0.000159  loss: 0.3532 (0.3599)  time: 0.0699  data: 0.0001  max mem: 2598
[13:54:21.705718] Epoch: [12]  [160/195]  eta: 0:00:02  lr: 0.000160  loss: 0.3510 (0.3590)  time: 0.0698  data: 0.0001  max mem: 2598
[13:54:23.103404] Epoch: [12]  [180/195]  eta: 0:00:01  lr: 0.000162  loss: 0.3435 (0.3576)  time: 0.0698  data: 0.0001  max mem: 2598
[13:54:24.069877] Epoch: [12]  [194/195]  eta: 0:00:00  lr: 0.000162  loss: 0.3420 (0.3567)  time: 0.0690  data: 0.0001  max mem: 2598
[13:54:24.123393] Epoch: [12] Total time: 0:00:14 (0.0720 s / it)
[13:54:24.123462] Averaged stats: lr: 0.000162  loss: 0.3420 (0.3567)
[13:54:24.125964] log_dir: ./output_dir/mae0.3
[13:54:24.511022] Epoch: [13]  [  0/195]  eta: 0:01:14  lr: 0.000163  loss: 0.3419 (0.3419)  time: 0.3832  data: 0.2971  max mem: 2598
[13:54:25.956197] Epoch: [13]  [ 20/195]  eta: 0:00:15  lr: 0.000164  loss: 0.3396 (0.3400)  time: 0.0722  data: 0.0002  max mem: 2598
[13:54:27.366501] Epoch: [13]  [ 40/195]  eta: 0:00:12  lr: 0.000165  loss: 0.3405 (0.3401)  time: 0.0705  data: 0.0002  max mem: 2598
[13:54:28.791348] Epoch: [13]  [ 60/195]  eta: 0:00:10  lr: 0.000166  loss: 0.3406 (0.3400)  time: 0.0712  data: 0.0001  max mem: 2598
[13:54:30.199972] Epoch: [13]  [ 80/195]  eta: 0:00:08  lr: 0.000168  loss: 0.3332 (0.3390)  time: 0.0704  data: 0.0002  max mem: 2598
[13:54:31.612403] Epoch: [13]  [100/195]  eta: 0:00:07  lr: 0.000169  loss: 0.3342 (0.3384)  time: 0.0706  data: 0.0001  max mem: 2598
[13:54:33.043896] Epoch: [13]  [120/195]  eta: 0:00:05  lr: 0.000170  loss: 0.3309 (0.3372)  time: 0.0715  data: 0.0001  max mem: 2598
[13:54:34.465030] Epoch: [13]  [140/195]  eta: 0:00:04  lr: 0.000171  loss: 0.3282 (0.3360)  time: 0.0710  data: 0.0001  max mem: 2598
[13:54:35.890388] Epoch: [13]  [160/195]  eta: 0:00:02  lr: 0.000173  loss: 0.3277 (0.3348)  time: 0.0712  data: 0.0001  max mem: 2598
[13:54:37.307268] Epoch: [13]  [180/195]  eta: 0:00:01  lr: 0.000174  loss: 0.3268 (0.3339)  time: 0.0708  data: 0.0002  max mem: 2598
[13:54:38.278988] Epoch: [13]  [194/195]  eta: 0:00:00  lr: 0.000175  loss: 0.3206 (0.3330)  time: 0.0695  data: 0.0001  max mem: 2598
[13:54:38.349294] Epoch: [13] Total time: 0:00:14 (0.0729 s / it)
[13:54:38.349390] Averaged stats: lr: 0.000175  loss: 0.3206 (0.3330)
[13:54:38.352544] log_dir: ./output_dir/mae0.3
[13:54:38.860152] Epoch: [14]  [  0/195]  eta: 0:01:38  lr: 0.000175  loss: 0.3330 (0.3330)  time: 0.5063  data: 0.3757  max mem: 2598
[13:54:40.306854] Epoch: [14]  [ 20/195]  eta: 0:00:16  lr: 0.000176  loss: 0.3126 (0.3179)  time: 0.0723  data: 0.0001  max mem: 2598
[13:54:41.703749] Epoch: [14]  [ 40/195]  eta: 0:00:12  lr: 0.000178  loss: 0.3223 (0.3198)  time: 0.0698  data: 0.0001  max mem: 2598
[13:54:43.103384] Epoch: [14]  [ 60/195]  eta: 0:00:10  lr: 0.000179  loss: 0.3186 (0.3191)  time: 0.0699  data: 0.0001  max mem: 2598
[13:54:44.501767] Epoch: [14]  [ 80/195]  eta: 0:00:08  lr: 0.000180  loss: 0.3129 (0.3180)  time: 0.0699  data: 0.0001  max mem: 2598
[13:54:45.919042] Epoch: [14]  [100/195]  eta: 0:00:07  lr: 0.000181  loss: 0.3135 (0.3174)  time: 0.0708  data: 0.0001  max mem: 2598
[13:54:47.334227] Epoch: [14]  [120/195]  eta: 0:00:05  lr: 0.000183  loss: 0.3142 (0.3166)  time: 0.0707  data: 0.0001  max mem: 2598
[13:54:48.748823] Epoch: [14]  [140/195]  eta: 0:00:04  lr: 0.000184  loss: 0.3128 (0.3162)  time: 0.0707  data: 0.0001  max mem: 2598
[13:54:50.164546] Epoch: [14]  [160/195]  eta: 0:00:02  lr: 0.000185  loss: 0.3106 (0.3154)  time: 0.0707  data: 0.0001  max mem: 2598
[13:54:51.556466] Epoch: [14]  [180/195]  eta: 0:00:01  lr: 0.000187  loss: 0.3030 (0.3144)  time: 0.0696  data: 0.0002  max mem: 2598
[13:54:52.521507] Epoch: [14]  [194/195]  eta: 0:00:00  lr: 0.000187  loss: 0.3030 (0.3137)  time: 0.0690  data: 0.0001  max mem: 2598
[13:54:52.586812] Epoch: [14] Total time: 0:00:14 (0.0730 s / it)
[13:54:52.586921] Averaged stats: lr: 0.000187  loss: 0.3030 (0.3137)
[13:54:52.591607] log_dir: ./output_dir/mae0.3
[13:54:53.032685] Epoch: [15]  [  0/195]  eta: 0:01:25  lr: 0.000188  loss: 0.3166 (0.3166)  time: 0.4403  data: 0.3548  max mem: 2598
[13:54:54.426881] Epoch: [15]  [ 20/195]  eta: 0:00:15  lr: 0.000189  loss: 0.3000 (0.3021)  time: 0.0697  data: 0.0001  max mem: 2598
[13:54:55.834269] Epoch: [15]  [ 40/195]  eta: 0:00:12  lr: 0.000190  loss: 0.3033 (0.3026)  time: 0.0703  data: 0.0001  max mem: 2598
[13:54:57.234910] Epoch: [15]  [ 60/195]  eta: 0:00:10  lr: 0.000191  loss: 0.3012 (0.3019)  time: 0.0700  data: 0.0001  max mem: 2598
[13:54:58.637505] Epoch: [15]  [ 80/195]  eta: 0:00:08  lr: 0.000193  loss: 0.2986 (0.3014)  time: 0.0701  data: 0.0001  max mem: 2598
[13:55:00.054750] Epoch: [15]  [100/195]  eta: 0:00:07  lr: 0.000194  loss: 0.2988 (0.3007)  time: 0.0708  data: 0.0001  max mem: 2598
[13:55:01.472162] Epoch: [15]  [120/195]  eta: 0:00:05  lr: 0.000195  loss: 0.3006 (0.3005)  time: 0.0708  data: 0.0001  max mem: 2598
[13:55:02.884421] Epoch: [15]  [140/195]  eta: 0:00:04  lr: 0.000196  loss: 0.2971 (0.3000)  time: 0.0706  data: 0.0001  max mem: 2598
[13:55:04.291883] Epoch: [15]  [160/195]  eta: 0:00:02  lr: 0.000198  loss: 0.2983 (0.2998)  time: 0.0703  data: 0.0001  max mem: 2598
[13:55:05.715249] Epoch: [15]  [180/195]  eta: 0:00:01  lr: 0.000199  loss: 0.2912 (0.2991)  time: 0.0711  data: 0.0002  max mem: 2598
[13:55:06.680709] Epoch: [15]  [194/195]  eta: 0:00:00  lr: 0.000200  loss: 0.2913 (0.2984)  time: 0.0691  data: 0.0001  max mem: 2598
[13:55:06.729423] Epoch: [15] Total time: 0:00:14 (0.0725 s / it)
[13:55:06.729628] Averaged stats: lr: 0.000200  loss: 0.2913 (0.2984)
[13:55:06.732227] log_dir: ./output_dir/mae0.3
[13:55:07.132321] Epoch: [16]  [  0/195]  eta: 0:01:17  lr: 0.000200  loss: 0.3012 (0.3012)  time: 0.3987  data: 0.2996  max mem: 2598
[13:55:08.580324] Epoch: [16]  [ 20/195]  eta: 0:00:15  lr: 0.000201  loss: 0.2909 (0.2907)  time: 0.0724  data: 0.0001  max mem: 2598
[13:55:09.985648] Epoch: [16]  [ 40/195]  eta: 0:00:12  lr: 0.000203  loss: 0.2894 (0.2902)  time: 0.0702  data: 0.0001  max mem: 2598
[13:55:11.385054] Epoch: [16]  [ 60/195]  eta: 0:00:10  lr: 0.000204  loss: 0.2853 (0.2891)  time: 0.0699  data: 0.0001  max mem: 2598
[13:55:12.783692] Epoch: [16]  [ 80/195]  eta: 0:00:08  lr: 0.000205  loss: 0.2885 (0.2892)  time: 0.0699  data: 0.0001  max mem: 2598
[13:55:14.198074] Epoch: [16]  [100/195]  eta: 0:00:07  lr: 0.000206  loss: 0.2847 (0.2882)  time: 0.0707  data: 0.0002  max mem: 2598
[13:55:15.611714] Epoch: [16]  [120/195]  eta: 0:00:05  lr: 0.000208  loss: 0.2827 (0.2871)  time: 0.0706  data: 0.0001  max mem: 2598
[13:55:17.022941] Epoch: [16]  [140/195]  eta: 0:00:04  lr: 0.000209  loss: 0.2823 (0.2868)  time: 0.0705  data: 0.0001  max mem: 2598
[13:55:18.436821] Epoch: [16]  [160/195]  eta: 0:00:02  lr: 0.000210  loss: 0.2858 (0.2867)  time: 0.0707  data: 0.0001  max mem: 2598
[13:55:19.845133] Epoch: [16]  [180/195]  eta: 0:00:01  lr: 0.000212  loss: 0.2775 (0.2859)  time: 0.0704  data: 0.0001  max mem: 2598
[13:55:20.813122] Epoch: [16]  [194/195]  eta: 0:00:00  lr: 0.000212  loss: 0.2817 (0.2855)  time: 0.0696  data: 0.0001  max mem: 2598
[13:55:20.858520] Epoch: [16] Total time: 0:00:14 (0.0724 s / it)
[13:55:20.858592] Averaged stats: lr: 0.000212  loss: 0.2817 (0.2855)
[13:55:20.861181] log_dir: ./output_dir/mae0.3
[13:55:21.243155] Epoch: [17]  [  0/195]  eta: 0:01:14  lr: 0.000213  loss: 0.2827 (0.2827)  time: 0.3807  data: 0.2979  max mem: 2598
[13:55:22.687048] Epoch: [17]  [ 20/195]  eta: 0:00:15  lr: 0.000214  loss: 0.2744 (0.2763)  time: 0.0721  data: 0.0001  max mem: 2598
[13:55:24.093691] Epoch: [17]  [ 40/195]  eta: 0:00:12  lr: 0.000215  loss: 0.2778 (0.2773)  time: 0.0703  data: 0.0001  max mem: 2598
[13:55:25.506859] Epoch: [17]  [ 60/195]  eta: 0:00:10  lr: 0.000216  loss: 0.2774 (0.2770)  time: 0.0706  data: 0.0001  max mem: 2598
[13:55:26.913628] Epoch: [17]  [ 80/195]  eta: 0:00:08  lr: 0.000218  loss: 0.2727 (0.2762)  time: 0.0703  data: 0.0001  max mem: 2598
[13:55:28.322510] Epoch: [17]  [100/195]  eta: 0:00:07  lr: 0.000219  loss: 0.2707 (0.2754)  time: 0.0704  data: 0.0001  max mem: 2598
[13:55:29.735292] Epoch: [17]  [120/195]  eta: 0:00:05  lr: 0.000220  loss: 0.2741 (0.2752)  time: 0.0706  data: 0.0001  max mem: 2598
[13:55:31.146066] Epoch: [17]  [140/195]  eta: 0:00:04  lr: 0.000221  loss: 0.2729 (0.2748)  time: 0.0705  data: 0.0001  max mem: 2598
[13:55:32.564087] Epoch: [17]  [160/195]  eta: 0:00:02  lr: 0.000223  loss: 0.2698 (0.2744)  time: 0.0709  data: 0.0001  max mem: 2598
[13:55:33.966904] Epoch: [17]  [180/195]  eta: 0:00:01  lr: 0.000224  loss: 0.2669 (0.2737)  time: 0.0701  data: 0.0001  max mem: 2598
[13:55:34.936500] Epoch: [17]  [194/195]  eta: 0:00:00  lr: 0.000225  loss: 0.2663 (0.2732)  time: 0.0692  data: 0.0001  max mem: 2598
[13:55:34.982094] Epoch: [17] Total time: 0:00:14 (0.0724 s / it)
[13:55:34.982184] Averaged stats: lr: 0.000225  loss: 0.2663 (0.2732)
[13:55:34.985579] log_dir: ./output_dir/mae0.3
[13:55:35.410659] Epoch: [18]  [  0/195]  eta: 0:01:22  lr: 0.000225  loss: 0.2766 (0.2766)  time: 0.4238  data: 0.3222  max mem: 2598
[13:55:36.811749] Epoch: [18]  [ 20/195]  eta: 0:00:15  lr: 0.000226  loss: 0.2640 (0.2650)  time: 0.0700  data: 0.0001  max mem: 2598
[13:55:38.203499] Epoch: [18]  [ 40/195]  eta: 0:00:12  lr: 0.000228  loss: 0.2617 (0.2647)  time: 0.0695  data: 0.0001  max mem: 2598
[13:55:39.581767] Epoch: [18]  [ 60/195]  eta: 0:00:10  lr: 0.000229  loss: 0.2661 (0.2643)  time: 0.0689  data: 0.0001  max mem: 2598
[13:55:40.960230] Epoch: [18]  [ 80/195]  eta: 0:00:08  lr: 0.000230  loss: 0.2641 (0.2644)  time: 0.0689  data: 0.0001  max mem: 2598
[13:55:42.353621] Epoch: [18]  [100/195]  eta: 0:00:06  lr: 0.000231  loss: 0.2607 (0.2641)  time: 0.0696  data: 0.0001  max mem: 2598
[13:55:43.757641] Epoch: [18]  [120/195]  eta: 0:00:05  lr: 0.000233  loss: 0.2610 (0.2635)  time: 0.0702  data: 0.0001  max mem: 2598
[13:55:45.130728] Epoch: [18]  [140/195]  eta: 0:00:03  lr: 0.000234  loss: 0.2597 (0.2628)  time: 0.0686  data: 0.0001  max mem: 2598
[13:55:46.498693] Epoch: [18]  [160/195]  eta: 0:00:02  lr: 0.000235  loss: 0.2563 (0.2621)  time: 0.0684  data: 0.0001  max mem: 2598
[13:55:47.865576] Epoch: [18]  [180/195]  eta: 0:00:01  lr: 0.000237  loss: 0.2571 (0.2616)  time: 0.0683  data: 0.0001  max mem: 2598
[13:55:48.818281] Epoch: [18]  [194/195]  eta: 0:00:00  lr: 0.000237  loss: 0.2558 (0.2612)  time: 0.0680  data: 0.0001  max mem: 2598
[13:55:48.871272] Epoch: [18] Total time: 0:00:13 (0.0712 s / it)
[13:55:48.871339] Averaged stats: lr: 0.000237  loss: 0.2558 (0.2612)
[13:55:48.873874] log_dir: ./output_dir/mae0.3
[13:55:49.323205] Epoch: [19]  [  0/195]  eta: 0:01:27  lr: 0.000237  loss: 0.2698 (0.2698)  time: 0.4478  data: 0.3184  max mem: 2598
[13:55:50.746425] Epoch: [19]  [ 20/195]  eta: 0:00:15  lr: 0.000239  loss: 0.2544 (0.2546)  time: 0.0711  data: 0.0001  max mem: 2598
[13:55:52.149514] Epoch: [19]  [ 40/195]  eta: 0:00:12  lr: 0.000240  loss: 0.2518 (0.2528)  time: 0.0701  data: 0.0001  max mem: 2598
[13:55:53.553323] Epoch: [19]  [ 60/195]  eta: 0:00:10  lr: 0.000241  loss: 0.2533 (0.2535)  time: 0.0702  data: 0.0001  max mem: 2598
[13:55:54.951062] Epoch: [19]  [ 80/195]  eta: 0:00:08  lr: 0.000243  loss: 0.2536 (0.2540)  time: 0.0699  data: 0.0001  max mem: 2598
[13:55:56.354711] Epoch: [19]  [100/195]  eta: 0:00:07  lr: 0.000244  loss: 0.2525 (0.2539)  time: 0.0701  data: 0.0001  max mem: 2598
[13:55:57.759302] Epoch: [19]  [120/195]  eta: 0:00:05  lr: 0.000245  loss: 0.2509 (0.2536)  time: 0.0702  data: 0.0001  max mem: 2598
[13:55:59.161531] Epoch: [19]  [140/195]  eta: 0:00:04  lr: 0.000246  loss: 0.2481 (0.2531)  time: 0.0701  data: 0.0001  max mem: 2598
[13:56:00.576865] Epoch: [19]  [160/195]  eta: 0:00:02  lr: 0.000248  loss: 0.2495 (0.2527)  time: 0.0707  data: 0.0001  max mem: 2598
[13:56:01.977796] Epoch: [19]  [180/195]  eta: 0:00:01  lr: 0.000249  loss: 0.2495 (0.2522)  time: 0.0700  data: 0.0001  max mem: 2598
[13:56:02.943242] Epoch: [19]  [194/195]  eta: 0:00:00  lr: 0.000250  loss: 0.2514 (0.2522)  time: 0.0692  data: 0.0001  max mem: 2598
[13:56:03.012016] Epoch: [19] Total time: 0:00:14 (0.0725 s / it)
[13:56:03.012084] Averaged stats: lr: 0.000250  loss: 0.2514 (0.2522)
[13:56:03.014580] log_dir: ./output_dir/mae0.3
[13:56:03.457806] Epoch: [20]  [  0/195]  eta: 0:01:26  lr: 0.000250  loss: 0.2627 (0.2627)  time: 0.4416  data: 0.3103  max mem: 2598
[13:56:04.907266] Epoch: [20]  [ 20/195]  eta: 0:00:15  lr: 0.000251  loss: 0.2438 (0.2453)  time: 0.0724  data: 0.0002  max mem: 2598
[13:56:06.318015] Epoch: [20]  [ 40/195]  eta: 0:00:12  lr: 0.000253  loss: 0.2422 (0.2439)  time: 0.0705  data: 0.0001  max mem: 2598
[13:56:07.738558] Epoch: [20]  [ 60/195]  eta: 0:00:10  lr: 0.000254  loss: 0.2445 (0.2444)  time: 0.0710  data: 0.0001  max mem: 2598
[13:56:09.165752] Epoch: [20]  [ 80/195]  eta: 0:00:08  lr: 0.000255  loss: 0.2418 (0.2444)  time: 0.0713  data: 0.0001  max mem: 2598
[13:56:10.589125] Epoch: [20]  [100/195]  eta: 0:00:07  lr: 0.000256  loss: 0.2442 (0.2441)  time: 0.0711  data: 0.0001  max mem: 2598
[13:56:12.018487] Epoch: [20]  [120/195]  eta: 0:00:05  lr: 0.000258  loss: 0.2435 (0.2442)  time: 0.0714  data: 0.0002  max mem: 2598
[13:56:13.425970] Epoch: [20]  [140/195]  eta: 0:00:04  lr: 0.000259  loss: 0.2418 (0.2441)  time: 0.0703  data: 0.0001  max mem: 2598
[13:56:14.801590] Epoch: [20]  [160/195]  eta: 0:00:02  lr: 0.000260  loss: 0.2451 (0.2442)  time: 0.0688  data: 0.0001  max mem: 2598
[13:56:16.171854] Epoch: [20]  [180/195]  eta: 0:00:01  lr: 0.000262  loss: 0.2420 (0.2439)  time: 0.0685  data: 0.0001  max mem: 2598
[13:56:17.129491] Epoch: [20]  [194/195]  eta: 0:00:00  lr: 0.000262  loss: 0.2394 (0.2434)  time: 0.0683  data: 0.0001  max mem: 2598
[13:56:17.178462] Epoch: [20] Total time: 0:00:14 (0.0726 s / it)
[13:56:17.178527] Averaged stats: lr: 0.000262  loss: 0.2394 (0.2434)
[13:56:17.333803] log_dir: ./output_dir/mae0.3
[13:56:17.707395] Epoch: [21]  [  0/195]  eta: 0:01:12  lr: 0.000263  loss: 0.2497 (0.2497)  time: 0.3724  data: 0.2881  max mem: 2598
[13:56:19.150168] Epoch: [21]  [ 20/195]  eta: 0:00:15  lr: 0.000264  loss: 0.2348 (0.2366)  time: 0.0721  data: 0.0001  max mem: 2598
[13:56:20.555864] Epoch: [21]  [ 40/195]  eta: 0:00:12  lr: 0.000265  loss: 0.2354 (0.2370)  time: 0.0702  data: 0.0001  max mem: 2598
[13:56:21.953713] Epoch: [21]  [ 60/195]  eta: 0:00:10  lr: 0.000266  loss: 0.2356 (0.2366)  time: 0.0698  data: 0.0001  max mem: 2598
[13:56:23.363166] Epoch: [21]  [ 80/195]  eta: 0:00:08  lr: 0.000268  loss: 0.2355 (0.2368)  time: 0.0704  data: 0.0001  max mem: 2598
[13:56:24.773281] Epoch: [21]  [100/195]  eta: 0:00:06  lr: 0.000269  loss: 0.2369 (0.2366)  time: 0.0704  data: 0.0001  max mem: 2598
[13:56:26.178805] Epoch: [21]  [120/195]  eta: 0:00:05  lr: 0.000270  loss: 0.2380 (0.2370)  time: 0.0702  data: 0.0001  max mem: 2598
[13:56:27.584528] Epoch: [21]  [140/195]  eta: 0:00:03  lr: 0.000271  loss: 0.2345 (0.2366)  time: 0.0702  data: 0.0002  max mem: 2598
[13:56:28.973736] Epoch: [21]  [160/195]  eta: 0:00:02  lr: 0.000273  loss: 0.2377 (0.2366)  time: 0.0694  data: 0.0001  max mem: 2598
[13:56:30.374309] Epoch: [21]  [180/195]  eta: 0:00:01  lr: 0.000274  loss: 0.2300 (0.2361)  time: 0.0700  data: 0.0001  max mem: 2598
[13:56:31.343041] Epoch: [21]  [194/195]  eta: 0:00:00  lr: 0.000275  loss: 0.2352 (0.2361)  time: 0.0691  data: 0.0001  max mem: 2598
[13:56:31.393097] Epoch: [21] Total time: 0:00:14 (0.0721 s / it)
[13:56:31.393392] Averaged stats: lr: 0.000275  loss: 0.2352 (0.2361)
[13:56:31.396044] log_dir: ./output_dir/mae0.3
[13:56:31.770548] Epoch: [22]  [  0/195]  eta: 0:01:12  lr: 0.000275  loss: 0.2357 (0.2357)  time: 0.3733  data: 0.2863  max mem: 2598
[13:56:33.234768] Epoch: [22]  [ 20/195]  eta: 0:00:15  lr: 0.000276  loss: 0.2330 (0.2304)  time: 0.0732  data: 0.0001  max mem: 2598
[13:56:34.647894] Epoch: [22]  [ 40/195]  eta: 0:00:12  lr: 0.000278  loss: 0.2333 (0.2317)  time: 0.0706  data: 0.0001  max mem: 2598
[13:56:36.070321] Epoch: [22]  [ 60/195]  eta: 0:00:10  lr: 0.000279  loss: 0.2308 (0.2320)  time: 0.0710  data: 0.0001  max mem: 2598
[13:56:37.512054] Epoch: [22]  [ 80/195]  eta: 0:00:08  lr: 0.000280  loss: 0.2304 (0.2318)  time: 0.0720  data: 0.0002  max mem: 2598
[13:56:38.928183] Epoch: [22]  [100/195]  eta: 0:00:07  lr: 0.000281  loss: 0.2306 (0.2314)  time: 0.0708  data: 0.0001  max mem: 2598
[13:56:40.343893] Epoch: [22]  [120/195]  eta: 0:00:05  lr: 0.000283  loss: 0.2296 (0.2314)  time: 0.0707  data: 0.0001  max mem: 2598
[13:56:41.759662] Epoch: [22]  [140/195]  eta: 0:00:04  lr: 0.000284  loss: 0.2298 (0.2312)  time: 0.0708  data: 0.0002  max mem: 2598
[13:56:43.176451] Epoch: [22]  [160/195]  eta: 0:00:02  lr: 0.000285  loss: 0.2295 (0.2311)  time: 0.0708  data: 0.0002  max mem: 2598
[13:56:44.607224] Epoch: [22]  [180/195]  eta: 0:00:01  lr: 0.000287  loss: 0.2263 (0.2306)  time: 0.0715  data: 0.0002  max mem: 2598
[13:56:45.572669] Epoch: [22]  [194/195]  eta: 0:00:00  lr: 0.000287  loss: 0.2255 (0.2303)  time: 0.0696  data: 0.0001  max mem: 2598
[13:56:45.618681] Epoch: [22] Total time: 0:00:14 (0.0729 s / it)
[13:56:45.618756] Averaged stats: lr: 0.000287  loss: 0.2255 (0.2303)
[13:56:45.621280] log_dir: ./output_dir/mae0.3
[13:56:45.975083] Epoch: [23]  [  0/195]  eta: 0:01:08  lr: 0.000287  loss: 0.2349 (0.2349)  time: 0.3525  data: 0.2710  max mem: 2598
[13:56:47.419658] Epoch: [23]  [ 20/195]  eta: 0:00:14  lr: 0.000289  loss: 0.2233 (0.2243)  time: 0.0722  data: 0.0002  max mem: 2598
[13:56:48.877053] Epoch: [23]  [ 40/195]  eta: 0:00:12  lr: 0.000290  loss: 0.2237 (0.2247)  time: 0.0728  data: 0.0002  max mem: 2598
[13:56:50.302748] Epoch: [23]  [ 60/195]  eta: 0:00:10  lr: 0.000291  loss: 0.2251 (0.2248)  time: 0.0712  data: 0.0001  max mem: 2598
[13:56:51.666869] Epoch: [23]  [ 80/195]  eta: 0:00:08  lr: 0.000293  loss: 0.2237 (0.2249)  time: 0.0682  data: 0.0001  max mem: 2598
[13:56:53.030431] Epoch: [23]  [100/195]  eta: 0:00:06  lr: 0.000294  loss: 0.2228 (0.2245)  time: 0.0681  data: 0.0001  max mem: 2598
[13:56:54.407455] Epoch: [23]  [120/195]  eta: 0:00:05  lr: 0.000295  loss: 0.2238 (0.2242)  time: 0.0688  data: 0.0001  max mem: 2598
[13:56:55.778626] Epoch: [23]  [140/195]  eta: 0:00:03  lr: 0.000296  loss: 0.2229 (0.2239)  time: 0.0685  data: 0.0001  max mem: 2598
[13:56:57.142359] Epoch: [23]  [160/195]  eta: 0:00:02  lr: 0.000298  loss: 0.2247 (0.2241)  time: 0.0682  data: 0.0001  max mem: 2598
[13:56:58.500502] Epoch: [23]  [180/195]  eta: 0:00:01  lr: 0.000299  loss: 0.2225 (0.2239)  time: 0.0679  data: 0.0001  max mem: 2598
[13:56:59.448793] Epoch: [23]  [194/195]  eta: 0:00:00  lr: 0.000300  loss: 0.2209 (0.2238)  time: 0.0677  data: 0.0001  max mem: 2598
[13:56:59.496601] Epoch: [23] Total time: 0:00:13 (0.0712 s / it)
[13:56:59.496668] Averaged stats: lr: 0.000300  loss: 0.2209 (0.2238)
[13:56:59.498933] log_dir: ./output_dir/mae0.3
[13:56:59.921836] Epoch: [24]  [  0/195]  eta: 0:01:22  lr: 0.000300  loss: 0.2274 (0.2274)  time: 0.4213  data: 0.3053  max mem: 2598
[13:57:01.339153] Epoch: [24]  [ 20/195]  eta: 0:00:15  lr: 0.000301  loss: 0.2198 (0.2196)  time: 0.0708  data: 0.0001  max mem: 2598
[13:57:02.719025] Epoch: [24]  [ 40/195]  eta: 0:00:12  lr: 0.000303  loss: 0.2234 (0.2205)  time: 0.0690  data: 0.0001  max mem: 2598
[13:57:04.090673] Epoch: [24]  [ 60/195]  eta: 0:00:10  lr: 0.000304  loss: 0.2211 (0.2204)  time: 0.0686  data: 0.0001  max mem: 2598
[13:57:05.459520] Epoch: [24]  [ 80/195]  eta: 0:00:08  lr: 0.000305  loss: 0.2182 (0.2203)  time: 0.0684  data: 0.0001  max mem: 2598
[13:57:06.826964] Epoch: [24]  [100/195]  eta: 0:00:06  lr: 0.000306  loss: 0.2168 (0.2199)  time: 0.0683  data: 0.0001  max mem: 2598
[13:57:08.197904] Epoch: [24]  [120/195]  eta: 0:00:05  lr: 0.000308  loss: 0.2202 (0.2201)  time: 0.0685  data: 0.0001  max mem: 2598
[13:57:09.582666] Epoch: [24]  [140/195]  eta: 0:00:03  lr: 0.000309  loss: 0.2159 (0.2197)  time: 0.0692  data: 0.0001  max mem: 2598
[13:57:10.962850] Epoch: [24]  [160/195]  eta: 0:00:02  lr: 0.000310  loss: 0.2159 (0.2193)  time: 0.0690  data: 0.0001  max mem: 2598
[13:57:12.332646] Epoch: [24]  [180/195]  eta: 0:00:01  lr: 0.000312  loss: 0.2144 (0.2190)  time: 0.0685  data: 0.0001  max mem: 2598
[13:57:13.282371] Epoch: [24]  [194/195]  eta: 0:00:00  lr: 0.000312  loss: 0.2154 (0.2187)  time: 0.0679  data: 0.0001  max mem: 2598
[13:57:13.331612] Epoch: [24] Total time: 0:00:13 (0.0709 s / it)
[13:57:13.331676] Averaged stats: lr: 0.000312  loss: 0.2154 (0.2187)
[13:57:13.333947] log_dir: ./output_dir/mae0.3
[13:57:13.796625] Epoch: [25]  [  0/195]  eta: 0:01:29  lr: 0.000313  loss: 0.2225 (0.2225)  time: 0.4613  data: 0.3291  max mem: 2598
[13:57:15.267578] Epoch: [25]  [ 20/195]  eta: 0:00:16  lr: 0.000314  loss: 0.2149 (0.2148)  time: 0.0735  data: 0.0001  max mem: 2598
[13:57:16.701511] Epoch: [25]  [ 40/195]  eta: 0:00:12  lr: 0.000315  loss: 0.2143 (0.2145)  time: 0.0717  data: 0.0001  max mem: 2598
[13:57:18.123848] Epoch: [25]  [ 60/195]  eta: 0:00:10  lr: 0.000316  loss: 0.2147 (0.2145)  time: 0.0711  data: 0.0001  max mem: 2598
[13:57:19.517026] Epoch: [25]  [ 80/195]  eta: 0:00:08  lr: 0.000318  loss: 0.2110 (0.2140)  time: 0.0696  data: 0.0001  max mem: 2598
[13:57:20.906120] Epoch: [25]  [100/195]  eta: 0:00:07  lr: 0.000319  loss: 0.2126 (0.2140)  time: 0.0694  data: 0.0001  max mem: 2598
[13:57:22.295197] Epoch: [25]  [120/195]  eta: 0:00:05  lr: 0.000320  loss: 0.2148 (0.2142)  time: 0.0694  data: 0.0001  max mem: 2598
[13:57:23.683383] Epoch: [25]  [140/195]  eta: 0:00:04  lr: 0.000321  loss: 0.2092 (0.2138)  time: 0.0694  data: 0.0001  max mem: 2598
[13:57:25.072931] Epoch: [25]  [160/195]  eta: 0:00:02  lr: 0.000323  loss: 0.2138 (0.2137)  time: 0.0694  data: 0.0001  max mem: 2598
[13:57:26.446378] Epoch: [25]  [180/195]  eta: 0:00:01  lr: 0.000324  loss: 0.2121 (0.2133)  time: 0.0686  data: 0.0001  max mem: 2598
[13:57:27.396804] Epoch: [25]  [194/195]  eta: 0:00:00  lr: 0.000325  loss: 0.2086 (0.2133)  time: 0.0679  data: 0.0001  max mem: 2598
[13:57:27.446682] Epoch: [25] Total time: 0:00:14 (0.0724 s / it)
[13:57:27.446755] Averaged stats: lr: 0.000325  loss: 0.2086 (0.2133)
[13:57:27.449451] log_dir: ./output_dir/mae0.3
[13:57:27.798657] Epoch: [26]  [  0/195]  eta: 0:01:07  lr: 0.000325  loss: 0.2116 (0.2116)  time: 0.3479  data: 0.2717  max mem: 2598
[13:57:29.222503] Epoch: [26]  [ 20/195]  eta: 0:00:14  lr: 0.000326  loss: 0.2072 (0.2081)  time: 0.0712  data: 0.0001  max mem: 2598
[13:57:30.632375] Epoch: [26]  [ 40/195]  eta: 0:00:12  lr: 0.000328  loss: 0.2097 (0.2093)  time: 0.0704  data: 0.0001  max mem: 2598
[13:57:32.044735] Epoch: [26]  [ 60/195]  eta: 0:00:10  lr: 0.000329  loss: 0.2083 (0.2095)  time: 0.0706  data: 0.0001  max mem: 2598
[13:57:33.452686] Epoch: [26]  [ 80/195]  eta: 0:00:08  lr: 0.000330  loss: 0.2082 (0.2095)  time: 0.0704  data: 0.0001  max mem: 2598
[13:57:34.857845] Epoch: [26]  [100/195]  eta: 0:00:06  lr: 0.000331  loss: 0.2094 (0.2096)  time: 0.0702  data: 0.0001  max mem: 2598
[13:57:36.292838] Epoch: [26]  [120/195]  eta: 0:00:05  lr: 0.000333  loss: 0.2084 (0.2094)  time: 0.0717  data: 0.0001  max mem: 2598
[13:57:37.706686] Epoch: [26]  [140/195]  eta: 0:00:03  lr: 0.000334  loss: 0.2105 (0.2094)  time: 0.0706  data: 0.0001  max mem: 2598
[13:57:39.118896] Epoch: [26]  [160/195]  eta: 0:00:02  lr: 0.000335  loss: 0.2095 (0.2094)  time: 0.0706  data: 0.0001  max mem: 2598
[13:57:40.530521] Epoch: [26]  [180/195]  eta: 0:00:01  lr: 0.000337  loss: 0.2044 (0.2089)  time: 0.0705  data: 0.0002  max mem: 2598
[13:57:41.498447] Epoch: [26]  [194/195]  eta: 0:00:00  lr: 0.000337  loss: 0.2081 (0.2088)  time: 0.0695  data: 0.0001  max mem: 2598
[13:57:41.568198] Epoch: [26] Total time: 0:00:14 (0.0724 s / it)
[13:57:41.568272] Averaged stats: lr: 0.000337  loss: 0.2081 (0.2088)
[13:57:41.570789] log_dir: ./output_dir/mae0.3
[13:57:42.011809] Epoch: [27]  [  0/195]  eta: 0:01:25  lr: 0.000338  loss: 0.2101 (0.2101)  time: 0.4394  data: 0.3062  max mem: 2598
[13:57:43.475466] Epoch: [27]  [ 20/195]  eta: 0:00:15  lr: 0.000339  loss: 0.2054 (0.2060)  time: 0.0731  data: 0.0002  max mem: 2598
[13:57:44.878417] Epoch: [27]  [ 40/195]  eta: 0:00:12  lr: 0.000340  loss: 0.2023 (0.2055)  time: 0.0701  data: 0.0001  max mem: 2598
[13:57:46.294028] Epoch: [27]  [ 60/195]  eta: 0:00:10  lr: 0.000341  loss: 0.2059 (0.2055)  time: 0.0707  data: 0.0001  max mem: 2598
[13:57:47.710810] Epoch: [27]  [ 80/195]  eta: 0:00:08  lr: 0.000343  loss: 0.2035 (0.2053)  time: 0.0708  data: 0.0002  max mem: 2598
[13:57:49.115390] Epoch: [27]  [100/195]  eta: 0:00:07  lr: 0.000344  loss: 0.2027 (0.2051)  time: 0.0702  data: 0.0001  max mem: 2598
[13:57:50.515827] Epoch: [27]  [120/195]  eta: 0:00:05  lr: 0.000345  loss: 0.2058 (0.2052)  time: 0.0700  data: 0.0002  max mem: 2598
[13:57:51.918664] Epoch: [27]  [140/195]  eta: 0:00:04  lr: 0.000346  loss: 0.2023 (0.2049)  time: 0.0701  data: 0.0002  max mem: 2598
[13:57:53.325297] Epoch: [27]  [160/195]  eta: 0:00:02  lr: 0.000348  loss: 0.2039 (0.2051)  time: 0.0703  data: 0.0001  max mem: 2598
[13:57:54.711018] Epoch: [27]  [180/195]  eta: 0:00:01  lr: 0.000349  loss: 0.2067 (0.2050)  time: 0.0692  data: 0.0001  max mem: 2598
[13:57:55.657514] Epoch: [27]  [194/195]  eta: 0:00:00  lr: 0.000350  loss: 0.2042 (0.2052)  time: 0.0677  data: 0.0001  max mem: 2598
[13:57:55.702308] Epoch: [27] Total time: 0:00:14 (0.0725 s / it)
[13:57:55.702389] Averaged stats: lr: 0.000350  loss: 0.2042 (0.2052)
[13:57:55.704892] log_dir: ./output_dir/mae0.3
[13:57:56.087373] Epoch: [28]  [  0/195]  eta: 0:01:14  lr: 0.000350  loss: 0.2103 (0.2103)  time: 0.3816  data: 0.3014  max mem: 2598
[13:57:57.483314] Epoch: [28]  [ 20/195]  eta: 0:00:14  lr: 0.000351  loss: 0.2021 (0.2022)  time: 0.0698  data: 0.0001  max mem: 2598
[13:57:58.859493] Epoch: [28]  [ 40/195]  eta: 0:00:11  lr: 0.000353  loss: 0.1991 (0.2010)  time: 0.0688  data: 0.0001  max mem: 2598
[13:58:00.258819] Epoch: [28]  [ 60/195]  eta: 0:00:10  lr: 0.000354  loss: 0.1996 (0.2010)  time: 0.0699  data: 0.0001  max mem: 2598
[13:58:01.652395] Epoch: [28]  [ 80/195]  eta: 0:00:08  lr: 0.000355  loss: 0.2019 (0.2015)  time: 0.0696  data: 0.0001  max mem: 2598
[13:58:03.028863] Epoch: [28]  [100/195]  eta: 0:00:06  lr: 0.000356  loss: 0.2025 (0.2018)  time: 0.0688  data: 0.0001  max mem: 2598
[13:58:04.422855] Epoch: [28]  [120/195]  eta: 0:00:05  lr: 0.000358  loss: 0.2015 (0.2017)  time: 0.0697  data: 0.0001  max mem: 2598
[13:58:05.798367] Epoch: [28]  [140/195]  eta: 0:00:03  lr: 0.000359  loss: 0.2029 (0.2019)  time: 0.0687  data: 0.0001  max mem: 2598
[13:58:07.169072] Epoch: [28]  [160/195]  eta: 0:00:02  lr: 0.000360  loss: 0.2024 (0.2017)  time: 0.0685  data: 0.0001  max mem: 2598
[13:58:08.548727] Epoch: [28]  [180/195]  eta: 0:00:01  lr: 0.000362  loss: 0.2004 (0.2016)  time: 0.0689  data: 0.0001  max mem: 2598
[13:58:09.518413] Epoch: [28]  [194/195]  eta: 0:00:00  lr: 0.000362  loss: 0.2004 (0.2015)  time: 0.0692  data: 0.0001  max mem: 2598
[13:58:09.589247] Epoch: [28] Total time: 0:00:13 (0.0712 s / it)
[13:58:09.589316] Averaged stats: lr: 0.000362  loss: 0.2004 (0.2015)
[13:58:09.591636] log_dir: ./output_dir/mae0.3
[13:58:09.967397] Epoch: [29]  [  0/195]  eta: 0:01:13  lr: 0.000363  loss: 0.1976 (0.1976)  time: 0.3747  data: 0.2881  max mem: 2598
[13:58:11.378694] Epoch: [29]  [ 20/195]  eta: 0:00:14  lr: 0.000364  loss: 0.1940 (0.1978)  time: 0.0705  data: 0.0001  max mem: 2598
[13:58:12.770332] Epoch: [29]  [ 40/195]  eta: 0:00:12  lr: 0.000365  loss: 0.1943 (0.1975)  time: 0.0695  data: 0.0001  max mem: 2598
[13:58:14.168545] Epoch: [29]  [ 60/195]  eta: 0:00:10  lr: 0.000366  loss: 0.2007 (0.1982)  time: 0.0699  data: 0.0001  max mem: 2598
[13:58:15.568930] Epoch: [29]  [ 80/195]  eta: 0:00:08  lr: 0.000368  loss: 0.1984 (0.1985)  time: 0.0700  data: 0.0001  max mem: 2598
[13:58:16.962584] Epoch: [29]  [100/195]  eta: 0:00:06  lr: 0.000369  loss: 0.1970 (0.1982)  time: 0.0696  data: 0.0001  max mem: 2598
[13:58:18.356661] Epoch: [29]  [120/195]  eta: 0:00:05  lr: 0.000370  loss: 0.1989 (0.1985)  time: 0.0697  data: 0.0001  max mem: 2598
[13:58:19.777207] Epoch: [29]  [140/195]  eta: 0:00:03  lr: 0.000371  loss: 0.1962 (0.1985)  time: 0.0710  data: 0.0001  max mem: 2598
[13:58:21.188399] Epoch: [29]  [160/195]  eta: 0:00:02  lr: 0.000373  loss: 0.1950 (0.1982)  time: 0.0705  data: 0.0001  max mem: 2598
[13:58:22.587995] Epoch: [29]  [180/195]  eta: 0:00:01  lr: 0.000374  loss: 0.1939 (0.1978)  time: 0.0699  data: 0.0002  max mem: 2598
[13:58:23.560027] Epoch: [29]  [194/195]  eta: 0:00:00  lr: 0.000375  loss: 0.1958 (0.1976)  time: 0.0694  data: 0.0001  max mem: 2598
[13:58:23.612879] Epoch: [29] Total time: 0:00:14 (0.0719 s / it)
[13:58:23.612952] Averaged stats: lr: 0.000375  loss: 0.1958 (0.1976)
[13:58:23.615523] log_dir: ./output_dir/mae0.3
[13:58:23.985181] Epoch: [30]  [  0/195]  eta: 0:01:11  lr: 0.000375  loss: 0.2054 (0.2054)  time: 0.3680  data: 0.2814  max mem: 2598
[13:58:25.398377] Epoch: [30]  [ 20/195]  eta: 0:00:14  lr: 0.000376  loss: 0.1942 (0.1944)  time: 0.0706  data: 0.0001  max mem: 2598
[13:58:26.797617] Epoch: [30]  [ 40/195]  eta: 0:00:12  lr: 0.000378  loss: 0.1957 (0.1957)  time: 0.0699  data: 0.0001  max mem: 2598
[13:58:28.211050] Epoch: [30]  [ 60/195]  eta: 0:00:10  lr: 0.000379  loss: 0.1946 (0.1961)  time: 0.0706  data: 0.0001  max mem: 2598
[13:58:29.622404] Epoch: [30]  [ 80/195]  eta: 0:00:08  lr: 0.000380  loss: 0.1985 (0.1963)  time: 0.0705  data: 0.0002  max mem: 2598
[13:58:31.035849] Epoch: [30]  [100/195]  eta: 0:00:06  lr: 0.000381  loss: 0.1947 (0.1961)  time: 0.0706  data: 0.0002  max mem: 2598
[13:58:32.445285] Epoch: [30]  [120/195]  eta: 0:00:05  lr: 0.000383  loss: 0.1955 (0.1958)  time: 0.0704  data: 0.0002  max mem: 2598
[13:58:33.850946] Epoch: [30]  [140/195]  eta: 0:00:03  lr: 0.000384  loss: 0.1952 (0.1959)  time: 0.0702  data: 0.0001  max mem: 2598
[13:58:35.261348] Epoch: [30]  [160/195]  eta: 0:00:02  lr: 0.000385  loss: 0.1965 (0.1961)  time: 0.0705  data: 0.0002  max mem: 2598
[13:58:36.662645] Epoch: [30]  [180/195]  eta: 0:00:01  lr: 0.000387  loss: 0.1939 (0.1960)  time: 0.0700  data: 0.0002  max mem: 2598
[13:58:37.628674] Epoch: [30]  [194/195]  eta: 0:00:00  lr: 0.000387  loss: 0.1927 (0.1959)  time: 0.0689  data: 0.0001  max mem: 2598
[13:58:37.700585] Epoch: [30] Total time: 0:00:14 (0.0722 s / it)
[13:58:37.700719] Averaged stats: lr: 0.000387  loss: 0.1927 (0.1959)
[13:58:37.706373] log_dir: ./output_dir/mae0.3
[13:58:38.144996] Epoch: [31]  [  0/195]  eta: 0:01:25  lr: 0.000387  loss: 0.1930 (0.1930)  time: 0.4371  data: 0.3052  max mem: 2598
[13:58:39.595673] Epoch: [31]  [ 20/195]  eta: 0:00:15  lr: 0.000389  loss: 0.1885 (0.1898)  time: 0.0725  data: 0.0002  max mem: 2598
[13:58:41.008125] Epoch: [31]  [ 40/195]  eta: 0:00:12  lr: 0.000390  loss: 0.1924 (0.1914)  time: 0.0706  data: 0.0001  max mem: 2598
[13:58:42.427024] Epoch: [31]  [ 60/195]  eta: 0:00:10  lr: 0.000391  loss: 0.1904 (0.1917)  time: 0.0709  data: 0.0001  max mem: 2598
[13:58:43.841648] Epoch: [31]  [ 80/195]  eta: 0:00:08  lr: 0.000393  loss: 0.1966 (0.1928)  time: 0.0707  data: 0.0001  max mem: 2598
[13:58:45.251051] Epoch: [31]  [100/195]  eta: 0:00:07  lr: 0.000394  loss: 0.1951 (0.1934)  time: 0.0704  data: 0.0001  max mem: 2598
[13:58:46.657865] Epoch: [31]  [120/195]  eta: 0:00:05  lr: 0.000395  loss: 0.1943 (0.1936)  time: 0.0703  data: 0.0001  max mem: 2598
[13:58:48.068902] Epoch: [31]  [140/195]  eta: 0:00:04  lr: 0.000396  loss: 0.1914 (0.1931)  time: 0.0705  data: 0.0001  max mem: 2598
[13:58:49.480106] Epoch: [31]  [160/195]  eta: 0:00:02  lr: 0.000398  loss: 0.1927 (0.1931)  time: 0.0705  data: 0.0001  max mem: 2598
[13:58:50.892453] Epoch: [31]  [180/195]  eta: 0:00:01  lr: 0.000399  loss: 0.1925 (0.1930)  time: 0.0706  data: 0.0001  max mem: 2598
[13:58:51.868557] Epoch: [31]  [194/195]  eta: 0:00:00  lr: 0.000400  loss: 0.1918 (0.1929)  time: 0.0698  data: 0.0001  max mem: 2598
[13:58:51.920956] Epoch: [31] Total time: 0:00:14 (0.0729 s / it)
[13:58:51.921031] Averaged stats: lr: 0.000400  loss: 0.1918 (0.1929)
[13:58:51.923530] log_dir: ./output_dir/mae0.3
[13:58:52.325007] Epoch: [32]  [  0/195]  eta: 0:01:18  lr: 0.000400  loss: 0.1929 (0.1929)  time: 0.4003  data: 0.3026  max mem: 2598
[13:58:53.773540] Epoch: [32]  [ 20/195]  eta: 0:00:15  lr: 0.000401  loss: 0.1893 (0.1904)  time: 0.0724  data: 0.0001  max mem: 2598
[13:58:55.198536] Epoch: [32]  [ 40/195]  eta: 0:00:12  lr: 0.000403  loss: 0.1880 (0.1905)  time: 0.0712  data: 0.0002  max mem: 2598
[13:58:56.610314] Epoch: [32]  [ 60/195]  eta: 0:00:10  lr: 0.000404  loss: 0.1886 (0.1904)  time: 0.0705  data: 0.0001  max mem: 2598
[13:58:58.019037] Epoch: [32]  [ 80/195]  eta: 0:00:08  lr: 0.000405  loss: 0.1901 (0.1907)  time: 0.0704  data: 0.0001  max mem: 2598
[13:58:59.443013] Epoch: [32]  [100/195]  eta: 0:00:07  lr: 0.000406  loss: 0.1892 (0.1906)  time: 0.0712  data: 0.0001  max mem: 2598
[13:59:00.847095] Epoch: [32]  [120/195]  eta: 0:00:05  lr: 0.000408  loss: 0.1875 (0.1906)  time: 0.0702  data: 0.0001  max mem: 2598
[13:59:02.252472] Epoch: [32]  [140/195]  eta: 0:00:04  lr: 0.000409  loss: 0.1907 (0.1907)  time: 0.0702  data: 0.0001  max mem: 2598
[13:59:03.662598] Epoch: [32]  [160/195]  eta: 0:00:02  lr: 0.000410  loss: 0.1880 (0.1908)  time: 0.0705  data: 0.0001  max mem: 2598
[13:59:05.067700] Epoch: [32]  [180/195]  eta: 0:00:01  lr: 0.000412  loss: 0.1908 (0.1909)  time: 0.0702  data: 0.0001  max mem: 2598
[13:59:06.038729] Epoch: [32]  [194/195]  eta: 0:00:00  lr: 0.000412  loss: 0.1884 (0.1908)  time: 0.0693  data: 0.0001  max mem: 2598
[13:59:06.110655] Epoch: [32] Total time: 0:00:14 (0.0728 s / it)
[13:59:06.110731] Averaged stats: lr: 0.000412  loss: 0.1884 (0.1908)
[13:59:06.113225] log_dir: ./output_dir/mae0.3
[13:59:06.524905] Epoch: [33]  [  0/195]  eta: 0:01:20  lr: 0.000412  loss: 0.1944 (0.1944)  time: 0.4104  data: 0.3257  max mem: 2598
[13:59:07.907866] Epoch: [33]  [ 20/195]  eta: 0:00:14  lr: 0.000414  loss: 0.1887 (0.1890)  time: 0.0691  data: 0.0001  max mem: 2598
[13:59:09.307032] Epoch: [33]  [ 40/195]  eta: 0:00:12  lr: 0.000415  loss: 0.1876 (0.1892)  time: 0.0699  data: 0.0001  max mem: 2598
[13:59:10.721970] Epoch: [33]  [ 60/195]  eta: 0:00:10  lr: 0.000416  loss: 0.1890 (0.1891)  time: 0.0707  data: 0.0002  max mem: 2598
[13:59:12.131677] Epoch: [33]  [ 80/195]  eta: 0:00:08  lr: 0.000418  loss: 0.1888 (0.1890)  time: 0.0704  data: 0.0001  max mem: 2598
[13:59:13.542859] Epoch: [33]  [100/195]  eta: 0:00:06  lr: 0.000419  loss: 0.1908 (0.1894)  time: 0.0705  data: 0.0002  max mem: 2598
[13:59:14.953764] Epoch: [33]  [120/195]  eta: 0:00:05  lr: 0.000420  loss: 0.1910 (0.1897)  time: 0.0705  data: 0.0001  max mem: 2598
[13:59:16.375416] Epoch: [33]  [140/195]  eta: 0:00:04  lr: 0.000421  loss: 0.1889 (0.1899)  time: 0.0710  data: 0.0001  max mem: 2598
[13:59:17.788531] Epoch: [33]  [160/195]  eta: 0:00:02  lr: 0.000423  loss: 0.1896 (0.1898)  time: 0.0706  data: 0.0002  max mem: 2598
[13:59:19.193293] Epoch: [33]  [180/195]  eta: 0:00:01  lr: 0.000424  loss: 0.1872 (0.1897)  time: 0.0702  data: 0.0001  max mem: 2598
[13:59:20.162899] Epoch: [33]  [194/195]  eta: 0:00:00  lr: 0.000425  loss: 0.1871 (0.1896)  time: 0.0693  data: 0.0001  max mem: 2598
[13:59:20.232375] Epoch: [33] Total time: 0:00:14 (0.0724 s / it)
[13:59:20.232453] Averaged stats: lr: 0.000425  loss: 0.1871 (0.1896)
[13:59:20.235372] log_dir: ./output_dir/mae0.3
[13:59:20.609913] Epoch: [34]  [  0/195]  eta: 0:01:12  lr: 0.000425  loss: 0.1888 (0.1888)  time: 0.3730  data: 0.2990  max mem: 2598
[13:59:22.020065] Epoch: [34]  [ 20/195]  eta: 0:00:14  lr: 0.000426  loss: 0.1845 (0.1868)  time: 0.0705  data: 0.0001  max mem: 2598
[13:59:23.418865] Epoch: [34]  [ 40/195]  eta: 0:00:12  lr: 0.000428  loss: 0.1851 (0.1870)  time: 0.0699  data: 0.0001  max mem: 2598
[13:59:24.820625] Epoch: [34]  [ 60/195]  eta: 0:00:10  lr: 0.000429  loss: 0.1889 (0.1875)  time: 0.0700  data: 0.0001  max mem: 2598
[13:59:26.243643] Epoch: [34]  [ 80/195]  eta: 0:00:08  lr: 0.000430  loss: 0.1906 (0.1878)  time: 0.0711  data: 0.0001  max mem: 2598
[13:59:27.647068] Epoch: [34]  [100/195]  eta: 0:00:06  lr: 0.000431  loss: 0.1865 (0.1876)  time: 0.0701  data: 0.0001  max mem: 2598
[13:59:29.092933] Epoch: [34]  [120/195]  eta: 0:00:05  lr: 0.000433  loss: 0.1858 (0.1876)  time: 0.0722  data: 0.0001  max mem: 2598
[13:59:30.533888] Epoch: [34]  [140/195]  eta: 0:00:04  lr: 0.000434  loss: 0.1880 (0.1876)  time: 0.0720  data: 0.0002  max mem: 2598
[13:59:31.944093] Epoch: [34]  [160/195]  eta: 0:00:02  lr: 0.000435  loss: 0.1883 (0.1877)  time: 0.0705  data: 0.0001  max mem: 2598
[13:59:33.343374] Epoch: [34]  [180/195]  eta: 0:00:01  lr: 0.000437  loss: 0.1855 (0.1876)  time: 0.0699  data: 0.0002  max mem: 2598
[13:59:34.320137] Epoch: [34]  [194/195]  eta: 0:00:00  lr: 0.000437  loss: 0.1857 (0.1875)  time: 0.0696  data: 0.0001  max mem: 2598
[13:59:34.390596] Epoch: [34] Total time: 0:00:14 (0.0726 s / it)
[13:59:34.390792] Averaged stats: lr: 0.000437  loss: 0.1857 (0.1875)
[13:59:34.393306] log_dir: ./output_dir/mae0.3
[13:59:34.828712] Epoch: [35]  [  0/195]  eta: 0:01:24  lr: 0.000438  loss: 0.1943 (0.1943)  time: 0.4337  data: 0.3151  max mem: 2598
[13:59:36.267931] Epoch: [35]  [ 20/195]  eta: 0:00:15  lr: 0.000439  loss: 0.1840 (0.1848)  time: 0.0719  data: 0.0002  max mem: 2598
[13:59:37.683125] Epoch: [35]  [ 40/195]  eta: 0:00:12  lr: 0.000440  loss: 0.1864 (0.1845)  time: 0.0707  data: 0.0001  max mem: 2598
[13:59:39.098053] Epoch: [35]  [ 60/195]  eta: 0:00:10  lr: 0.000441  loss: 0.1868 (0.1852)  time: 0.0707  data: 0.0001  max mem: 2598
[13:59:40.509604] Epoch: [35]  [ 80/195]  eta: 0:00:08  lr: 0.000443  loss: 0.1873 (0.1857)  time: 0.0705  data: 0.0001  max mem: 2598
[13:59:41.924512] Epoch: [35]  [100/195]  eta: 0:00:07  lr: 0.000444  loss: 0.1840 (0.1856)  time: 0.0707  data: 0.0001  max mem: 2598
[13:59:43.341571] Epoch: [35]  [120/195]  eta: 0:00:05  lr: 0.000445  loss: 0.1871 (0.1859)  time: 0.0708  data: 0.0002  max mem: 2598
[13:59:44.747434] Epoch: [35]  [140/195]  eta: 0:00:04  lr: 0.000446  loss: 0.1859 (0.1860)  time: 0.0702  data: 0.0002  max mem: 2598
[13:59:46.146352] Epoch: [35]  [160/195]  eta: 0:00:02  lr: 0.000448  loss: 0.1851 (0.1859)  time: 0.0699  data: 0.0002  max mem: 2598
[13:59:47.543462] Epoch: [35]  [180/195]  eta: 0:00:01  lr: 0.000449  loss: 0.1852 (0.1860)  time: 0.0698  data: 0.0001  max mem: 2598
[13:59:48.510473] Epoch: [35]  [194/195]  eta: 0:00:00  lr: 0.000450  loss: 0.1829 (0.1859)  time: 0.0691  data: 0.0001  max mem: 2598
[13:59:48.569268] Epoch: [35] Total time: 0:00:14 (0.0727 s / it)
[13:59:48.569338] Averaged stats: lr: 0.000450  loss: 0.1829 (0.1859)
[13:59:48.571845] log_dir: ./output_dir/mae0.3
[13:59:48.984649] Epoch: [36]  [  0/195]  eta: 0:01:20  lr: 0.000450  loss: 0.1852 (0.1852)  time: 0.4118  data: 0.3264  max mem: 2598
[13:59:50.399184] Epoch: [36]  [ 20/195]  eta: 0:00:15  lr: 0.000451  loss: 0.1818 (0.1826)  time: 0.0707  data: 0.0001  max mem: 2598
[13:59:51.818547] Epoch: [36]  [ 40/195]  eta: 0:00:12  lr: 0.000453  loss: 0.1825 (0.1830)  time: 0.0709  data: 0.0002  max mem: 2598
[13:59:53.229625] Epoch: [36]  [ 60/195]  eta: 0:00:10  lr: 0.000454  loss: 0.1829 (0.1836)  time: 0.0705  data: 0.0001  max mem: 2598
[13:59:54.636376] Epoch: [36]  [ 80/195]  eta: 0:00:08  lr: 0.000455  loss: 0.1852 (0.1843)  time: 0.0703  data: 0.0001  max mem: 2598
[13:59:56.041962] Epoch: [36]  [100/195]  eta: 0:00:07  lr: 0.000456  loss: 0.1860 (0.1845)  time: 0.0702  data: 0.0001  max mem: 2598
[13:59:57.448519] Epoch: [36]  [120/195]  eta: 0:00:05  lr: 0.000458  loss: 0.1852 (0.1847)  time: 0.0703  data: 0.0001  max mem: 2598
[13:59:58.853927] Epoch: [36]  [140/195]  eta: 0:00:04  lr: 0.000459  loss: 0.1845 (0.1847)  time: 0.0702  data: 0.0001  max mem: 2598
[14:00:00.267563] Epoch: [36]  [160/195]  eta: 0:00:02  lr: 0.000460  loss: 0.1857 (0.1848)  time: 0.0706  data: 0.0001  max mem: 2598
[14:00:01.671126] Epoch: [36]  [180/195]  eta: 0:00:01  lr: 0.000462  loss: 0.1885 (0.1849)  time: 0.0701  data: 0.0002  max mem: 2598
[14:00:02.636891] Epoch: [36]  [194/195]  eta: 0:00:00  lr: 0.000462  loss: 0.1841 (0.1848)  time: 0.0690  data: 0.0001  max mem: 2598
[14:00:02.681572] Epoch: [36] Total time: 0:00:14 (0.0724 s / it)
[14:00:02.681641] Averaged stats: lr: 0.000462  loss: 0.1841 (0.1848)
[14:00:02.684152] log_dir: ./output_dir/mae0.3
[14:00:03.099734] Epoch: [37]  [  0/195]  eta: 0:01:20  lr: 0.000462  loss: 0.1826 (0.1826)  time: 0.4143  data: 0.3266  max mem: 2598
[14:00:04.511619] Epoch: [37]  [ 20/195]  eta: 0:00:15  lr: 0.000464  loss: 0.1844 (0.1823)  time: 0.0706  data: 0.0002  max mem: 2598
[14:00:05.889182] Epoch: [37]  [ 40/195]  eta: 0:00:12  lr: 0.000465  loss: 0.1775 (0.1814)  time: 0.0688  data: 0.0001  max mem: 2598
[14:00:07.270098] Epoch: [37]  [ 60/195]  eta: 0:00:10  lr: 0.000466  loss: 0.1836 (0.1825)  time: 0.0690  data: 0.0001  max mem: 2598
[14:00:08.654240] Epoch: [37]  [ 80/195]  eta: 0:00:08  lr: 0.000468  loss: 0.1849 (0.1829)  time: 0.0692  data: 0.0001  max mem: 2598
[14:00:10.048653] Epoch: [37]  [100/195]  eta: 0:00:06  lr: 0.000469  loss: 0.1815 (0.1829)  time: 0.0697  data: 0.0001  max mem: 2598
[14:00:11.471149] Epoch: [37]  [120/195]  eta: 0:00:05  lr: 0.000470  loss: 0.1816 (0.1827)  time: 0.0711  data: 0.0002  max mem: 2598
[14:00:12.898131] Epoch: [37]  [140/195]  eta: 0:00:03  lr: 0.000471  loss: 0.1808 (0.1826)  time: 0.0713  data: 0.0002  max mem: 2598
[14:00:14.301961] Epoch: [37]  [160/195]  eta: 0:00:02  lr: 0.000473  loss: 0.1829 (0.1828)  time: 0.0702  data: 0.0001  max mem: 2598
[14:00:15.716847] Epoch: [37]  [180/195]  eta: 0:00:01  lr: 0.000474  loss: 0.1833 (0.1827)  time: 0.0707  data: 0.0002  max mem: 2598
[14:00:16.683805] Epoch: [37]  [194/195]  eta: 0:00:00  lr: 0.000475  loss: 0.1834 (0.1828)  time: 0.0691  data: 0.0001  max mem: 2598
[14:00:16.734961] Epoch: [37] Total time: 0:00:14 (0.0721 s / it)
[14:00:16.735030] Averaged stats: lr: 0.000475  loss: 0.1834 (0.1828)
[14:00:16.737551] log_dir: ./output_dir/mae0.3
[14:00:17.149034] Epoch: [38]  [  0/195]  eta: 0:01:19  lr: 0.000475  loss: 0.1842 (0.1842)  time: 0.4099  data: 0.2890  max mem: 2598
[14:00:18.557680] Epoch: [38]  [ 20/195]  eta: 0:00:15  lr: 0.000476  loss: 0.1801 (0.1802)  time: 0.0704  data: 0.0001  max mem: 2598
[14:00:19.943051] Epoch: [38]  [ 40/195]  eta: 0:00:12  lr: 0.000478  loss: 0.1766 (0.1798)  time: 0.0692  data: 0.0001  max mem: 2598
[14:00:21.356724] Epoch: [38]  [ 60/195]  eta: 0:00:10  lr: 0.000479  loss: 0.1825 (0.1809)  time: 0.0706  data: 0.0001  max mem: 2598
[14:00:22.764534] Epoch: [38]  [ 80/195]  eta: 0:00:08  lr: 0.000480  loss: 0.1838 (0.1815)  time: 0.0703  data: 0.0001  max mem: 2598
[14:00:24.146779] Epoch: [38]  [100/195]  eta: 0:00:06  lr: 0.000481  loss: 0.1809 (0.1813)  time: 0.0691  data: 0.0001  max mem: 2598
[14:00:25.512764] Epoch: [38]  [120/195]  eta: 0:00:05  lr: 0.000483  loss: 0.1812 (0.1815)  time: 0.0683  data: 0.0001  max mem: 2598
[14:00:26.874927] Epoch: [38]  [140/195]  eta: 0:00:03  lr: 0.000484  loss: 0.1826 (0.1816)  time: 0.0681  data: 0.0001  max mem: 2598
[14:00:28.239857] Epoch: [38]  [160/195]  eta: 0:00:02  lr: 0.000485  loss: 0.1804 (0.1816)  time: 0.0682  data: 0.0001  max mem: 2598
[14:00:29.600865] Epoch: [38]  [180/195]  eta: 0:00:01  lr: 0.000487  loss: 0.1801 (0.1816)  time: 0.0680  data: 0.0001  max mem: 2598
[14:00:30.550397] Epoch: [38]  [194/195]  eta: 0:00:00  lr: 0.000487  loss: 0.1808 (0.1816)  time: 0.0677  data: 0.0001  max mem: 2598
[14:00:30.621375] Epoch: [38] Total time: 0:00:13 (0.0712 s / it)
[14:00:30.621440] Averaged stats: lr: 0.000487  loss: 0.1808 (0.1816)
[14:00:30.623733] log_dir: ./output_dir/mae0.3
[14:00:31.062117] Epoch: [39]  [  0/195]  eta: 0:01:25  lr: 0.000487  loss: 0.1839 (0.1839)  time: 0.4370  data: 0.3089  max mem: 2598
[14:00:32.558694] Epoch: [39]  [ 20/195]  eta: 0:00:16  lr: 0.000489  loss: 0.1809 (0.1801)  time: 0.0748  data: 0.0002  max mem: 2598
[14:00:33.986996] Epoch: [39]  [ 40/195]  eta: 0:00:12  lr: 0.000490  loss: 0.1774 (0.1797)  time: 0.0714  data: 0.0001  max mem: 2598
[14:00:35.392354] Epoch: [39]  [ 60/195]  eta: 0:00:10  lr: 0.000491  loss: 0.1815 (0.1807)  time: 0.0702  data: 0.0001  max mem: 2598
[14:00:36.806548] Epoch: [39]  [ 80/195]  eta: 0:00:08  lr: 0.000493  loss: 0.1821 (0.1813)  time: 0.0707  data: 0.0001  max mem: 2598
[14:00:38.216823] Epoch: [39]  [100/195]  eta: 0:00:07  lr: 0.000494  loss: 0.1799 (0.1812)  time: 0.0705  data: 0.0001  max mem: 2598
[14:00:39.631117] Epoch: [39]  [120/195]  eta: 0:00:05  lr: 0.000495  loss: 0.1817 (0.1813)  time: 0.0707  data: 0.0001  max mem: 2598
[14:00:41.048848] Epoch: [39]  [140/195]  eta: 0:00:04  lr: 0.000496  loss: 0.1804 (0.1814)  time: 0.0708  data: 0.0001  max mem: 2598
[14:00:42.467097] Epoch: [39]  [160/195]  eta: 0:00:02  lr: 0.000498  loss: 0.1834 (0.1815)  time: 0.0709  data: 0.0001  max mem: 2598
[14:00:43.887647] Epoch: [39]  [180/195]  eta: 0:00:01  lr: 0.000499  loss: 0.1793 (0.1812)  time: 0.0710  data: 0.0001  max mem: 2598
[14:00:44.855888] Epoch: [39]  [194/195]  eta: 0:00:00  lr: 0.000500  loss: 0.1823 (0.1813)  time: 0.0692  data: 0.0001  max mem: 2598
[14:00:44.910970] Epoch: [39] Total time: 0:00:14 (0.0733 s / it)
[14:00:44.911038] Averaged stats: lr: 0.000500  loss: 0.1823 (0.1813)
[14:00:44.913539] log_dir: ./output_dir/mae0.3
[14:00:45.293621] Epoch: [40]  [  0/195]  eta: 0:01:13  lr: 0.000500  loss: 0.1864 (0.1864)  time: 0.3788  data: 0.2900  max mem: 2598
[14:00:46.710972] Epoch: [40]  [ 20/195]  eta: 0:00:14  lr: 0.000500  loss: 0.1762 (0.1776)  time: 0.0708  data: 0.0001  max mem: 2598
[14:00:48.114994] Epoch: [40]  [ 40/195]  eta: 0:00:12  lr: 0.000500  loss: 0.1788 (0.1786)  time: 0.0702  data: 0.0001  max mem: 2598
[14:00:49.519300] Epoch: [40]  [ 60/195]  eta: 0:00:10  lr: 0.000500  loss: 0.1797 (0.1789)  time: 0.0702  data: 0.0001  max mem: 2598
[14:00:50.921942] Epoch: [40]  [ 80/195]  eta: 0:00:08  lr: 0.000500  loss: 0.1786 (0.1792)  time: 0.0701  data: 0.0001  max mem: 2598
[14:00:52.328361] Epoch: [40]  [100/195]  eta: 0:00:06  lr: 0.000500  loss: 0.1801 (0.1791)  time: 0.0703  data: 0.0001  max mem: 2598
[14:00:53.744352] Epoch: [40]  [120/195]  eta: 0:00:05  lr: 0.000500  loss: 0.1817 (0.1795)  time: 0.0707  data: 0.0001  max mem: 2598
[14:00:55.147857] Epoch: [40]  [140/195]  eta: 0:00:03  lr: 0.000500  loss: 0.1789 (0.1792)  time: 0.0701  data: 0.0001  max mem: 2598
[14:00:56.556185] Epoch: [40]  [160/195]  eta: 0:00:02  lr: 0.000500  loss: 0.1781 (0.1793)  time: 0.0703  data: 0.0001  max mem: 2598
[14:00:57.952548] Epoch: [40]  [180/195]  eta: 0:00:01  lr: 0.000500  loss: 0.1769 (0.1792)  time: 0.0698  data: 0.0001  max mem: 2598
[14:00:58.919863] Epoch: [40]  [194/195]  eta: 0:00:00  lr: 0.000500  loss: 0.1785 (0.1793)  time: 0.0691  data: 0.0001  max mem: 2598
[14:00:58.972602] Epoch: [40] Total time: 0:00:14 (0.0721 s / it)
[14:00:58.972672] Averaged stats: lr: 0.000500  loss: 0.1785 (0.1793)
[14:00:59.158687] log_dir: ./output_dir/mae0.3
[14:00:59.605657] Epoch: [41]  [  0/195]  eta: 0:01:26  lr: 0.000500  loss: 0.1764 (0.1764)  time: 0.4452  data: 0.3081  max mem: 2598
[14:01:01.071769] Epoch: [41]  [ 20/195]  eta: 0:00:15  lr: 0.000500  loss: 0.1753 (0.1769)  time: 0.0733  data: 0.0002  max mem: 2598
[14:01:02.478304] Epoch: [41]  [ 40/195]  eta: 0:00:12  lr: 0.000500  loss: 0.1748 (0.1768)  time: 0.0703  data: 0.0002  max mem: 2598
[14:01:03.889306] Epoch: [41]  [ 60/195]  eta: 0:00:10  lr: 0.000500  loss: 0.1790 (0.1779)  time: 0.0705  data: 0.0001  max mem: 2598
[14:01:05.298640] Epoch: [41]  [ 80/195]  eta: 0:00:08  lr: 0.000500  loss: 0.1778 (0.1779)  time: 0.0704  data: 0.0001  max mem: 2598
[14:01:06.733154] Epoch: [41]  [100/195]  eta: 0:00:07  lr: 0.000500  loss: 0.1778 (0.1778)  time: 0.0717  data: 0.0001  max mem: 2598
[14:01:08.108933] Epoch: [41]  [120/195]  eta: 0:00:05  lr: 0.000500  loss: 0.1783 (0.1777)  time: 0.0688  data: 0.0001  max mem: 2598
[14:01:09.490307] Epoch: [41]  [140/195]  eta: 0:00:04  lr: 0.000500  loss: 0.1762 (0.1774)  time: 0.0690  data: 0.0001  max mem: 2598
[14:01:10.861704] Epoch: [41]  [160/195]  eta: 0:00:02  lr: 0.000500  loss: 0.1821 (0.1779)  time: 0.0685  data: 0.0001  max mem: 2598
[14:01:12.231501] Epoch: [41]  [180/195]  eta: 0:00:01  lr: 0.000500  loss: 0.1763 (0.1778)  time: 0.0684  data: 0.0001  max mem: 2598
[14:01:13.183002] Epoch: [41]  [194/195]  eta: 0:00:00  lr: 0.000500  loss: 0.1755 (0.1777)  time: 0.0680  data: 0.0001  max mem: 2598
[14:01:13.235329] Epoch: [41] Total time: 0:00:14 (0.0722 s / it)
[14:01:13.235628] Averaged stats: lr: 0.000500  loss: 0.1755 (0.1777)
[14:01:13.237861] log_dir: ./output_dir/mae0.3
[14:01:13.654512] Epoch: [42]  [  0/195]  eta: 0:01:20  lr: 0.000500  loss: 0.1837 (0.1837)  time: 0.4150  data: 0.2956  max mem: 2598
[14:01:15.103065] Epoch: [42]  [ 20/195]  eta: 0:00:15  lr: 0.000500  loss: 0.1755 (0.1753)  time: 0.0724  data: 0.0002  max mem: 2598
[14:01:16.517380] Epoch: [42]  [ 40/195]  eta: 0:00:12  lr: 0.000500  loss: 0.1741 (0.1752)  time: 0.0706  data: 0.0001  max mem: 2598
[14:01:17.938551] Epoch: [42]  [ 60/195]  eta: 0:00:10  lr: 0.000500  loss: 0.1760 (0.1754)  time: 0.0710  data: 0.0001  max mem: 2598
[14:01:19.352345] Epoch: [42]  [ 80/195]  eta: 0:00:08  lr: 0.000500  loss: 0.1765 (0.1761)  time: 0.0706  data: 0.0001  max mem: 2598
[14:01:20.764570] Epoch: [42]  [100/195]  eta: 0:00:07  lr: 0.000500  loss: 0.1761 (0.1762)  time: 0.0706  data: 0.0001  max mem: 2598
[14:01:22.178089] Epoch: [42]  [120/195]  eta: 0:00:05  lr: 0.000500  loss: 0.1792 (0.1767)  time: 0.0706  data: 0.0001  max mem: 2598
[14:01:23.592505] Epoch: [42]  [140/195]  eta: 0:00:04  lr: 0.000500  loss: 0.1757 (0.1767)  time: 0.0707  data: 0.0001  max mem: 2598
[14:01:25.011955] Epoch: [42]  [160/195]  eta: 0:00:02  lr: 0.000500  loss: 0.1767 (0.1767)  time: 0.0709  data: 0.0001  max mem: 2598
[14:01:26.418955] Epoch: [42]  [180/195]  eta: 0:00:01  lr: 0.000500  loss: 0.1758 (0.1767)  time: 0.0703  data: 0.0002  max mem: 2598
[14:01:27.386704] Epoch: [42]  [194/195]  eta: 0:00:00  lr: 0.000500  loss: 0.1744 (0.1766)  time: 0.0691  data: 0.0001  max mem: 2598
[14:01:27.432753] Epoch: [42] Total time: 0:00:14 (0.0728 s / it)
[14:01:27.432937] Averaged stats: lr: 0.000500  loss: 0.1744 (0.1766)
[14:01:27.435485] log_dir: ./output_dir/mae0.3
[14:01:27.873405] Epoch: [43]  [  0/195]  eta: 0:01:25  lr: 0.000500  loss: 0.1798 (0.1798)  time: 0.4364  data: 0.3192  max mem: 2598
[14:01:29.302876] Epoch: [43]  [ 20/195]  eta: 0:00:15  lr: 0.000500  loss: 0.1753 (0.1756)  time: 0.0714  data: 0.0001  max mem: 2598
[14:01:30.716021] Epoch: [43]  [ 40/195]  eta: 0:00:12  lr: 0.000500  loss: 0.1769 (0.1762)  time: 0.0706  data: 0.0001  max mem: 2598
[14:01:32.114781] Epoch: [43]  [ 60/195]  eta: 0:00:10  lr: 0.000499  loss: 0.1746 (0.1759)  time: 0.0699  data: 0.0001  max mem: 2598
[14:01:33.517232] Epoch: [43]  [ 80/195]  eta: 0:00:08  lr: 0.000499  loss: 0.1736 (0.1758)  time: 0.0701  data: 0.0001  max mem: 2598
[14:01:34.930646] Epoch: [43]  [100/195]  eta: 0:00:07  lr: 0.000499  loss: 0.1775 (0.1762)  time: 0.0706  data: 0.0001  max mem: 2598
[14:01:36.336865] Epoch: [43]  [120/195]  eta: 0:00:05  lr: 0.000499  loss: 0.1744 (0.1761)  time: 0.0703  data: 0.0001  max mem: 2598
[14:01:37.740609] Epoch: [43]  [140/195]  eta: 0:00:04  lr: 0.000499  loss: 0.1752 (0.1759)  time: 0.0701  data: 0.0001  max mem: 2598
[14:01:39.118350] Epoch: [43]  [160/195]  eta: 0:00:02  lr: 0.000499  loss: 0.1764 (0.1761)  time: 0.0688  data: 0.0001  max mem: 2598
[14:01:40.516103] Epoch: [43]  [180/195]  eta: 0:00:01  lr: 0.000499  loss: 0.1758 (0.1760)  time: 0.0698  data: 0.0001  max mem: 2598
[14:01:41.495871] Epoch: [43]  [194/195]  eta: 0:00:00  lr: 0.000499  loss: 0.1760 (0.1761)  time: 0.0698  data: 0.0001  max mem: 2598
[14:01:41.566632] Epoch: [43] Total time: 0:00:14 (0.0725 s / it)
[14:01:41.566700] Averaged stats: lr: 0.000499  loss: 0.1760 (0.1761)
[14:01:41.569157] log_dir: ./output_dir/mae0.3
[14:01:41.993352] Epoch: [44]  [  0/195]  eta: 0:01:22  lr: 0.000499  loss: 0.1734 (0.1734)  time: 0.4231  data: 0.3402  max mem: 2598
[14:01:43.401496] Epoch: [44]  [ 20/195]  eta: 0:00:15  lr: 0.000499  loss: 0.1724 (0.1737)  time: 0.0704  data: 0.0001  max mem: 2598
[14:01:44.811427] Epoch: [44]  [ 40/195]  eta: 0:00:12  lr: 0.000499  loss: 0.1761 (0.1749)  time: 0.0705  data: 0.0001  max mem: 2598
[14:01:46.227094] Epoch: [44]  [ 60/195]  eta: 0:00:10  lr: 0.000499  loss: 0.1747 (0.1748)  time: 0.0707  data: 0.0001  max mem: 2598
[14:01:47.647326] Epoch: [44]  [ 80/195]  eta: 0:00:08  lr: 0.000499  loss: 0.1774 (0.1750)  time: 0.0710  data: 0.0001  max mem: 2598
[14:01:49.046561] Epoch: [44]  [100/195]  eta: 0:00:07  lr: 0.000499  loss: 0.1760 (0.1750)  time: 0.0699  data: 0.0001  max mem: 2598
[14:01:50.450436] Epoch: [44]  [120/195]  eta: 0:00:05  lr: 0.000499  loss: 0.1723 (0.1748)  time: 0.0702  data: 0.0001  max mem: 2598
[14:01:51.844117] Epoch: [44]  [140/195]  eta: 0:00:04  lr: 0.000499  loss: 0.1766 (0.1750)  time: 0.0696  data: 0.0001  max mem: 2598
[14:01:53.241988] Epoch: [44]  [160/195]  eta: 0:00:02  lr: 0.000499  loss: 0.1760 (0.1752)  time: 0.0699  data: 0.0001  max mem: 2598
[14:01:54.648169] Epoch: [44]  [180/195]  eta: 0:00:01  lr: 0.000499  loss: 0.1749 (0.1751)  time: 0.0703  data: 0.0001  max mem: 2598
[14:01:55.617274] Epoch: [44]  [194/195]  eta: 0:00:00  lr: 0.000499  loss: 0.1738 (0.1749)  time: 0.0693  data: 0.0001  max mem: 2598
[14:01:55.666518] Epoch: [44] Total time: 0:00:14 (0.0723 s / it)
[14:01:55.666588] Averaged stats: lr: 0.000499  loss: 0.1738 (0.1749)
[14:01:55.669074] log_dir: ./output_dir/mae0.3
[14:01:56.035531] Epoch: [45]  [  0/195]  eta: 0:01:11  lr: 0.000499  loss: 0.1783 (0.1783)  time: 0.3648  data: 0.2704  max mem: 2598
[14:01:57.487093] Epoch: [45]  [ 20/195]  eta: 0:00:15  lr: 0.000499  loss: 0.1725 (0.1742)  time: 0.0725  data: 0.0002  max mem: 2598
[14:01:58.925401] Epoch: [45]  [ 40/195]  eta: 0:00:12  lr: 0.000499  loss: 0.1731 (0.1743)  time: 0.0719  data: 0.0002  max mem: 2598
[14:02:00.331137] Epoch: [45]  [ 60/195]  eta: 0:00:10  lr: 0.000499  loss: 0.1741 (0.1747)  time: 0.0703  data: 0.0001  max mem: 2598
[14:02:01.723923] Epoch: [45]  [ 80/195]  eta: 0:00:08  lr: 0.000499  loss: 0.1754 (0.1749)  time: 0.0696  data: 0.0002  max mem: 2598
[14:02:03.115019] Epoch: [45]  [100/195]  eta: 0:00:06  lr: 0.000499  loss: 0.1767 (0.1751)  time: 0.0695  data: 0.0001  max mem: 2598
[14:02:04.524333] Epoch: [45]  [120/195]  eta: 0:00:05  lr: 0.000498  loss: 0.1738 (0.1751)  time: 0.0704  data: 0.0002  max mem: 2598
[14:02:05.956759] Epoch: [45]  [140/195]  eta: 0:00:04  lr: 0.000498  loss: 0.1704 (0.1746)  time: 0.0716  data: 0.0001  max mem: 2598
[14:02:07.364753] Epoch: [45]  [160/195]  eta: 0:00:02  lr: 0.000498  loss: 0.1761 (0.1747)  time: 0.0704  data: 0.0001  max mem: 2598
[14:02:08.767774] Epoch: [45]  [180/195]  eta: 0:00:01  lr: 0.000498  loss: 0.1747 (0.1748)  time: 0.0701  data: 0.0002  max mem: 2598
[14:02:09.735350] Epoch: [45]  [194/195]  eta: 0:00:00  lr: 0.000498  loss: 0.1727 (0.1747)  time: 0.0691  data: 0.0001  max mem: 2598
[14:02:09.780743] Epoch: [45] Total time: 0:00:14 (0.0724 s / it)
[14:02:09.780814] Averaged stats: lr: 0.000498  loss: 0.1727 (0.1747)
[14:02:09.783310] log_dir: ./output_dir/mae0.3
[14:02:10.177875] Epoch: [46]  [  0/195]  eta: 0:01:16  lr: 0.000498  loss: 0.1788 (0.1788)  time: 0.3935  data: 0.3077  max mem: 2598
[14:02:11.598238] Epoch: [46]  [ 20/195]  eta: 0:00:15  lr: 0.000498  loss: 0.1686 (0.1709)  time: 0.0710  data: 0.0001  max mem: 2598
[14:02:12.997414] Epoch: [46]  [ 40/195]  eta: 0:00:12  lr: 0.000498  loss: 0.1714 (0.1718)  time: 0.0699  data: 0.0001  max mem: 2598
[14:02:14.413127] Epoch: [46]  [ 60/195]  eta: 0:00:10  lr: 0.000498  loss: 0.1774 (0.1732)  time: 0.0707  data: 0.0001  max mem: 2598
[14:02:15.820187] Epoch: [46]  [ 80/195]  eta: 0:00:08  lr: 0.000498  loss: 0.1741 (0.1737)  time: 0.0703  data: 0.0001  max mem: 2598
[14:02:17.225609] Epoch: [46]  [100/195]  eta: 0:00:06  lr: 0.000498  loss: 0.1715 (0.1736)  time: 0.0702  data: 0.0001  max mem: 2598
[14:02:18.630817] Epoch: [46]  [120/195]  eta: 0:00:05  lr: 0.000498  loss: 0.1727 (0.1736)  time: 0.0702  data: 0.0001  max mem: 2598
[14:02:20.035984] Epoch: [46]  [140/195]  eta: 0:00:03  lr: 0.000498  loss: 0.1726 (0.1735)  time: 0.0702  data: 0.0001  max mem: 2598
[14:02:21.445964] Epoch: [46]  [160/195]  eta: 0:00:02  lr: 0.000498  loss: 0.1750 (0.1735)  time: 0.0705  data: 0.0001  max mem: 2598
[14:02:22.846662] Epoch: [46]  [180/195]  eta: 0:00:01  lr: 0.000498  loss: 0.1747 (0.1735)  time: 0.0700  data: 0.0001  max mem: 2598
[14:02:23.829177] Epoch: [46]  [194/195]  eta: 0:00:00  lr: 0.000498  loss: 0.1741 (0.1736)  time: 0.0698  data: 0.0001  max mem: 2598
[14:02:23.893397] Epoch: [46] Total time: 0:00:14 (0.0724 s / it)
[14:02:23.893477] Averaged stats: lr: 0.000498  loss: 0.1741 (0.1736)
[14:02:23.896147] log_dir: ./output_dir/mae0.3
[14:02:24.331142] Epoch: [47]  [  0/195]  eta: 0:01:24  lr: 0.000498  loss: 0.1835 (0.1835)  time: 0.4338  data: 0.3279  max mem: 2598
[14:02:25.841742] Epoch: [47]  [ 20/195]  eta: 0:00:16  lr: 0.000498  loss: 0.1724 (0.1740)  time: 0.0755  data: 0.0002  max mem: 2598
[14:02:27.296612] Epoch: [47]  [ 40/195]  eta: 0:00:12  lr: 0.000498  loss: 0.1720 (0.1719)  time: 0.0727  data: 0.0002  max mem: 2598
[14:02:28.715558] Epoch: [47]  [ 60/195]  eta: 0:00:10  lr: 0.000497  loss: 0.1747 (0.1729)  time: 0.0709  data: 0.0001  max mem: 2598
[14:02:30.118325] Epoch: [47]  [ 80/195]  eta: 0:00:08  lr: 0.000497  loss: 0.1702 (0.1728)  time: 0.0701  data: 0.0001  max mem: 2598
[14:02:31.534530] Epoch: [47]  [100/195]  eta: 0:00:07  lr: 0.000497  loss: 0.1739 (0.1729)  time: 0.0708  data: 0.0001  max mem: 2598
[14:02:32.975282] Epoch: [47]  [120/195]  eta: 0:00:05  lr: 0.000497  loss: 0.1737 (0.1730)  time: 0.0720  data: 0.0002  max mem: 2598
[14:02:34.529633] Epoch: [47]  [140/195]  eta: 0:00:04  lr: 0.000497  loss: 0.1739 (0.1730)  time: 0.0777  data: 0.0002  max mem: 2598
[14:02:35.943118] Epoch: [47]  [160/195]  eta: 0:00:02  lr: 0.000497  loss: 0.1730 (0.1731)  time: 0.0706  data: 0.0001  max mem: 2598
[14:02:37.344585] Epoch: [47]  [180/195]  eta: 0:00:01  lr: 0.000497  loss: 0.1722 (0.1730)  time: 0.0700  data: 0.0001  max mem: 2598
[14:02:38.335043] Epoch: [47]  [194/195]  eta: 0:00:00  lr: 0.000497  loss: 0.1727 (0.1731)  time: 0.0703  data: 0.0001  max mem: 2598
[14:02:38.391463] Epoch: [47] Total time: 0:00:14 (0.0743 s / it)
[14:02:38.391528] Averaged stats: lr: 0.000497  loss: 0.1727 (0.1731)
[14:02:38.393931] log_dir: ./output_dir/mae0.3
[14:02:38.851104] Epoch: [48]  [  0/195]  eta: 0:01:28  lr: 0.000497  loss: 0.1691 (0.1691)  time: 0.4555  data: 0.3268  max mem: 2598
[14:02:40.288734] Epoch: [48]  [ 20/195]  eta: 0:00:15  lr: 0.000497  loss: 0.1732 (0.1728)  time: 0.0718  data: 0.0002  max mem: 2598
[14:02:41.691383] Epoch: [48]  [ 40/195]  eta: 0:00:12  lr: 0.000497  loss: 0.1723 (0.1727)  time: 0.0701  data: 0.0001  max mem: 2598
[14:02:43.087413] Epoch: [48]  [ 60/195]  eta: 0:00:10  lr: 0.000497  loss: 0.1715 (0.1722)  time: 0.0697  data: 0.0001  max mem: 2598
[14:02:44.493356] Epoch: [48]  [ 80/195]  eta: 0:00:08  lr: 0.000497  loss: 0.1729 (0.1726)  time: 0.0703  data: 0.0002  max mem: 2598
[14:02:45.900459] Epoch: [48]  [100/195]  eta: 0:00:07  lr: 0.000497  loss: 0.1731 (0.1727)  time: 0.0703  data: 0.0001  max mem: 2598
[14:02:47.339142] Epoch: [48]  [120/195]  eta: 0:00:05  lr: 0.000496  loss: 0.1725 (0.1727)  time: 0.0719  data: 0.0002  max mem: 2598
[14:02:48.751013] Epoch: [48]  [140/195]  eta: 0:00:04  lr: 0.000496  loss: 0.1723 (0.1726)  time: 0.0705  data: 0.0001  max mem: 2598
[14:02:50.144274] Epoch: [48]  [160/195]  eta: 0:00:02  lr: 0.000496  loss: 0.1748 (0.1729)  time: 0.0696  data: 0.0001  max mem: 2598
[14:02:51.509788] Epoch: [48]  [180/195]  eta: 0:00:01  lr: 0.000496  loss: 0.1717 (0.1726)  time: 0.0682  data: 0.0001  max mem: 2598
[14:02:52.464763] Epoch: [48]  [194/195]  eta: 0:00:00  lr: 0.000496  loss: 0.1717 (0.1726)  time: 0.0682  data: 0.0001  max mem: 2598
[14:02:52.501159] Epoch: [48] Total time: 0:00:14 (0.0723 s / it)
[14:02:52.501220] Averaged stats: lr: 0.000496  loss: 0.1717 (0.1726)
[14:02:52.503433] log_dir: ./output_dir/mae0.3
[14:02:52.949218] Epoch: [49]  [  0/195]  eta: 0:01:26  lr: 0.000496  loss: 0.1825 (0.1825)  time: 0.4443  data: 0.3035  max mem: 2598
[14:02:54.387348] Epoch: [49]  [ 20/195]  eta: 0:00:15  lr: 0.000496  loss: 0.1710 (0.1709)  time: 0.0719  data: 0.0002  max mem: 2598
[14:02:55.794447] Epoch: [49]  [ 40/195]  eta: 0:00:12  lr: 0.000496  loss: 0.1740 (0.1715)  time: 0.0703  data: 0.0001  max mem: 2598
[14:02:57.197467] Epoch: [49]  [ 60/195]  eta: 0:00:10  lr: 0.000496  loss: 0.1694 (0.1712)  time: 0.0701  data: 0.0001  max mem: 2598
[14:02:58.603361] Epoch: [49]  [ 80/195]  eta: 0:00:08  lr: 0.000496  loss: 0.1693 (0.1707)  time: 0.0703  data: 0.0001  max mem: 2598
[14:03:00.002571] Epoch: [49]  [100/195]  eta: 0:00:07  lr: 0.000496  loss: 0.1723 (0.1707)  time: 0.0699  data: 0.0001  max mem: 2598
[14:03:01.401615] Epoch: [49]  [120/195]  eta: 0:00:05  lr: 0.000496  loss: 0.1758 (0.1714)  time: 0.0699  data: 0.0001  max mem: 2598
[14:03:02.806296] Epoch: [49]  [140/195]  eta: 0:00:04  lr: 0.000495  loss: 0.1695 (0.1713)  time: 0.0702  data: 0.0001  max mem: 2598
[14:03:04.205150] Epoch: [49]  [160/195]  eta: 0:00:02  lr: 0.000495  loss: 0.1684 (0.1711)  time: 0.0699  data: 0.0001  max mem: 2598
[14:03:05.600421] Epoch: [49]  [180/195]  eta: 0:00:01  lr: 0.000495  loss: 0.1701 (0.1710)  time: 0.0697  data: 0.0001  max mem: 2598
[14:03:06.565944] Epoch: [49]  [194/195]  eta: 0:00:00  lr: 0.000495  loss: 0.1669 (0.1709)  time: 0.0689  data: 0.0001  max mem: 2598
[14:03:06.636211] Epoch: [49] Total time: 0:00:14 (0.0725 s / it)
[14:03:06.636281] Averaged stats: lr: 0.000495  loss: 0.1669 (0.1709)
[14:03:06.638801] log_dir: ./output_dir/mae0.3
[14:03:07.041125] Epoch: [50]  [  0/195]  eta: 0:01:18  lr: 0.000495  loss: 0.1798 (0.1798)  time: 0.4014  data: 0.3139  max mem: 2598
[14:03:08.519847] Epoch: [50]  [ 20/195]  eta: 0:00:15  lr: 0.000495  loss: 0.1677 (0.1702)  time: 0.0739  data: 0.0002  max mem: 2598
[14:03:09.988079] Epoch: [50]  [ 40/195]  eta: 0:00:12  lr: 0.000495  loss: 0.1730 (0.1704)  time: 0.0734  data: 0.0002  max mem: 2598
[14:03:11.430387] Epoch: [50]  [ 60/195]  eta: 0:00:10  lr: 0.000495  loss: 0.1697 (0.1702)  time: 0.0721  data: 0.0002  max mem: 2598
[14:03:12.883947] Epoch: [50]  [ 80/195]  eta: 0:00:08  lr: 0.000495  loss: 0.1716 (0.1707)  time: 0.0726  data: 0.0001  max mem: 2598
[14:03:14.317917] Epoch: [50]  [100/195]  eta: 0:00:07  lr: 0.000495  loss: 0.1705 (0.1708)  time: 0.0717  data: 0.0002  max mem: 2598
[14:03:15.754489] Epoch: [50]  [120/195]  eta: 0:00:05  lr: 0.000495  loss: 0.1694 (0.1707)  time: 0.0718  data: 0.0001  max mem: 2598
[14:03:17.174796] Epoch: [50]  [140/195]  eta: 0:00:04  lr: 0.000494  loss: 0.1699 (0.1708)  time: 0.0710  data: 0.0001  max mem: 2598
[14:03:18.565852] Epoch: [50]  [160/195]  eta: 0:00:02  lr: 0.000494  loss: 0.1728 (0.1710)  time: 0.0695  data: 0.0001  max mem: 2598
[14:03:19.957518] Epoch: [50]  [180/195]  eta: 0:00:01  lr: 0.000494  loss: 0.1693 (0.1710)  time: 0.0695  data: 0.0001  max mem: 2598
[14:03:20.920708] Epoch: [50]  [194/195]  eta: 0:00:00  lr: 0.000494  loss: 0.1707 (0.1710)  time: 0.0687  data: 0.0001  max mem: 2598
[14:03:20.965735] Epoch: [50] Total time: 0:00:14 (0.0735 s / it)
[14:03:20.965940] Averaged stats: lr: 0.000494  loss: 0.1707 (0.1710)
[14:03:20.968322] log_dir: ./output_dir/mae0.3
[14:03:21.340702] Epoch: [51]  [  0/195]  eta: 0:01:12  lr: 0.000494  loss: 0.1702 (0.1702)  time: 0.3711  data: 0.2914  max mem: 2598
[14:03:22.771769] Epoch: [51]  [ 20/195]  eta: 0:00:15  lr: 0.000494  loss: 0.1676 (0.1684)  time: 0.0715  data: 0.0001  max mem: 2598
[14:03:24.173631] Epoch: [51]  [ 40/195]  eta: 0:00:12  lr: 0.000494  loss: 0.1693 (0.1685)  time: 0.0700  data: 0.0001  max mem: 2598
[14:03:25.585150] Epoch: [51]  [ 60/195]  eta: 0:00:10  lr: 0.000494  loss: 0.1690 (0.1685)  time: 0.0705  data: 0.0001  max mem: 2598
[14:03:26.991059] Epoch: [51]  [ 80/195]  eta: 0:00:08  lr: 0.000494  loss: 0.1710 (0.1693)  time: 0.0703  data: 0.0001  max mem: 2598
[14:03:28.398634] Epoch: [51]  [100/195]  eta: 0:00:06  lr: 0.000494  loss: 0.1688 (0.1694)  time: 0.0703  data: 0.0001  max mem: 2598
[14:03:29.808639] Epoch: [51]  [120/195]  eta: 0:00:05  lr: 0.000494  loss: 0.1686 (0.1696)  time: 0.0705  data: 0.0001  max mem: 2598
[14:03:31.211645] Epoch: [51]  [140/195]  eta: 0:00:03  lr: 0.000493  loss: 0.1680 (0.1695)  time: 0.0701  data: 0.0001  max mem: 2598
[14:03:32.616518] Epoch: [51]  [160/195]  eta: 0:00:02  lr: 0.000493  loss: 0.1688 (0.1696)  time: 0.0702  data: 0.0001  max mem: 2598
[14:03:34.014204] Epoch: [51]  [180/195]  eta: 0:00:01  lr: 0.000493  loss: 0.1711 (0.1698)  time: 0.0698  data: 0.0001  max mem: 2598
[14:03:34.981025] Epoch: [51]  [194/195]  eta: 0:00:00  lr: 0.000493  loss: 0.1691 (0.1699)  time: 0.0691  data: 0.0001  max mem: 2598
[14:03:35.028438] Epoch: [51] Total time: 0:00:14 (0.0721 s / it)
[14:03:35.028514] Averaged stats: lr: 0.000493  loss: 0.1691 (0.1699)
[14:03:35.031171] log_dir: ./output_dir/mae0.3
[14:03:35.453599] Epoch: [52]  [  0/195]  eta: 0:01:22  lr: 0.000493  loss: 0.1694 (0.1694)  time: 0.4213  data: 0.3306  max mem: 2598
[14:03:36.891906] Epoch: [52]  [ 20/195]  eta: 0:00:15  lr: 0.000493  loss: 0.1676 (0.1693)  time: 0.0719  data: 0.0001  max mem: 2598
[14:03:38.299912] Epoch: [52]  [ 40/195]  eta: 0:00:12  lr: 0.000493  loss: 0.1678 (0.1691)  time: 0.0704  data: 0.0001  max mem: 2598
[14:03:39.710200] Epoch: [52]  [ 60/195]  eta: 0:00:10  lr: 0.000493  loss: 0.1678 (0.1688)  time: 0.0705  data: 0.0001  max mem: 2598
[14:03:41.111299] Epoch: [52]  [ 80/195]  eta: 0:00:08  lr: 0.000493  loss: 0.1689 (0.1690)  time: 0.0700  data: 0.0001  max mem: 2598
[14:03:42.512941] Epoch: [52]  [100/195]  eta: 0:00:07  lr: 0.000492  loss: 0.1705 (0.1695)  time: 0.0700  data: 0.0001  max mem: 2598
[14:03:43.923609] Epoch: [52]  [120/195]  eta: 0:00:05  lr: 0.000492  loss: 0.1666 (0.1692)  time: 0.0705  data: 0.0002  max mem: 2598
[14:03:45.356553] Epoch: [52]  [140/195]  eta: 0:00:04  lr: 0.000492  loss: 0.1700 (0.1691)  time: 0.0716  data: 0.0002  max mem: 2598
[14:03:46.761284] Epoch: [52]  [160/195]  eta: 0:00:02  lr: 0.000492  loss: 0.1698 (0.1693)  time: 0.0702  data: 0.0002  max mem: 2598
[14:03:48.191021] Epoch: [52]  [180/195]  eta: 0:00:01  lr: 0.000492  loss: 0.1702 (0.1695)  time: 0.0714  data: 0.0001  max mem: 2598
[14:03:49.161554] Epoch: [52]  [194/195]  eta: 0:00:00  lr: 0.000492  loss: 0.1703 (0.1696)  time: 0.0704  data: 0.0001  max mem: 2598
[14:03:49.226896] Epoch: [52] Total time: 0:00:14 (0.0728 s / it)
[14:03:49.227142] Averaged stats: lr: 0.000492  loss: 0.1703 (0.1696)
[14:03:49.229654] log_dir: ./output_dir/mae0.3
[14:03:49.678322] Epoch: [53]  [  0/195]  eta: 0:01:27  lr: 0.000492  loss: 0.1651 (0.1651)  time: 0.4473  data: 0.3328  max mem: 2598
[14:03:51.122961] Epoch: [53]  [ 20/195]  eta: 0:00:15  lr: 0.000492  loss: 0.1676 (0.1680)  time: 0.0722  data: 0.0002  max mem: 2598
[14:03:52.553986] Epoch: [53]  [ 40/195]  eta: 0:00:12  lr: 0.000492  loss: 0.1649 (0.1685)  time: 0.0715  data: 0.0002  max mem: 2598
[14:03:53.949181] Epoch: [53]  [ 60/195]  eta: 0:00:10  lr: 0.000492  loss: 0.1695 (0.1686)  time: 0.0697  data: 0.0001  max mem: 2598
[14:03:55.350018] Epoch: [53]  [ 80/195]  eta: 0:00:08  lr: 0.000491  loss: 0.1706 (0.1693)  time: 0.0700  data: 0.0001  max mem: 2598
[14:03:56.744315] Epoch: [53]  [100/195]  eta: 0:00:07  lr: 0.000491  loss: 0.1695 (0.1692)  time: 0.0697  data: 0.0001  max mem: 2598
[14:03:58.148261] Epoch: [53]  [120/195]  eta: 0:00:05  lr: 0.000491  loss: 0.1679 (0.1689)  time: 0.0702  data: 0.0001  max mem: 2598
[14:03:59.545902] Epoch: [53]  [140/195]  eta: 0:00:04  lr: 0.000491  loss: 0.1680 (0.1689)  time: 0.0698  data: 0.0001  max mem: 2598
[14:04:00.955286] Epoch: [53]  [160/195]  eta: 0:00:02  lr: 0.000491  loss: 0.1712 (0.1690)  time: 0.0704  data: 0.0001  max mem: 2598
[14:04:02.349808] Epoch: [53]  [180/195]  eta: 0:00:01  lr: 0.000491  loss: 0.1672 (0.1689)  time: 0.0697  data: 0.0001  max mem: 2598
[14:04:03.307721] Epoch: [53]  [194/195]  eta: 0:00:00  lr: 0.000491  loss: 0.1698 (0.1690)  time: 0.0685  data: 0.0001  max mem: 2598
[14:04:03.361239] Epoch: [53] Total time: 0:00:14 (0.0725 s / it)
[14:04:03.361302] Averaged stats: lr: 0.000491  loss: 0.1698 (0.1690)
[14:04:03.363688] log_dir: ./output_dir/mae0.3
[14:04:03.783310] Epoch: [54]  [  0/195]  eta: 0:01:21  lr: 0.000491  loss: 0.1641 (0.1641)  time: 0.4183  data: 0.2948  max mem: 2598
[14:04:05.257411] Epoch: [54]  [ 20/195]  eta: 0:00:15  lr: 0.000490  loss: 0.1650 (0.1669)  time: 0.0737  data: 0.0002  max mem: 2598
[14:04:06.666294] Epoch: [54]  [ 40/195]  eta: 0:00:12  lr: 0.000490  loss: 0.1650 (0.1666)  time: 0.0704  data: 0.0001  max mem: 2598
[14:04:08.073084] Epoch: [54]  [ 60/195]  eta: 0:00:10  lr: 0.000490  loss: 0.1689 (0.1671)  time: 0.0703  data: 0.0001  max mem: 2598
[14:04:09.481569] Epoch: [54]  [ 80/195]  eta: 0:00:08  lr: 0.000490  loss: 0.1662 (0.1674)  time: 0.0704  data: 0.0001  max mem: 2598
[14:04:10.914468] Epoch: [54]  [100/195]  eta: 0:00:07  lr: 0.000490  loss: 0.1672 (0.1674)  time: 0.0716  data: 0.0001  max mem: 2598
[14:04:12.321103] Epoch: [54]  [120/195]  eta: 0:00:05  lr: 0.000490  loss: 0.1676 (0.1677)  time: 0.0703  data: 0.0001  max mem: 2598
[14:04:13.741077] Epoch: [54]  [140/195]  eta: 0:00:04  lr: 0.000490  loss: 0.1675 (0.1676)  time: 0.0710  data: 0.0001  max mem: 2598
[14:04:15.160707] Epoch: [54]  [160/195]  eta: 0:00:02  lr: 0.000489  loss: 0.1696 (0.1679)  time: 0.0709  data: 0.0001  max mem: 2598
[14:04:16.563586] Epoch: [54]  [180/195]  eta: 0:00:01  lr: 0.000489  loss: 0.1664 (0.1679)  time: 0.0701  data: 0.0001  max mem: 2598
[14:04:17.529974] Epoch: [54]  [194/195]  eta: 0:00:00  lr: 0.000489  loss: 0.1665 (0.1679)  time: 0.0691  data: 0.0001  max mem: 2598
[14:04:17.577125] Epoch: [54] Total time: 0:00:14 (0.0729 s / it)
[14:04:17.577198] Averaged stats: lr: 0.000489  loss: 0.1665 (0.1679)
[14:04:17.579677] log_dir: ./output_dir/mae0.3
[14:04:18.012168] Epoch: [55]  [  0/195]  eta: 0:01:24  lr: 0.000489  loss: 0.1736 (0.1736)  time: 0.4311  data: 0.3073  max mem: 2598
[14:04:19.465004] Epoch: [55]  [ 20/195]  eta: 0:00:15  lr: 0.000489  loss: 0.1676 (0.1675)  time: 0.0726  data: 0.0002  max mem: 2598
[14:04:20.874905] Epoch: [55]  [ 40/195]  eta: 0:00:12  lr: 0.000489  loss: 0.1656 (0.1678)  time: 0.0705  data: 0.0001  max mem: 2598
[14:04:22.275682] Epoch: [55]  [ 60/195]  eta: 0:00:10  lr: 0.000489  loss: 0.1684 (0.1679)  time: 0.0700  data: 0.0001  max mem: 2598
[14:04:23.664825] Epoch: [55]  [ 80/195]  eta: 0:00:08  lr: 0.000489  loss: 0.1665 (0.1676)  time: 0.0694  data: 0.0001  max mem: 2598
[14:04:25.052228] Epoch: [55]  [100/195]  eta: 0:00:07  lr: 0.000488  loss: 0.1641 (0.1672)  time: 0.0693  data: 0.0001  max mem: 2598
[14:04:26.441974] Epoch: [55]  [120/195]  eta: 0:00:05  lr: 0.000488  loss: 0.1675 (0.1675)  time: 0.0695  data: 0.0001  max mem: 2598
[14:04:27.847864] Epoch: [55]  [140/195]  eta: 0:00:04  lr: 0.000488  loss: 0.1649 (0.1673)  time: 0.0703  data: 0.0001  max mem: 2598
[14:04:29.220639] Epoch: [55]  [160/195]  eta: 0:00:02  lr: 0.000488  loss: 0.1668 (0.1675)  time: 0.0686  data: 0.0001  max mem: 2598
[14:04:30.592723] Epoch: [55]  [180/195]  eta: 0:00:01  lr: 0.000488  loss: 0.1659 (0.1674)  time: 0.0686  data: 0.0001  max mem: 2598
[14:04:31.544293] Epoch: [55]  [194/195]  eta: 0:00:00  lr: 0.000488  loss: 0.1657 (0.1674)  time: 0.0680  data: 0.0001  max mem: 2598
[14:04:31.615984] Epoch: [55] Total time: 0:00:14 (0.0720 s / it)
[14:04:31.616052] Averaged stats: lr: 0.000488  loss: 0.1657 (0.1674)
[14:04:31.618575] log_dir: ./output_dir/mae0.3
[14:04:32.019839] Epoch: [56]  [  0/195]  eta: 0:01:18  lr: 0.000488  loss: 0.1767 (0.1767)  time: 0.4005  data: 0.3247  max mem: 2598
[14:04:33.437201] Epoch: [56]  [ 20/195]  eta: 0:00:15  lr: 0.000488  loss: 0.1628 (0.1638)  time: 0.0708  data: 0.0001  max mem: 2598
[14:04:34.836915] Epoch: [56]  [ 40/195]  eta: 0:00:12  lr: 0.000487  loss: 0.1634 (0.1642)  time: 0.0699  data: 0.0001  max mem: 2598
[14:04:36.238739] Epoch: [56]  [ 60/195]  eta: 0:00:10  lr: 0.000487  loss: 0.1669 (0.1650)  time: 0.0701  data: 0.0001  max mem: 2598
[14:04:37.649650] Epoch: [56]  [ 80/195]  eta: 0:00:08  lr: 0.000487  loss: 0.1684 (0.1663)  time: 0.0705  data: 0.0001  max mem: 2598
[14:04:39.055112] Epoch: [56]  [100/195]  eta: 0:00:06  lr: 0.000487  loss: 0.1659 (0.1662)  time: 0.0702  data: 0.0001  max mem: 2598
[14:04:40.463631] Epoch: [56]  [120/195]  eta: 0:00:05  lr: 0.000487  loss: 0.1655 (0.1665)  time: 0.0704  data: 0.0001  max mem: 2598
[14:04:41.875634] Epoch: [56]  [140/195]  eta: 0:00:03  lr: 0.000487  loss: 0.1673 (0.1666)  time: 0.0706  data: 0.0001  max mem: 2598
[14:04:43.283625] Epoch: [56]  [160/195]  eta: 0:00:02  lr: 0.000486  loss: 0.1678 (0.1668)  time: 0.0704  data: 0.0001  max mem: 2598
[14:04:44.682861] Epoch: [56]  [180/195]  eta: 0:00:01  lr: 0.000486  loss: 0.1657 (0.1668)  time: 0.0699  data: 0.0001  max mem: 2598
[14:04:45.647908] Epoch: [56]  [194/195]  eta: 0:00:00  lr: 0.000486  loss: 0.1669 (0.1668)  time: 0.0690  data: 0.0001  max mem: 2598
[14:04:45.696827] Epoch: [56] Total time: 0:00:14 (0.0722 s / it)
[14:04:45.696896] Averaged stats: lr: 0.000486  loss: 0.1669 (0.1668)
[14:04:45.699388] log_dir: ./output_dir/mae0.3
[14:04:46.133395] Epoch: [57]  [  0/195]  eta: 0:01:24  lr: 0.000486  loss: 0.1625 (0.1625)  time: 0.4323  data: 0.3126  max mem: 2598
[14:04:47.592380] Epoch: [57]  [ 20/195]  eta: 0:00:15  lr: 0.000486  loss: 0.1644 (0.1652)  time: 0.0729  data: 0.0002  max mem: 2598
[14:04:49.015878] Epoch: [57]  [ 40/195]  eta: 0:00:12  lr: 0.000486  loss: 0.1661 (0.1648)  time: 0.0711  data: 0.0001  max mem: 2598
[14:04:50.440948] Epoch: [57]  [ 60/195]  eta: 0:00:10  lr: 0.000486  loss: 0.1671 (0.1660)  time: 0.0712  data: 0.0001  max mem: 2598
[14:04:51.819268] Epoch: [57]  [ 80/195]  eta: 0:00:08  lr: 0.000486  loss: 0.1673 (0.1664)  time: 0.0689  data: 0.0001  max mem: 2598
[14:04:53.198986] Epoch: [57]  [100/195]  eta: 0:00:07  lr: 0.000485  loss: 0.1668 (0.1667)  time: 0.0689  data: 0.0001  max mem: 2598
[14:04:54.583286] Epoch: [57]  [120/195]  eta: 0:00:05  lr: 0.000485  loss: 0.1684 (0.1671)  time: 0.0692  data: 0.0001  max mem: 2598
[14:04:55.971372] Epoch: [57]  [140/195]  eta: 0:00:04  lr: 0.000485  loss: 0.1691 (0.1672)  time: 0.0694  data: 0.0001  max mem: 2598
[14:04:57.361483] Epoch: [57]  [160/195]  eta: 0:00:02  lr: 0.000485  loss: 0.1672 (0.1673)  time: 0.0695  data: 0.0001  max mem: 2598
[14:04:58.741267] Epoch: [57]  [180/195]  eta: 0:00:01  lr: 0.000485  loss: 0.1649 (0.1672)  time: 0.0690  data: 0.0001  max mem: 2598
[14:04:59.693777] Epoch: [57]  [194/195]  eta: 0:00:00  lr: 0.000485  loss: 0.1659 (0.1671)  time: 0.0680  data: 0.0001  max mem: 2598
[14:04:59.740877] Epoch: [57] Total time: 0:00:14 (0.0720 s / it)
[14:04:59.740946] Averaged stats: lr: 0.000485  loss: 0.1659 (0.1671)
[14:04:59.743217] log_dir: ./output_dir/mae0.3
[14:05:00.150851] Epoch: [58]  [  0/195]  eta: 0:01:19  lr: 0.000485  loss: 0.1709 (0.1709)  time: 0.4061  data: 0.2900  max mem: 2598
[14:05:01.605120] Epoch: [58]  [ 20/195]  eta: 0:00:15  lr: 0.000484  loss: 0.1658 (0.1656)  time: 0.0726  data: 0.0002  max mem: 2598
[14:05:03.011890] Epoch: [58]  [ 40/195]  eta: 0:00:12  lr: 0.000484  loss: 0.1658 (0.1654)  time: 0.0703  data: 0.0001  max mem: 2598
[14:05:04.419141] Epoch: [58]  [ 60/195]  eta: 0:00:10  lr: 0.000484  loss: 0.1633 (0.1656)  time: 0.0703  data: 0.0001  max mem: 2598
[14:05:05.829813] Epoch: [58]  [ 80/195]  eta: 0:00:08  lr: 0.000484  loss: 0.1654 (0.1658)  time: 0.0705  data: 0.0001  max mem: 2598
[14:05:07.239647] Epoch: [58]  [100/195]  eta: 0:00:07  lr: 0.000484  loss: 0.1652 (0.1661)  time: 0.0704  data: 0.0002  max mem: 2598
[14:05:08.651178] Epoch: [58]  [120/195]  eta: 0:00:05  lr: 0.000483  loss: 0.1646 (0.1661)  time: 0.0705  data: 0.0001  max mem: 2598
[14:05:10.058693] Epoch: [58]  [140/195]  eta: 0:00:04  lr: 0.000483  loss: 0.1612 (0.1657)  time: 0.0703  data: 0.0002  max mem: 2598
[14:05:11.468326] Epoch: [58]  [160/195]  eta: 0:00:02  lr: 0.000483  loss: 0.1651 (0.1658)  time: 0.0704  data: 0.0002  max mem: 2598
[14:05:12.870505] Epoch: [58]  [180/195]  eta: 0:00:01  lr: 0.000483  loss: 0.1654 (0.1657)  time: 0.0701  data: 0.0001  max mem: 2598
[14:05:13.836123] Epoch: [58]  [194/195]  eta: 0:00:00  lr: 0.000483  loss: 0.1689 (0.1657)  time: 0.0690  data: 0.0001  max mem: 2598
[14:05:13.885532] Epoch: [58] Total time: 0:00:14 (0.0725 s / it)
[14:05:13.885604] Averaged stats: lr: 0.000483  loss: 0.1689 (0.1657)
[14:05:13.888143] log_dir: ./output_dir/mae0.3
[14:05:14.273719] Epoch: [59]  [  0/195]  eta: 0:01:14  lr: 0.000483  loss: 0.1641 (0.1641)  time: 0.3845  data: 0.3070  max mem: 2598
[14:05:15.731998] Epoch: [59]  [ 20/195]  eta: 0:00:15  lr: 0.000483  loss: 0.1623 (0.1631)  time: 0.0729  data: 0.0002  max mem: 2598
[14:05:17.166012] Epoch: [59]  [ 40/195]  eta: 0:00:12  lr: 0.000482  loss: 0.1644 (0.1634)  time: 0.0717  data: 0.0001  max mem: 2598
[14:05:18.610978] Epoch: [59]  [ 60/195]  eta: 0:00:10  lr: 0.000482  loss: 0.1650 (0.1642)  time: 0.0722  data: 0.0001  max mem: 2598
[14:05:20.020143] Epoch: [59]  [ 80/195]  eta: 0:00:08  lr: 0.000482  loss: 0.1635 (0.1643)  time: 0.0704  data: 0.0001  max mem: 2598
[14:05:21.425854] Epoch: [59]  [100/195]  eta: 0:00:07  lr: 0.000482  loss: 0.1643 (0.1646)  time: 0.0702  data: 0.0001  max mem: 2598
[14:05:22.829354] Epoch: [59]  [120/195]  eta: 0:00:05  lr: 0.000482  loss: 0.1650 (0.1647)  time: 0.0701  data: 0.0001  max mem: 2598
[14:05:24.240044] Epoch: [59]  [140/195]  eta: 0:00:04  lr: 0.000481  loss: 0.1669 (0.1647)  time: 0.0705  data: 0.0001  max mem: 2598
[14:05:25.654680] Epoch: [59]  [160/195]  eta: 0:00:02  lr: 0.000481  loss: 0.1672 (0.1649)  time: 0.0707  data: 0.0001  max mem: 2598
[14:05:27.058714] Epoch: [59]  [180/195]  eta: 0:00:01  lr: 0.000481  loss: 0.1617 (0.1648)  time: 0.0702  data: 0.0001  max mem: 2598
[14:05:28.012822] Epoch: [59]  [194/195]  eta: 0:00:00  lr: 0.000481  loss: 0.1609 (0.1647)  time: 0.0682  data: 0.0001  max mem: 2598
[14:05:28.058620] Epoch: [59] Total time: 0:00:14 (0.0727 s / it)
[14:05:28.058690] Averaged stats: lr: 0.000481  loss: 0.1609 (0.1647)
[14:05:28.061182] log_dir: ./output_dir/mae0.3
[14:05:28.447473] Epoch: [60]  [  0/195]  eta: 0:01:15  lr: 0.000481  loss: 0.1600 (0.1600)  time: 0.3848  data: 0.3031  max mem: 2598
[14:05:29.889816] Epoch: [60]  [ 20/195]  eta: 0:00:15  lr: 0.000481  loss: 0.1617 (0.1618)  time: 0.0721  data: 0.0002  max mem: 2598
[14:05:31.271060] Epoch: [60]  [ 40/195]  eta: 0:00:12  lr: 0.000481  loss: 0.1632 (0.1626)  time: 0.0690  data: 0.0001  max mem: 2598
[14:05:32.646398] Epoch: [60]  [ 60/195]  eta: 0:00:10  lr: 0.000480  loss: 0.1655 (0.1631)  time: 0.0687  data: 0.0001  max mem: 2598
[14:05:34.031596] Epoch: [60]  [ 80/195]  eta: 0:00:08  lr: 0.000480  loss: 0.1656 (0.1634)  time: 0.0692  data: 0.0001  max mem: 2598
[14:05:35.411991] Epoch: [60]  [100/195]  eta: 0:00:06  lr: 0.000480  loss: 0.1650 (0.1638)  time: 0.0690  data: 0.0001  max mem: 2598
[14:05:36.801059] Epoch: [60]  [120/195]  eta: 0:00:05  lr: 0.000480  loss: 0.1661 (0.1640)  time: 0.0694  data: 0.0001  max mem: 2598
[14:05:38.202621] Epoch: [60]  [140/195]  eta: 0:00:03  lr: 0.000480  loss: 0.1665 (0.1644)  time: 0.0700  data: 0.0001  max mem: 2598
[14:05:39.595271] Epoch: [60]  [160/195]  eta: 0:00:02  lr: 0.000479  loss: 0.1639 (0.1645)  time: 0.0696  data: 0.0001  max mem: 2598
[14:05:40.979585] Epoch: [60]  [180/195]  eta: 0:00:01  lr: 0.000479  loss: 0.1630 (0.1643)  time: 0.0692  data: 0.0001  max mem: 2598
[14:05:41.930050] Epoch: [60]  [194/195]  eta: 0:00:00  lr: 0.000479  loss: 0.1614 (0.1644)  time: 0.0680  data: 0.0001  max mem: 2598
[14:05:42.002319] Epoch: [60] Total time: 0:00:13 (0.0715 s / it)
[14:05:42.002504] Averaged stats: lr: 0.000479  loss: 0.1614 (0.1644)
[14:05:42.158221] log_dir: ./output_dir/mae0.3
[14:05:42.583982] Epoch: [61]  [  0/195]  eta: 0:01:22  lr: 0.000479  loss: 0.1706 (0.1706)  time: 0.4241  data: 0.2802  max mem: 2598
[14:05:44.007303] Epoch: [61]  [ 20/195]  eta: 0:00:15  lr: 0.000479  loss: 0.1638 (0.1630)  time: 0.0711  data: 0.0001  max mem: 2598
[14:05:45.391705] Epoch: [61]  [ 40/195]  eta: 0:00:12  lr: 0.000479  loss: 0.1613 (0.1635)  time: 0.0692  data: 0.0001  max mem: 2598
[14:05:46.760494] Epoch: [61]  [ 60/195]  eta: 0:00:10  lr: 0.000478  loss: 0.1642 (0.1636)  time: 0.0684  data: 0.0001  max mem: 2598
[14:05:48.129650] Epoch: [61]  [ 80/195]  eta: 0:00:08  lr: 0.000478  loss: 0.1618 (0.1636)  time: 0.0684  data: 0.0001  max mem: 2598
[14:05:49.502681] Epoch: [61]  [100/195]  eta: 0:00:06  lr: 0.000478  loss: 0.1634 (0.1638)  time: 0.0686  data: 0.0001  max mem: 2598
[14:05:50.867117] Epoch: [61]  [120/195]  eta: 0:00:05  lr: 0.000478  loss: 0.1648 (0.1641)  time: 0.0682  data: 0.0001  max mem: 2598
[14:05:52.236360] Epoch: [61]  [140/195]  eta: 0:00:03  lr: 0.000478  loss: 0.1641 (0.1641)  time: 0.0684  data: 0.0001  max mem: 2598
[14:05:53.605561] Epoch: [61]  [160/195]  eta: 0:00:02  lr: 0.000477  loss: 0.1654 (0.1643)  time: 0.0684  data: 0.0001  max mem: 2598
[14:05:54.974228] Epoch: [61]  [180/195]  eta: 0:00:01  lr: 0.000477  loss: 0.1648 (0.1645)  time: 0.0684  data: 0.0001  max mem: 2598
[14:05:55.932122] Epoch: [61]  [194/195]  eta: 0:00:00  lr: 0.000477  loss: 0.1636 (0.1643)  time: 0.0683  data: 0.0001  max mem: 2598
[14:05:55.984518] Epoch: [61] Total time: 0:00:13 (0.0709 s / it)
[14:05:55.984589] Averaged stats: lr: 0.000477  loss: 0.1636 (0.1643)
[14:05:55.986839] log_dir: ./output_dir/mae0.3
[14:05:56.370474] Epoch: [62]  [  0/195]  eta: 0:01:14  lr: 0.000477  loss: 0.1664 (0.1664)  time: 0.3822  data: 0.3049  max mem: 2598
[14:05:57.790096] Epoch: [62]  [ 20/195]  eta: 0:00:15  lr: 0.000477  loss: 0.1639 (0.1631)  time: 0.0709  data: 0.0002  max mem: 2598
[14:05:59.214847] Epoch: [62]  [ 40/195]  eta: 0:00:12  lr: 0.000477  loss: 0.1620 (0.1628)  time: 0.0712  data: 0.0002  max mem: 2598
[14:06:00.603346] Epoch: [62]  [ 60/195]  eta: 0:00:10  lr: 0.000476  loss: 0.1620 (0.1628)  time: 0.0694  data: 0.0001  max mem: 2598
[14:06:01.989639] Epoch: [62]  [ 80/195]  eta: 0:00:08  lr: 0.000476  loss: 0.1658 (0.1633)  time: 0.0693  data: 0.0001  max mem: 2598
[14:06:03.362839] Epoch: [62]  [100/195]  eta: 0:00:06  lr: 0.000476  loss: 0.1620 (0.1633)  time: 0.0686  data: 0.0001  max mem: 2598
[14:06:04.730905] Epoch: [62]  [120/195]  eta: 0:00:05  lr: 0.000476  loss: 0.1638 (0.1634)  time: 0.0684  data: 0.0001  max mem: 2598
[14:06:06.120256] Epoch: [62]  [140/195]  eta: 0:00:03  lr: 0.000476  loss: 0.1628 (0.1634)  time: 0.0694  data: 0.0001  max mem: 2598
[14:06:07.525652] Epoch: [62]  [160/195]  eta: 0:00:02  lr: 0.000475  loss: 0.1622 (0.1634)  time: 0.0702  data: 0.0002  max mem: 2598
[14:06:08.929178] Epoch: [62]  [180/195]  eta: 0:00:01  lr: 0.000475  loss: 0.1621 (0.1633)  time: 0.0701  data: 0.0002  max mem: 2598
[14:06:09.896933] Epoch: [62]  [194/195]  eta: 0:00:00  lr: 0.000475  loss: 0.1643 (0.1635)  time: 0.0691  data: 0.0001  max mem: 2598
[14:06:09.942188] Epoch: [62] Total time: 0:00:13 (0.0716 s / it)
[14:06:09.942356] Averaged stats: lr: 0.000475  loss: 0.1643 (0.1635)
[14:06:09.944882] log_dir: ./output_dir/mae0.3
[14:06:10.345770] Epoch: [63]  [  0/195]  eta: 0:01:17  lr: 0.000475  loss: 0.1651 (0.1651)  time: 0.3998  data: 0.2909  max mem: 2598
[14:06:11.782926] Epoch: [63]  [ 20/195]  eta: 0:00:15  lr: 0.000475  loss: 0.1618 (0.1616)  time: 0.0718  data: 0.0002  max mem: 2598
[14:06:13.187343] Epoch: [63]  [ 40/195]  eta: 0:00:12  lr: 0.000474  loss: 0.1603 (0.1623)  time: 0.0702  data: 0.0001  max mem: 2598
[14:06:14.586781] Epoch: [63]  [ 60/195]  eta: 0:00:10  lr: 0.000474  loss: 0.1634 (0.1625)  time: 0.0699  data: 0.0001  max mem: 2598
[14:06:15.983801] Epoch: [63]  [ 80/195]  eta: 0:00:08  lr: 0.000474  loss: 0.1647 (0.1631)  time: 0.0698  data: 0.0001  max mem: 2598
[14:06:17.395900] Epoch: [63]  [100/195]  eta: 0:00:07  lr: 0.000474  loss: 0.1615 (0.1627)  time: 0.0705  data: 0.0001  max mem: 2598
[14:06:18.811123] Epoch: [63]  [120/195]  eta: 0:00:05  lr: 0.000474  loss: 0.1652 (0.1631)  time: 0.0707  data: 0.0002  max mem: 2598
[14:06:20.221885] Epoch: [63]  [140/195]  eta: 0:00:04  lr: 0.000473  loss: 0.1623 (0.1629)  time: 0.0705  data: 0.0001  max mem: 2598
[14:06:21.635783] Epoch: [63]  [160/195]  eta: 0:00:02  lr: 0.000473  loss: 0.1634 (0.1631)  time: 0.0706  data: 0.0001  max mem: 2598
[14:06:23.045723] Epoch: [63]  [180/195]  eta: 0:00:01  lr: 0.000473  loss: 0.1608 (0.1628)  time: 0.0704  data: 0.0001  max mem: 2598
[14:06:24.005617] Epoch: [63]  [194/195]  eta: 0:00:00  lr: 0.000473  loss: 0.1604 (0.1627)  time: 0.0688  data: 0.0001  max mem: 2598
[14:06:24.053315] Epoch: [63] Total time: 0:00:14 (0.0724 s / it)
[14:06:24.053486] Averaged stats: lr: 0.000473  loss: 0.1604 (0.1627)
[14:06:24.056031] log_dir: ./output_dir/mae0.3
[14:06:24.446519] Epoch: [64]  [  0/195]  eta: 0:01:15  lr: 0.000473  loss: 0.1605 (0.1605)  time: 0.3891  data: 0.3047  max mem: 2598
[14:06:25.864476] Epoch: [64]  [ 20/195]  eta: 0:00:15  lr: 0.000473  loss: 0.1573 (0.1599)  time: 0.0709  data: 0.0001  max mem: 2598
[14:06:27.236739] Epoch: [64]  [ 40/195]  eta: 0:00:12  lr: 0.000472  loss: 0.1597 (0.1608)  time: 0.0686  data: 0.0001  max mem: 2598
[14:06:28.622985] Epoch: [64]  [ 60/195]  eta: 0:00:10  lr: 0.000472  loss: 0.1594 (0.1610)  time: 0.0693  data: 0.0001  max mem: 2598
[14:06:30.020286] Epoch: [64]  [ 80/195]  eta: 0:00:08  lr: 0.000472  loss: 0.1627 (0.1616)  time: 0.0698  data: 0.0001  max mem: 2598
[14:06:31.406760] Epoch: [64]  [100/195]  eta: 0:00:06  lr: 0.000472  loss: 0.1612 (0.1616)  time: 0.0693  data: 0.0001  max mem: 2598
[14:06:32.784080] Epoch: [64]  [120/195]  eta: 0:00:05  lr: 0.000471  loss: 0.1611 (0.1618)  time: 0.0688  data: 0.0001  max mem: 2598
[14:06:34.175595] Epoch: [64]  [140/195]  eta: 0:00:03  lr: 0.000471  loss: 0.1631 (0.1619)  time: 0.0695  data: 0.0001  max mem: 2598
[14:06:35.559906] Epoch: [64]  [160/195]  eta: 0:00:02  lr: 0.000471  loss: 0.1654 (0.1622)  time: 0.0692  data: 0.0001  max mem: 2598
[14:06:36.941033] Epoch: [64]  [180/195]  eta: 0:00:01  lr: 0.000471  loss: 0.1613 (0.1622)  time: 0.0690  data: 0.0001  max mem: 2598
[14:06:37.892438] Epoch: [64]  [194/195]  eta: 0:00:00  lr: 0.000470  loss: 0.1610 (0.1622)  time: 0.0683  data: 0.0001  max mem: 2598
[14:06:37.936924] Epoch: [64] Total time: 0:00:13 (0.0712 s / it)
[14:06:37.936987] Averaged stats: lr: 0.000470  loss: 0.1610 (0.1622)
[14:06:37.939216] log_dir: ./output_dir/mae0.3
[14:06:38.328425] Epoch: [65]  [  0/195]  eta: 0:01:15  lr: 0.000470  loss: 0.1674 (0.1674)  time: 0.3877  data: 0.2646  max mem: 2598
[14:06:39.759730] Epoch: [65]  [ 20/195]  eta: 0:00:15  lr: 0.000470  loss: 0.1596 (0.1600)  time: 0.0715  data: 0.0002  max mem: 2598
[14:06:41.165276] Epoch: [65]  [ 40/195]  eta: 0:00:12  lr: 0.000470  loss: 0.1593 (0.1607)  time: 0.0702  data: 0.0002  max mem: 2598
[14:06:42.573787] Epoch: [65]  [ 60/195]  eta: 0:00:10  lr: 0.000470  loss: 0.1610 (0.1610)  time: 0.0704  data: 0.0001  max mem: 2598
[14:06:43.981158] Epoch: [65]  [ 80/195]  eta: 0:00:08  lr: 0.000470  loss: 0.1608 (0.1614)  time: 0.0703  data: 0.0002  max mem: 2598
[14:06:45.374228] Epoch: [65]  [100/195]  eta: 0:00:06  lr: 0.000469  loss: 0.1635 (0.1614)  time: 0.0696  data: 0.0002  max mem: 2598
[14:06:46.769848] Epoch: [65]  [120/195]  eta: 0:00:05  lr: 0.000469  loss: 0.1600 (0.1613)  time: 0.0697  data: 0.0002  max mem: 2598
[14:06:48.166013] Epoch: [65]  [140/195]  eta: 0:00:03  lr: 0.000469  loss: 0.1611 (0.1612)  time: 0.0698  data: 0.0002  max mem: 2598
[14:06:49.543473] Epoch: [65]  [160/195]  eta: 0:00:02  lr: 0.000469  loss: 0.1643 (0.1616)  time: 0.0688  data: 0.0001  max mem: 2598
[14:06:50.899730] Epoch: [65]  [180/195]  eta: 0:00:01  lr: 0.000468  loss: 0.1608 (0.1616)  time: 0.0678  data: 0.0001  max mem: 2598
[14:06:51.845624] Epoch: [65]  [194/195]  eta: 0:00:00  lr: 0.000468  loss: 0.1619 (0.1615)  time: 0.0676  data: 0.0001  max mem: 2598
[14:06:51.914027] Epoch: [65] Total time: 0:00:13 (0.0717 s / it)
[14:06:51.914177] Averaged stats: lr: 0.000468  loss: 0.1619 (0.1615)
[14:06:51.916393] log_dir: ./output_dir/mae0.3
[14:06:52.340697] Epoch: [66]  [  0/195]  eta: 0:01:22  lr: 0.000468  loss: 0.1643 (0.1643)  time: 0.4228  data: 0.3007  max mem: 2598
[14:06:53.742078] Epoch: [66]  [ 20/195]  eta: 0:00:15  lr: 0.000468  loss: 0.1638 (0.1638)  time: 0.0700  data: 0.0001  max mem: 2598
[14:06:55.132478] Epoch: [66]  [ 40/195]  eta: 0:00:12  lr: 0.000468  loss: 0.1606 (0.1623)  time: 0.0695  data: 0.0001  max mem: 2598
[14:06:56.507502] Epoch: [66]  [ 60/195]  eta: 0:00:10  lr: 0.000467  loss: 0.1592 (0.1616)  time: 0.0687  data: 0.0001  max mem: 2598
[14:06:57.884420] Epoch: [66]  [ 80/195]  eta: 0:00:08  lr: 0.000467  loss: 0.1634 (0.1622)  time: 0.0688  data: 0.0002  max mem: 2598
[14:06:59.262687] Epoch: [66]  [100/195]  eta: 0:00:06  lr: 0.000467  loss: 0.1601 (0.1621)  time: 0.0689  data: 0.0001  max mem: 2598
[14:07:00.638583] Epoch: [66]  [120/195]  eta: 0:00:05  lr: 0.000467  loss: 0.1593 (0.1618)  time: 0.0688  data: 0.0001  max mem: 2598
[14:07:02.013260] Epoch: [66]  [140/195]  eta: 0:00:03  lr: 0.000466  loss: 0.1612 (0.1616)  time: 0.0687  data: 0.0001  max mem: 2598
[14:07:03.385518] Epoch: [66]  [160/195]  eta: 0:00:02  lr: 0.000466  loss: 0.1635 (0.1618)  time: 0.0686  data: 0.0001  max mem: 2598
[14:07:04.763289] Epoch: [66]  [180/195]  eta: 0:00:01  lr: 0.000466  loss: 0.1596 (0.1616)  time: 0.0688  data: 0.0001  max mem: 2598
[14:07:05.716496] Epoch: [66]  [194/195]  eta: 0:00:00  lr: 0.000466  loss: 0.1584 (0.1614)  time: 0.0682  data: 0.0001  max mem: 2598
[14:07:05.769790] Epoch: [66] Total time: 0:00:13 (0.0710 s / it)
[14:07:05.769860] Averaged stats: lr: 0.000466  loss: 0.1584 (0.1614)
[14:07:05.772175] log_dir: ./output_dir/mae0.3
[14:07:06.154128] Epoch: [67]  [  0/195]  eta: 0:01:14  lr: 0.000466  loss: 0.1644 (0.1644)  time: 0.3807  data: 0.2987  max mem: 2598
[14:07:07.553262] Epoch: [67]  [ 20/195]  eta: 0:00:14  lr: 0.000465  loss: 0.1592 (0.1602)  time: 0.0699  data: 0.0001  max mem: 2598
[14:07:08.943083] Epoch: [67]  [ 40/195]  eta: 0:00:11  lr: 0.000465  loss: 0.1567 (0.1598)  time: 0.0694  data: 0.0001  max mem: 2598
[14:07:10.329100] Epoch: [67]  [ 60/195]  eta: 0:00:10  lr: 0.000465  loss: 0.1631 (0.1604)  time: 0.0693  data: 0.0001  max mem: 2598
[14:07:11.718508] Epoch: [67]  [ 80/195]  eta: 0:00:08  lr: 0.000465  loss: 0.1591 (0.1603)  time: 0.0694  data: 0.0001  max mem: 2598
[14:07:13.099912] Epoch: [67]  [100/195]  eta: 0:00:06  lr: 0.000464  loss: 0.1610 (0.1602)  time: 0.0690  data: 0.0001  max mem: 2598
[14:07:14.477764] Epoch: [67]  [120/195]  eta: 0:00:05  lr: 0.000464  loss: 0.1614 (0.1603)  time: 0.0689  data: 0.0001  max mem: 2598
[14:07:15.846884] Epoch: [67]  [140/195]  eta: 0:00:03  lr: 0.000464  loss: 0.1614 (0.1605)  time: 0.0684  data: 0.0001  max mem: 2598
[14:07:17.215854] Epoch: [67]  [160/195]  eta: 0:00:02  lr: 0.000464  loss: 0.1590 (0.1604)  time: 0.0684  data: 0.0001  max mem: 2598
[14:07:18.581984] Epoch: [67]  [180/195]  eta: 0:00:01  lr: 0.000463  loss: 0.1593 (0.1603)  time: 0.0683  data: 0.0001  max mem: 2598
[14:07:19.530068] Epoch: [67]  [194/195]  eta: 0:00:00  lr: 0.000463  loss: 0.1587 (0.1603)  time: 0.0677  data: 0.0001  max mem: 2598
[14:07:19.573781] Epoch: [67] Total time: 0:00:13 (0.0708 s / it)
[14:07:19.573847] Averaged stats: lr: 0.000463  loss: 0.1587 (0.1603)
[14:07:19.576124] log_dir: ./output_dir/mae0.3
[14:07:19.944412] Epoch: [68]  [  0/195]  eta: 0:01:11  lr: 0.000463  loss: 0.1651 (0.1651)  time: 0.3670  data: 0.2783  max mem: 2598
[14:07:21.392575] Epoch: [68]  [ 20/195]  eta: 0:00:15  lr: 0.000463  loss: 0.1570 (0.1601)  time: 0.0724  data: 0.0001  max mem: 2598
[14:07:22.798706] Epoch: [68]  [ 40/195]  eta: 0:00:12  lr: 0.000463  loss: 0.1579 (0.1595)  time: 0.0703  data: 0.0001  max mem: 2598
[14:07:24.223850] Epoch: [68]  [ 60/195]  eta: 0:00:10  lr: 0.000462  loss: 0.1604 (0.1601)  time: 0.0712  data: 0.0001  max mem: 2598
[14:07:25.630780] Epoch: [68]  [ 80/195]  eta: 0:00:08  lr: 0.000462  loss: 0.1595 (0.1603)  time: 0.0703  data: 0.0002  max mem: 2598
[14:07:27.036280] Epoch: [68]  [100/195]  eta: 0:00:07  lr: 0.000462  loss: 0.1616 (0.1605)  time: 0.0702  data: 0.0001  max mem: 2598
[14:07:28.456861] Epoch: [68]  [120/195]  eta: 0:00:05  lr: 0.000462  loss: 0.1625 (0.1607)  time: 0.0710  data: 0.0002  max mem: 2598
[14:07:29.869584] Epoch: [68]  [140/195]  eta: 0:00:04  lr: 0.000461  loss: 0.1621 (0.1606)  time: 0.0706  data: 0.0001  max mem: 2598
[14:07:31.277440] Epoch: [68]  [160/195]  eta: 0:00:02  lr: 0.000461  loss: 0.1593 (0.1606)  time: 0.0704  data: 0.0001  max mem: 2598
[14:07:32.677667] Epoch: [68]  [180/195]  eta: 0:00:01  lr: 0.000461  loss: 0.1597 (0.1605)  time: 0.0700  data: 0.0001  max mem: 2598
[14:07:33.642856] Epoch: [68]  [194/195]  eta: 0:00:00  lr: 0.000461  loss: 0.1606 (0.1605)  time: 0.0690  data: 0.0001  max mem: 2598
[14:07:33.710843] Epoch: [68] Total time: 0:00:14 (0.0725 s / it)
[14:07:33.710911] Averaged stats: lr: 0.000461  loss: 0.1606 (0.1605)
[14:07:33.713412] log_dir: ./output_dir/mae0.3
[14:07:34.118139] Epoch: [69]  [  0/195]  eta: 0:01:18  lr: 0.000461  loss: 0.1630 (0.1630)  time: 0.4033  data: 0.3007  max mem: 2598
[14:07:35.546611] Epoch: [69]  [ 20/195]  eta: 0:00:15  lr: 0.000460  loss: 0.1549 (0.1582)  time: 0.0714  data: 0.0001  max mem: 2598
[14:07:36.955122] Epoch: [69]  [ 40/195]  eta: 0:00:12  lr: 0.000460  loss: 0.1563 (0.1576)  time: 0.0704  data: 0.0002  max mem: 2598
[14:07:38.361410] Epoch: [69]  [ 60/195]  eta: 0:00:10  lr: 0.000460  loss: 0.1587 (0.1577)  time: 0.0703  data: 0.0001  max mem: 2598
[14:07:39.802881] Epoch: [69]  [ 80/195]  eta: 0:00:08  lr: 0.000459  loss: 0.1603 (0.1582)  time: 0.0720  data: 0.0002  max mem: 2598
[14:07:41.219199] Epoch: [69]  [100/195]  eta: 0:00:07  lr: 0.000459  loss: 0.1579 (0.1586)  time: 0.0708  data: 0.0002  max mem: 2598
[14:07:42.623791] Epoch: [69]  [120/195]  eta: 0:00:05  lr: 0.000459  loss: 0.1587 (0.1588)  time: 0.0702  data: 0.0001  max mem: 2598
[14:07:44.028616] Epoch: [69]  [140/195]  eta: 0:00:04  lr: 0.000459  loss: 0.1609 (0.1590)  time: 0.0702  data: 0.0001  max mem: 2598
[14:07:45.436807] Epoch: [69]  [160/195]  eta: 0:00:02  lr: 0.000458  loss: 0.1614 (0.1591)  time: 0.0704  data: 0.0001  max mem: 2598
[14:07:46.836116] Epoch: [69]  [180/195]  eta: 0:00:01  lr: 0.000458  loss: 0.1572 (0.1590)  time: 0.0699  data: 0.0001  max mem: 2598
[14:07:47.801664] Epoch: [69]  [194/195]  eta: 0:00:00  lr: 0.000458  loss: 0.1599 (0.1591)  time: 0.0691  data: 0.0001  max mem: 2598
[14:07:47.858213] Epoch: [69] Total time: 0:00:14 (0.0725 s / it)
[14:07:47.858397] Averaged stats: lr: 0.000458  loss: 0.1599 (0.1591)
[14:07:47.861014] log_dir: ./output_dir/mae0.3
[14:07:48.245637] Epoch: [70]  [  0/195]  eta: 0:01:14  lr: 0.000458  loss: 0.1499 (0.1499)  time: 0.3822  data: 0.2906  max mem: 2598
[14:07:49.640642] Epoch: [70]  [ 20/195]  eta: 0:00:14  lr: 0.000458  loss: 0.1560 (0.1560)  time: 0.0697  data: 0.0001  max mem: 2598
[14:07:51.047217] Epoch: [70]  [ 40/195]  eta: 0:00:12  lr: 0.000457  loss: 0.1582 (0.1575)  time: 0.0703  data: 0.0001  max mem: 2598
[14:07:52.458139] Epoch: [70]  [ 60/195]  eta: 0:00:10  lr: 0.000457  loss: 0.1579 (0.1581)  time: 0.0705  data: 0.0001  max mem: 2598
[14:07:53.862720] Epoch: [70]  [ 80/195]  eta: 0:00:08  lr: 0.000457  loss: 0.1589 (0.1587)  time: 0.0702  data: 0.0001  max mem: 2598
[14:07:55.272443] Epoch: [70]  [100/195]  eta: 0:00:06  lr: 0.000456  loss: 0.1617 (0.1592)  time: 0.0704  data: 0.0001  max mem: 2598
[14:07:56.679870] Epoch: [70]  [120/195]  eta: 0:00:05  lr: 0.000456  loss: 0.1555 (0.1589)  time: 0.0703  data: 0.0001  max mem: 2598
[14:07:58.089817] Epoch: [70]  [140/195]  eta: 0:00:03  lr: 0.000456  loss: 0.1561 (0.1587)  time: 0.0704  data: 0.0001  max mem: 2598
[14:07:59.496828] Epoch: [70]  [160/195]  eta: 0:00:02  lr: 0.000456  loss: 0.1566 (0.1588)  time: 0.0703  data: 0.0001  max mem: 2598
[14:08:00.898475] Epoch: [70]  [180/195]  eta: 0:00:01  lr: 0.000455  loss: 0.1618 (0.1590)  time: 0.0700  data: 0.0001  max mem: 2598
[14:08:01.872601] Epoch: [70]  [194/195]  eta: 0:00:00  lr: 0.000455  loss: 0.1595 (0.1590)  time: 0.0695  data: 0.0001  max mem: 2598
[14:08:01.937521] Epoch: [70] Total time: 0:00:14 (0.0722 s / it)
[14:08:01.937725] Averaged stats: lr: 0.000455  loss: 0.1595 (0.1590)
[14:08:01.940267] log_dir: ./output_dir/mae0.3
[14:08:02.323842] Epoch: [71]  [  0/195]  eta: 0:01:14  lr: 0.000455  loss: 0.1532 (0.1532)  time: 0.3828  data: 0.2977  max mem: 2598
[14:08:03.736191] Epoch: [71]  [ 20/195]  eta: 0:00:14  lr: 0.000455  loss: 0.1593 (0.1576)  time: 0.0706  data: 0.0001  max mem: 2598
[14:08:05.140112] Epoch: [71]  [ 40/195]  eta: 0:00:12  lr: 0.000455  loss: 0.1565 (0.1577)  time: 0.0702  data: 0.0001  max mem: 2598
[14:08:06.562099] Epoch: [71]  [ 60/195]  eta: 0:00:10  lr: 0.000454  loss: 0.1583 (0.1577)  time: 0.0711  data: 0.0001  max mem: 2598
[14:08:07.968878] Epoch: [71]  [ 80/195]  eta: 0:00:08  lr: 0.000454  loss: 0.1549 (0.1576)  time: 0.0703  data: 0.0001  max mem: 2598
[14:08:09.380844] Epoch: [71]  [100/195]  eta: 0:00:06  lr: 0.000454  loss: 0.1550 (0.1573)  time: 0.0706  data: 0.0001  max mem: 2598
[14:08:10.782818] Epoch: [71]  [120/195]  eta: 0:00:05  lr: 0.000453  loss: 0.1587 (0.1576)  time: 0.0701  data: 0.0001  max mem: 2598
[14:08:12.184757] Epoch: [71]  [140/195]  eta: 0:00:03  lr: 0.000453  loss: 0.1585 (0.1577)  time: 0.0701  data: 0.0001  max mem: 2598
[14:08:13.583417] Epoch: [71]  [160/195]  eta: 0:00:02  lr: 0.000453  loss: 0.1605 (0.1580)  time: 0.0699  data: 0.0001  max mem: 2598
[14:08:14.978647] Epoch: [71]  [180/195]  eta: 0:00:01  lr: 0.000452  loss: 0.1586 (0.1582)  time: 0.0697  data: 0.0001  max mem: 2598
[14:08:15.945595] Epoch: [71]  [194/195]  eta: 0:00:00  lr: 0.000452  loss: 0.1573 (0.1581)  time: 0.0691  data: 0.0001  max mem: 2598
[14:08:16.020943] Epoch: [71] Total time: 0:00:14 (0.0722 s / it)
[14:08:16.021013] Averaged stats: lr: 0.000452  loss: 0.1573 (0.1581)
[14:08:16.023504] log_dir: ./output_dir/mae0.3
[14:08:16.399485] Epoch: [72]  [  0/195]  eta: 0:01:13  lr: 0.000452  loss: 0.1612 (0.1612)  time: 0.3746  data: 0.2949  max mem: 2598
[14:08:17.821361] Epoch: [72]  [ 20/195]  eta: 0:00:14  lr: 0.000452  loss: 0.1548 (0.1562)  time: 0.0710  data: 0.0002  max mem: 2598
[14:08:19.229189] Epoch: [72]  [ 40/195]  eta: 0:00:12  lr: 0.000452  loss: 0.1568 (0.1578)  time: 0.0703  data: 0.0001  max mem: 2598
[14:08:20.634785] Epoch: [72]  [ 60/195]  eta: 0:00:10  lr: 0.000451  loss: 0.1568 (0.1575)  time: 0.0702  data: 0.0001  max mem: 2598
[14:08:22.043413] Epoch: [72]  [ 80/195]  eta: 0:00:08  lr: 0.000451  loss: 0.1571 (0.1577)  time: 0.0704  data: 0.0001  max mem: 2598
[14:08:23.455109] Epoch: [72]  [100/195]  eta: 0:00:06  lr: 0.000451  loss: 0.1595 (0.1577)  time: 0.0705  data: 0.0001  max mem: 2598
[14:08:24.860210] Epoch: [72]  [120/195]  eta: 0:00:05  lr: 0.000450  loss: 0.1577 (0.1578)  time: 0.0702  data: 0.0001  max mem: 2598
[14:08:26.267598] Epoch: [72]  [140/195]  eta: 0:00:03  lr: 0.000450  loss: 0.1558 (0.1576)  time: 0.0703  data: 0.0001  max mem: 2598
[14:08:27.661032] Epoch: [72]  [160/195]  eta: 0:00:02  lr: 0.000450  loss: 0.1554 (0.1576)  time: 0.0696  data: 0.0001  max mem: 2598
[14:08:29.048369] Epoch: [72]  [180/195]  eta: 0:00:01  lr: 0.000450  loss: 0.1575 (0.1577)  time: 0.0693  data: 0.0001  max mem: 2598
[14:08:29.998921] Epoch: [72]  [194/195]  eta: 0:00:00  lr: 0.000449  loss: 0.1568 (0.1577)  time: 0.0679  data: 0.0001  max mem: 2598
[14:08:30.067889] Epoch: [72] Total time: 0:00:14 (0.0720 s / it)
[14:08:30.068241] Averaged stats: lr: 0.000449  loss: 0.1568 (0.1577)
[14:08:30.071821] log_dir: ./output_dir/mae0.3
[14:08:30.493852] Epoch: [73]  [  0/195]  eta: 0:01:21  lr: 0.000449  loss: 0.1630 (0.1630)  time: 0.4204  data: 0.3050  max mem: 2598
[14:08:31.935395] Epoch: [73]  [ 20/195]  eta: 0:00:15  lr: 0.000449  loss: 0.1558 (0.1555)  time: 0.0720  data: 0.0001  max mem: 2598
[14:08:33.343743] Epoch: [73]  [ 40/195]  eta: 0:00:12  lr: 0.000449  loss: 0.1587 (0.1564)  time: 0.0704  data: 0.0001  max mem: 2598
[14:08:34.763203] Epoch: [73]  [ 60/195]  eta: 0:00:10  lr: 0.000448  loss: 0.1581 (0.1566)  time: 0.0709  data: 0.0001  max mem: 2598
[14:08:36.172624] Epoch: [73]  [ 80/195]  eta: 0:00:08  lr: 0.000448  loss: 0.1573 (0.1570)  time: 0.0704  data: 0.0001  max mem: 2598
[14:08:37.580353] Epoch: [73]  [100/195]  eta: 0:00:07  lr: 0.000448  loss: 0.1555 (0.1567)  time: 0.0703  data: 0.0001  max mem: 2598
[14:08:38.988252] Epoch: [73]  [120/195]  eta: 0:00:05  lr: 0.000447  loss: 0.1580 (0.1567)  time: 0.0703  data: 0.0001  max mem: 2598
[14:08:40.398487] Epoch: [73]  [140/195]  eta: 0:00:04  lr: 0.000447  loss: 0.1574 (0.1568)  time: 0.0705  data: 0.0001  max mem: 2598
[14:08:41.810432] Epoch: [73]  [160/195]  eta: 0:00:02  lr: 0.000447  loss: 0.1589 (0.1570)  time: 0.0705  data: 0.0001  max mem: 2598
[14:08:43.220292] Epoch: [73]  [180/195]  eta: 0:00:01  lr: 0.000447  loss: 0.1570 (0.1571)  time: 0.0705  data: 0.0001  max mem: 2598
[14:08:44.186516] Epoch: [73]  [194/195]  eta: 0:00:00  lr: 0.000446  loss: 0.1567 (0.1571)  time: 0.0693  data: 0.0001  max mem: 2598
[14:08:44.261043] Epoch: [73] Total time: 0:00:14 (0.0728 s / it)
[14:08:44.261138] Averaged stats: lr: 0.000446  loss: 0.1567 (0.1571)
[14:08:44.264893] log_dir: ./output_dir/mae0.3
[14:08:44.720558] Epoch: [74]  [  0/195]  eta: 0:01:28  lr: 0.000446  loss: 0.1564 (0.1564)  time: 0.4541  data: 0.3255  max mem: 2598
[14:08:46.164253] Epoch: [74]  [ 20/195]  eta: 0:00:15  lr: 0.000446  loss: 0.1541 (0.1556)  time: 0.0721  data: 0.0001  max mem: 2598
[14:08:47.567165] Epoch: [74]  [ 40/195]  eta: 0:00:12  lr: 0.000446  loss: 0.1547 (0.1555)  time: 0.0701  data: 0.0001  max mem: 2598
[14:08:49.007872] Epoch: [74]  [ 60/195]  eta: 0:00:10  lr: 0.000445  loss: 0.1593 (0.1563)  time: 0.0720  data: 0.0001  max mem: 2598
[14:08:50.402579] Epoch: [74]  [ 80/195]  eta: 0:00:08  lr: 0.000445  loss: 0.1571 (0.1564)  time: 0.0697  data: 0.0001  max mem: 2598
[14:08:51.809652] Epoch: [74]  [100/195]  eta: 0:00:07  lr: 0.000445  loss: 0.1534 (0.1559)  time: 0.0703  data: 0.0001  max mem: 2598
[14:08:53.203817] Epoch: [74]  [120/195]  eta: 0:00:05  lr: 0.000444  loss: 0.1548 (0.1561)  time: 0.0697  data: 0.0001  max mem: 2598
[14:08:54.600795] Epoch: [74]  [140/195]  eta: 0:00:04  lr: 0.000444  loss: 0.1555 (0.1561)  time: 0.0698  data: 0.0001  max mem: 2598
[14:08:55.999521] Epoch: [74]  [160/195]  eta: 0:00:02  lr: 0.000444  loss: 0.1585 (0.1564)  time: 0.0699  data: 0.0001  max mem: 2598
[14:08:57.378699] Epoch: [74]  [180/195]  eta: 0:00:01  lr: 0.000443  loss: 0.1599 (0.1566)  time: 0.0689  data: 0.0001  max mem: 2598
[14:08:58.330247] Epoch: [74]  [194/195]  eta: 0:00:00  lr: 0.000443  loss: 0.1551 (0.1565)  time: 0.0680  data: 0.0001  max mem: 2598
[14:08:58.398914] Epoch: [74] Total time: 0:00:14 (0.0725 s / it)
[14:08:58.399017] Averaged stats: lr: 0.000443  loss: 0.1551 (0.1565)
[14:08:58.402104] log_dir: ./output_dir/mae0.3
[14:08:58.784112] Epoch: [75]  [  0/195]  eta: 0:01:14  lr: 0.000443  loss: 0.1617 (0.1617)  time: 0.3807  data: 0.2821  max mem: 2598
[14:09:00.249883] Epoch: [75]  [ 20/195]  eta: 0:00:15  lr: 0.000443  loss: 0.1550 (0.1542)  time: 0.0732  data: 0.0002  max mem: 2598
[14:09:01.673525] Epoch: [75]  [ 40/195]  eta: 0:00:12  lr: 0.000443  loss: 0.1530 (0.1542)  time: 0.0711  data: 0.0002  max mem: 2598
[14:09:03.098070] Epoch: [75]  [ 60/195]  eta: 0:00:10  lr: 0.000442  loss: 0.1569 (0.1547)  time: 0.0712  data: 0.0002  max mem: 2598
[14:09:04.537437] Epoch: [75]  [ 80/195]  eta: 0:00:08  lr: 0.000442  loss: 0.1555 (0.1551)  time: 0.0719  data: 0.0001  max mem: 2598
[14:09:05.964643] Epoch: [75]  [100/195]  eta: 0:00:07  lr: 0.000442  loss: 0.1550 (0.1550)  time: 0.0713  data: 0.0002  max mem: 2598
[14:09:07.386283] Epoch: [75]  [120/195]  eta: 0:00:05  lr: 0.000441  loss: 0.1557 (0.1551)  time: 0.0710  data: 0.0002  max mem: 2598
[14:09:08.802150] Epoch: [75]  [140/195]  eta: 0:00:04  lr: 0.000441  loss: 0.1542 (0.1551)  time: 0.0707  data: 0.0002  max mem: 2598
[14:09:10.210186] Epoch: [75]  [160/195]  eta: 0:00:02  lr: 0.000441  loss: 0.1579 (0.1554)  time: 0.0704  data: 0.0001  max mem: 2598
[14:09:11.613343] Epoch: [75]  [180/195]  eta: 0:00:01  lr: 0.000440  loss: 0.1534 (0.1554)  time: 0.0701  data: 0.0001  max mem: 2598
[14:09:12.583076] Epoch: [75]  [194/195]  eta: 0:00:00  lr: 0.000440  loss: 0.1547 (0.1556)  time: 0.0693  data: 0.0001  max mem: 2598
[14:09:12.631794] Epoch: [75] Total time: 0:00:14 (0.0730 s / it)
[14:09:12.632025] Averaged stats: lr: 0.000440  loss: 0.1547 (0.1556)
[14:09:12.634527] log_dir: ./output_dir/mae0.3
[14:09:13.041761] Epoch: [76]  [  0/195]  eta: 0:01:19  lr: 0.000440  loss: 0.1610 (0.1610)  time: 0.4063  data: 0.3200  max mem: 2598
[14:09:14.456076] Epoch: [76]  [ 20/195]  eta: 0:00:15  lr: 0.000440  loss: 0.1536 (0.1535)  time: 0.0707  data: 0.0001  max mem: 2598
[14:09:15.850894] Epoch: [76]  [ 40/195]  eta: 0:00:12  lr: 0.000439  loss: 0.1561 (0.1549)  time: 0.0697  data: 0.0001  max mem: 2598
[14:09:17.250606] Epoch: [76]  [ 60/195]  eta: 0:00:10  lr: 0.000439  loss: 0.1535 (0.1546)  time: 0.0700  data: 0.0001  max mem: 2598
[14:09:18.644105] Epoch: [76]  [ 80/195]  eta: 0:00:08  lr: 0.000439  loss: 0.1563 (0.1549)  time: 0.0696  data: 0.0001  max mem: 2598
[14:09:20.035581] Epoch: [76]  [100/195]  eta: 0:00:06  lr: 0.000438  loss: 0.1554 (0.1549)  time: 0.0695  data: 0.0001  max mem: 2598
[14:09:21.420264] Epoch: [76]  [120/195]  eta: 0:00:05  lr: 0.000438  loss: 0.1560 (0.1550)  time: 0.0692  data: 0.0001  max mem: 2598
[14:09:22.810260] Epoch: [76]  [140/195]  eta: 0:00:03  lr: 0.000438  loss: 0.1546 (0.1551)  time: 0.0695  data: 0.0001  max mem: 2598
[14:09:24.199730] Epoch: [76]  [160/195]  eta: 0:00:02  lr: 0.000437  loss: 0.1575 (0.1553)  time: 0.0694  data: 0.0001  max mem: 2598
[14:09:25.586945] Epoch: [76]  [180/195]  eta: 0:00:01  lr: 0.000437  loss: 0.1548 (0.1553)  time: 0.0693  data: 0.0001  max mem: 2598
[14:09:26.537387] Epoch: [76]  [194/195]  eta: 0:00:00  lr: 0.000437  loss: 0.1533 (0.1553)  time: 0.0678  data: 0.0001  max mem: 2598
[14:09:26.592100] Epoch: [76] Total time: 0:00:13 (0.0716 s / it)
[14:09:26.592166] Averaged stats: lr: 0.000437  loss: 0.1533 (0.1553)
[14:09:26.594420] log_dir: ./output_dir/mae0.3
[14:09:27.007280] Epoch: [77]  [  0/195]  eta: 0:01:20  lr: 0.000437  loss: 0.1531 (0.1531)  time: 0.4114  data: 0.2935  max mem: 2598
[14:09:28.465925] Epoch: [77]  [ 20/195]  eta: 0:00:15  lr: 0.000437  loss: 0.1549 (0.1556)  time: 0.0729  data: 0.0002  max mem: 2598
[14:09:29.888882] Epoch: [77]  [ 40/195]  eta: 0:00:12  lr: 0.000436  loss: 0.1527 (0.1550)  time: 0.0711  data: 0.0001  max mem: 2598
[14:09:31.291508] Epoch: [77]  [ 60/195]  eta: 0:00:10  lr: 0.000436  loss: 0.1531 (0.1545)  time: 0.0701  data: 0.0001  max mem: 2598
[14:09:32.696247] Epoch: [77]  [ 80/195]  eta: 0:00:08  lr: 0.000436  loss: 0.1526 (0.1545)  time: 0.0702  data: 0.0001  max mem: 2598
[14:09:34.107276] Epoch: [77]  [100/195]  eta: 0:00:07  lr: 0.000435  loss: 0.1551 (0.1544)  time: 0.0705  data: 0.0001  max mem: 2598
[14:09:35.515212] Epoch: [77]  [120/195]  eta: 0:00:05  lr: 0.000435  loss: 0.1533 (0.1546)  time: 0.0704  data: 0.0001  max mem: 2598
[14:09:36.917655] Epoch: [77]  [140/195]  eta: 0:00:04  lr: 0.000435  loss: 0.1513 (0.1544)  time: 0.0701  data: 0.0001  max mem: 2598
[14:09:38.317469] Epoch: [77]  [160/195]  eta: 0:00:02  lr: 0.000434  loss: 0.1571 (0.1546)  time: 0.0700  data: 0.0002  max mem: 2598
[14:09:39.710827] Epoch: [77]  [180/195]  eta: 0:00:01  lr: 0.000434  loss: 0.1530 (0.1546)  time: 0.0696  data: 0.0001  max mem: 2598
[14:09:40.679677] Epoch: [77]  [194/195]  eta: 0:00:00  lr: 0.000434  loss: 0.1526 (0.1545)  time: 0.0691  data: 0.0001  max mem: 2598
[14:09:40.732419] Epoch: [77] Total time: 0:00:14 (0.0725 s / it)
[14:09:40.732486] Averaged stats: lr: 0.000434  loss: 0.1526 (0.1545)
[14:09:40.734975] log_dir: ./output_dir/mae0.3
[14:09:41.115165] Epoch: [78]  [  0/195]  eta: 0:01:13  lr: 0.000434  loss: 0.1601 (0.1601)  time: 0.3792  data: 0.2953  max mem: 2598
[14:09:42.561995] Epoch: [78]  [ 20/195]  eta: 0:00:15  lr: 0.000433  loss: 0.1500 (0.1516)  time: 0.0723  data: 0.0001  max mem: 2598
[14:09:43.998718] Epoch: [78]  [ 40/195]  eta: 0:00:12  lr: 0.000433  loss: 0.1510 (0.1524)  time: 0.0718  data: 0.0002  max mem: 2598
[14:09:45.434428] Epoch: [78]  [ 60/195]  eta: 0:00:10  lr: 0.000433  loss: 0.1547 (0.1528)  time: 0.0717  data: 0.0002  max mem: 2598
[14:09:46.827931] Epoch: [78]  [ 80/195]  eta: 0:00:08  lr: 0.000432  loss: 0.1507 (0.1528)  time: 0.0696  data: 0.0001  max mem: 2598
[14:09:48.220210] Epoch: [78]  [100/195]  eta: 0:00:07  lr: 0.000432  loss: 0.1536 (0.1530)  time: 0.0696  data: 0.0001  max mem: 2598
[14:09:49.621552] Epoch: [78]  [120/195]  eta: 0:00:05  lr: 0.000432  loss: 0.1519 (0.1532)  time: 0.0700  data: 0.0001  max mem: 2598
[14:09:51.008918] Epoch: [78]  [140/195]  eta: 0:00:04  lr: 0.000431  loss: 0.1533 (0.1533)  time: 0.0693  data: 0.0001  max mem: 2598
[14:09:52.400729] Epoch: [78]  [160/195]  eta: 0:00:02  lr: 0.000431  loss: 0.1530 (0.1534)  time: 0.0696  data: 0.0001  max mem: 2598
[14:09:53.781665] Epoch: [78]  [180/195]  eta: 0:00:01  lr: 0.000430  loss: 0.1541 (0.1534)  time: 0.0690  data: 0.0001  max mem: 2598
[14:09:54.734296] Epoch: [78]  [194/195]  eta: 0:00:00  lr: 0.000430  loss: 0.1552 (0.1535)  time: 0.0681  data: 0.0001  max mem: 2598
[14:09:54.788000] Epoch: [78] Total time: 0:00:14 (0.0721 s / it)
[14:09:54.788064] Averaged stats: lr: 0.000430  loss: 0.1552 (0.1535)
[14:09:54.790298] log_dir: ./output_dir/mae0.3
[14:09:55.190414] Epoch: [79]  [  0/195]  eta: 0:01:17  lr: 0.000430  loss: 0.1579 (0.1579)  time: 0.3986  data: 0.2729  max mem: 2598
[14:09:56.637115] Epoch: [79]  [ 20/195]  eta: 0:00:15  lr: 0.000430  loss: 0.1525 (0.1530)  time: 0.0723  data: 0.0002  max mem: 2598
[14:09:58.040749] Epoch: [79]  [ 40/195]  eta: 0:00:12  lr: 0.000430  loss: 0.1514 (0.1528)  time: 0.0701  data: 0.0001  max mem: 2598
[14:09:59.445959] Epoch: [79]  [ 60/195]  eta: 0:00:10  lr: 0.000429  loss: 0.1540 (0.1530)  time: 0.0702  data: 0.0001  max mem: 2598
[14:10:00.854944] Epoch: [79]  [ 80/195]  eta: 0:00:08  lr: 0.000429  loss: 0.1512 (0.1527)  time: 0.0704  data: 0.0001  max mem: 2598
[14:10:02.259879] Epoch: [79]  [100/195]  eta: 0:00:07  lr: 0.000428  loss: 0.1554 (0.1534)  time: 0.0702  data: 0.0001  max mem: 2598
[14:10:03.659933] Epoch: [79]  [120/195]  eta: 0:00:05  lr: 0.000428  loss: 0.1499 (0.1531)  time: 0.0700  data: 0.0001  max mem: 2598
[14:10:05.067292] Epoch: [79]  [140/195]  eta: 0:00:04  lr: 0.000428  loss: 0.1540 (0.1533)  time: 0.0703  data: 0.0001  max mem: 2598
[14:10:06.479540] Epoch: [79]  [160/195]  eta: 0:00:02  lr: 0.000427  loss: 0.1522 (0.1534)  time: 0.0706  data: 0.0001  max mem: 2598
[14:10:07.880627] Epoch: [79]  [180/195]  eta: 0:00:01  lr: 0.000427  loss: 0.1526 (0.1534)  time: 0.0700  data: 0.0001  max mem: 2598
[14:10:08.847711] Epoch: [79]  [194/195]  eta: 0:00:00  lr: 0.000427  loss: 0.1526 (0.1533)  time: 0.0691  data: 0.0001  max mem: 2598
[14:10:08.897770] Epoch: [79] Total time: 0:00:14 (0.0723 s / it)
[14:10:08.897858] Averaged stats: lr: 0.000427  loss: 0.1526 (0.1533)
[14:10:08.901237] log_dir: ./output_dir/mae0.3
[14:10:09.320441] Epoch: [80]  [  0/195]  eta: 0:01:21  lr: 0.000427  loss: 0.1537 (0.1537)  time: 0.4181  data: 0.3061  max mem: 2598
[14:10:10.764554] Epoch: [80]  [ 20/195]  eta: 0:00:15  lr: 0.000426  loss: 0.1511 (0.1534)  time: 0.0722  data: 0.0001  max mem: 2598
[14:10:12.168597] Epoch: [80]  [ 40/195]  eta: 0:00:12  lr: 0.000426  loss: 0.1544 (0.1536)  time: 0.0702  data: 0.0001  max mem: 2598
[14:10:13.584584] Epoch: [80]  [ 60/195]  eta: 0:00:10  lr: 0.000426  loss: 0.1496 (0.1533)  time: 0.0708  data: 0.0001  max mem: 2598
[14:10:14.996564] Epoch: [80]  [ 80/195]  eta: 0:00:08  lr: 0.000425  loss: 0.1536 (0.1534)  time: 0.0706  data: 0.0001  max mem: 2598
[14:10:16.400898] Epoch: [80]  [100/195]  eta: 0:00:07  lr: 0.000425  loss: 0.1521 (0.1532)  time: 0.0702  data: 0.0001  max mem: 2598
[14:10:17.806529] Epoch: [80]  [120/195]  eta: 0:00:05  lr: 0.000425  loss: 0.1533 (0.1530)  time: 0.0702  data: 0.0001  max mem: 2598
[14:10:19.195444] Epoch: [80]  [140/195]  eta: 0:00:04  lr: 0.000424  loss: 0.1523 (0.1530)  time: 0.0694  data: 0.0001  max mem: 2598
[14:10:20.602747] Epoch: [80]  [160/195]  eta: 0:00:02  lr: 0.000424  loss: 0.1510 (0.1528)  time: 0.0703  data: 0.0001  max mem: 2598
[14:10:22.008039] Epoch: [80]  [180/195]  eta: 0:00:01  lr: 0.000424  loss: 0.1515 (0.1529)  time: 0.0702  data: 0.0001  max mem: 2598
[14:10:22.975886] Epoch: [80]  [194/195]  eta: 0:00:00  lr: 0.000423  loss: 0.1507 (0.1529)  time: 0.0691  data: 0.0001  max mem: 2598
[14:10:23.022925] Epoch: [80] Total time: 0:00:14 (0.0724 s / it)
[14:10:23.023014] Averaged stats: lr: 0.000423  loss: 0.1507 (0.1529)
[14:10:23.209824] log_dir: ./output_dir/mae0.3
[14:10:23.619437] Epoch: [81]  [  0/195]  eta: 0:01:19  lr: 0.000423  loss: 0.1561 (0.1561)  time: 0.4082  data: 0.3069  max mem: 2598
[14:10:25.033727] Epoch: [81]  [ 20/195]  eta: 0:00:15  lr: 0.000423  loss: 0.1505 (0.1505)  time: 0.0707  data: 0.0002  max mem: 2598
[14:10:26.412844] Epoch: [81]  [ 40/195]  eta: 0:00:12  lr: 0.000423  loss: 0.1530 (0.1513)  time: 0.0689  data: 0.0001  max mem: 2598
[14:10:27.788971] Epoch: [81]  [ 60/195]  eta: 0:00:10  lr: 0.000422  loss: 0.1529 (0.1520)  time: 0.0688  data: 0.0001  max mem: 2598
[14:10:29.198794] Epoch: [81]  [ 80/195]  eta: 0:00:08  lr: 0.000422  loss: 0.1540 (0.1524)  time: 0.0704  data: 0.0001  max mem: 2598
[14:10:30.609161] Epoch: [81]  [100/195]  eta: 0:00:06  lr: 0.000421  loss: 0.1533 (0.1525)  time: 0.0705  data: 0.0001  max mem: 2598
[14:10:32.021260] Epoch: [81]  [120/195]  eta: 0:00:05  lr: 0.000421  loss: 0.1507 (0.1525)  time: 0.0706  data: 0.0001  max mem: 2598
[14:10:33.436873] Epoch: [81]  [140/195]  eta: 0:00:03  lr: 0.000421  loss: 0.1506 (0.1523)  time: 0.0707  data: 0.0001  max mem: 2598
[14:10:34.849102] Epoch: [81]  [160/195]  eta: 0:00:02  lr: 0.000420  loss: 0.1513 (0.1522)  time: 0.0706  data: 0.0001  max mem: 2598
[14:10:36.256774] Epoch: [81]  [180/195]  eta: 0:00:01  lr: 0.000420  loss: 0.1526 (0.1523)  time: 0.0703  data: 0.0001  max mem: 2598
[14:10:37.224968] Epoch: [81]  [194/195]  eta: 0:00:00  lr: 0.000420  loss: 0.1478 (0.1522)  time: 0.0691  data: 0.0001  max mem: 2598
[14:10:37.271097] Epoch: [81] Total time: 0:00:14 (0.0721 s / it)
[14:10:37.271377] Averaged stats: lr: 0.000420  loss: 0.1478 (0.1522)
[14:10:37.273880] log_dir: ./output_dir/mae0.3
[14:10:37.690715] Epoch: [82]  [  0/195]  eta: 0:01:20  lr: 0.000420  loss: 0.1574 (0.1574)  time: 0.4151  data: 0.2889  max mem: 2598
[14:10:39.143760] Epoch: [82]  [ 20/195]  eta: 0:00:15  lr: 0.000419  loss: 0.1474 (0.1514)  time: 0.0726  data: 0.0002  max mem: 2598
[14:10:40.549921] Epoch: [82]  [ 40/195]  eta: 0:00:12  lr: 0.000419  loss: 0.1501 (0.1512)  time: 0.0703  data: 0.0001  max mem: 2598
[14:10:41.956215] Epoch: [82]  [ 60/195]  eta: 0:00:10  lr: 0.000419  loss: 0.1516 (0.1512)  time: 0.0703  data: 0.0001  max mem: 2598
[14:10:43.364593] Epoch: [82]  [ 80/195]  eta: 0:00:08  lr: 0.000418  loss: 0.1523 (0.1520)  time: 0.0704  data: 0.0001  max mem: 2598
[14:10:44.785143] Epoch: [82]  [100/195]  eta: 0:00:07  lr: 0.000418  loss: 0.1514 (0.1520)  time: 0.0710  data: 0.0002  max mem: 2598
[14:10:46.195783] Epoch: [82]  [120/195]  eta: 0:00:05  lr: 0.000417  loss: 0.1536 (0.1523)  time: 0.0705  data: 0.0001  max mem: 2598
[14:10:47.620493] Epoch: [82]  [140/195]  eta: 0:00:04  lr: 0.000417  loss: 0.1510 (0.1523)  time: 0.0712  data: 0.0002  max mem: 2598
[14:10:49.033194] Epoch: [82]  [160/195]  eta: 0:00:02  lr: 0.000417  loss: 0.1547 (0.1525)  time: 0.0706  data: 0.0001  max mem: 2598
[14:10:50.434760] Epoch: [82]  [180/195]  eta: 0:00:01  lr: 0.000416  loss: 0.1516 (0.1525)  time: 0.0700  data: 0.0002  max mem: 2598
[14:10:51.404547] Epoch: [82]  [194/195]  eta: 0:00:00  lr: 0.000416  loss: 0.1523 (0.1524)  time: 0.0693  data: 0.0001  max mem: 2598
[14:10:51.448797] Epoch: [82] Total time: 0:00:14 (0.0727 s / it)
[14:10:51.448864] Averaged stats: lr: 0.000416  loss: 0.1523 (0.1524)
[14:10:51.451350] log_dir: ./output_dir/mae0.3
[14:10:51.847069] Epoch: [83]  [  0/195]  eta: 0:01:17  lr: 0.000416  loss: 0.1545 (0.1545)  time: 0.3949  data: 0.3037  max mem: 2598
[14:10:53.255593] Epoch: [83]  [ 20/195]  eta: 0:00:15  lr: 0.000416  loss: 0.1509 (0.1507)  time: 0.0704  data: 0.0002  max mem: 2598
[14:10:54.655046] Epoch: [83]  [ 40/195]  eta: 0:00:12  lr: 0.000415  loss: 0.1485 (0.1505)  time: 0.0699  data: 0.0001  max mem: 2598
[14:10:56.083607] Epoch: [83]  [ 60/195]  eta: 0:00:10  lr: 0.000415  loss: 0.1507 (0.1512)  time: 0.0714  data: 0.0002  max mem: 2598
[14:10:57.495564] Epoch: [83]  [ 80/195]  eta: 0:00:08  lr: 0.000415  loss: 0.1506 (0.1515)  time: 0.0706  data: 0.0001  max mem: 2598
[14:10:58.887197] Epoch: [83]  [100/195]  eta: 0:00:06  lr: 0.000414  loss: 0.1486 (0.1513)  time: 0.0695  data: 0.0001  max mem: 2598
[14:11:00.259614] Epoch: [83]  [120/195]  eta: 0:00:05  lr: 0.000414  loss: 0.1500 (0.1510)  time: 0.0686  data: 0.0001  max mem: 2598
[14:11:01.636148] Epoch: [83]  [140/195]  eta: 0:00:03  lr: 0.000413  loss: 0.1527 (0.1513)  time: 0.0688  data: 0.0001  max mem: 2598
[14:11:03.012617] Epoch: [83]  [160/195]  eta: 0:00:02  lr: 0.000413  loss: 0.1505 (0.1513)  time: 0.0688  data: 0.0001  max mem: 2598
[14:11:04.384497] Epoch: [83]  [180/195]  eta: 0:00:01  lr: 0.000413  loss: 0.1518 (0.1514)  time: 0.0686  data: 0.0001  max mem: 2598
[14:11:05.336926] Epoch: [83]  [194/195]  eta: 0:00:00  lr: 0.000412  loss: 0.1518 (0.1513)  time: 0.0680  data: 0.0001  max mem: 2598
[14:11:05.399004] Epoch: [83] Total time: 0:00:13 (0.0715 s / it)
[14:11:05.399073] Averaged stats: lr: 0.000412  loss: 0.1518 (0.1513)
[14:11:05.401369] log_dir: ./output_dir/mae0.3
[14:11:05.821821] Epoch: [84]  [  0/195]  eta: 0:01:21  lr: 0.000412  loss: 0.1552 (0.1552)  time: 0.4189  data: 0.2982  max mem: 2598
[14:11:07.272600] Epoch: [84]  [ 20/195]  eta: 0:00:15  lr: 0.000412  loss: 0.1479 (0.1503)  time: 0.0725  data: 0.0002  max mem: 2598
[14:11:08.697963] Epoch: [84]  [ 40/195]  eta: 0:00:12  lr: 0.000412  loss: 0.1501 (0.1510)  time: 0.0712  data: 0.0002  max mem: 2598
[14:11:10.111176] Epoch: [84]  [ 60/195]  eta: 0:00:10  lr: 0.000411  loss: 0.1512 (0.1512)  time: 0.0706  data: 0.0001  max mem: 2598
[14:11:11.525273] Epoch: [84]  [ 80/195]  eta: 0:00:08  lr: 0.000411  loss: 0.1508 (0.1512)  time: 0.0707  data: 0.0001  max mem: 2598
[14:11:12.932175] Epoch: [84]  [100/195]  eta: 0:00:07  lr: 0.000410  loss: 0.1503 (0.1513)  time: 0.0703  data: 0.0001  max mem: 2598
[14:11:14.342463] Epoch: [84]  [120/195]  eta: 0:00:05  lr: 0.000410  loss: 0.1517 (0.1515)  time: 0.0705  data: 0.0001  max mem: 2598
[14:11:15.756102] Epoch: [84]  [140/195]  eta: 0:00:04  lr: 0.000410  loss: 0.1502 (0.1515)  time: 0.0706  data: 0.0002  max mem: 2598
[14:11:17.176531] Epoch: [84]  [160/195]  eta: 0:00:02  lr: 0.000409  loss: 0.1478 (0.1515)  time: 0.0710  data: 0.0001  max mem: 2598
[14:11:18.584260] Epoch: [84]  [180/195]  eta: 0:00:01  lr: 0.000409  loss: 0.1493 (0.1514)  time: 0.0703  data: 0.0001  max mem: 2598
[14:11:19.552300] Epoch: [84]  [194/195]  eta: 0:00:00  lr: 0.000409  loss: 0.1542 (0.1515)  time: 0.0693  data: 0.0001  max mem: 2598
[14:11:19.605428] Epoch: [84] Total time: 0:00:14 (0.0728 s / it)
[14:11:19.605501] Averaged stats: lr: 0.000409  loss: 0.1542 (0.1515)
[14:11:19.608074] log_dir: ./output_dir/mae0.3
[14:11:19.983142] Epoch: [85]  [  0/195]  eta: 0:01:12  lr: 0.000409  loss: 0.1578 (0.1578)  time: 0.3741  data: 0.2995  max mem: 2598
[14:11:21.395900] Epoch: [85]  [ 20/195]  eta: 0:00:14  lr: 0.000408  loss: 0.1484 (0.1516)  time: 0.0706  data: 0.0002  max mem: 2598
[14:11:22.780183] Epoch: [85]  [ 40/195]  eta: 0:00:11  lr: 0.000408  loss: 0.1472 (0.1504)  time: 0.0692  data: 0.0001  max mem: 2598
[14:11:24.161581] Epoch: [85]  [ 60/195]  eta: 0:00:10  lr: 0.000407  loss: 0.1504 (0.1502)  time: 0.0690  data: 0.0001  max mem: 2598
[14:11:25.572117] Epoch: [85]  [ 80/195]  eta: 0:00:08  lr: 0.000407  loss: 0.1494 (0.1502)  time: 0.0705  data: 0.0002  max mem: 2598
[14:11:26.986500] Epoch: [85]  [100/195]  eta: 0:00:06  lr: 0.000407  loss: 0.1501 (0.1500)  time: 0.0707  data: 0.0001  max mem: 2598
[14:11:28.394259] Epoch: [85]  [120/195]  eta: 0:00:05  lr: 0.000406  loss: 0.1478 (0.1499)  time: 0.0704  data: 0.0002  max mem: 2598
[14:11:29.802706] Epoch: [85]  [140/195]  eta: 0:00:03  lr: 0.000406  loss: 0.1498 (0.1499)  time: 0.0704  data: 0.0001  max mem: 2598
[14:11:31.205781] Epoch: [85]  [160/195]  eta: 0:00:02  lr: 0.000405  loss: 0.1501 (0.1500)  time: 0.0701  data: 0.0001  max mem: 2598
[14:11:32.605174] Epoch: [85]  [180/195]  eta: 0:00:01  lr: 0.000405  loss: 0.1518 (0.1501)  time: 0.0699  data: 0.0002  max mem: 2598
[14:11:33.588869] Epoch: [85]  [194/195]  eta: 0:00:00  lr: 0.000405  loss: 0.1490 (0.1500)  time: 0.0700  data: 0.0001  max mem: 2598
[14:11:33.634417] Epoch: [85] Total time: 0:00:14 (0.0719 s / it)
[14:11:33.634485] Averaged stats: lr: 0.000405  loss: 0.1490 (0.1500)
[14:11:33.636986] log_dir: ./output_dir/mae0.3
[14:11:34.059219] Epoch: [86]  [  0/195]  eta: 0:01:22  lr: 0.000405  loss: 0.1585 (0.1585)  time: 0.4205  data: 0.2982  max mem: 2598
[14:11:35.501638] Epoch: [86]  [ 20/195]  eta: 0:00:15  lr: 0.000404  loss: 0.1512 (0.1509)  time: 0.0721  data: 0.0002  max mem: 2598
[14:11:36.900550] Epoch: [86]  [ 40/195]  eta: 0:00:12  lr: 0.000404  loss: 0.1456 (0.1495)  time: 0.0699  data: 0.0001  max mem: 2598
[14:11:38.306537] Epoch: [86]  [ 60/195]  eta: 0:00:10  lr: 0.000404  loss: 0.1529 (0.1503)  time: 0.0703  data: 0.0001  max mem: 2598
[14:11:39.709902] Epoch: [86]  [ 80/195]  eta: 0:00:08  lr: 0.000403  loss: 0.1477 (0.1501)  time: 0.0701  data: 0.0001  max mem: 2598
[14:11:41.085488] Epoch: [86]  [100/195]  eta: 0:00:07  lr: 0.000403  loss: 0.1482 (0.1498)  time: 0.0687  data: 0.0001  max mem: 2598
[14:11:42.484827] Epoch: [86]  [120/195]  eta: 0:00:05  lr: 0.000402  loss: 0.1501 (0.1501)  time: 0.0699  data: 0.0001  max mem: 2598
[14:11:43.903391] Epoch: [86]  [140/195]  eta: 0:00:04  lr: 0.000402  loss: 0.1510 (0.1502)  time: 0.0709  data: 0.0001  max mem: 2598
[14:11:45.322033] Epoch: [86]  [160/195]  eta: 0:00:02  lr: 0.000402  loss: 0.1526 (0.1504)  time: 0.0709  data: 0.0001  max mem: 2598
[14:11:46.729388] Epoch: [86]  [180/195]  eta: 0:00:01  lr: 0.000401  loss: 0.1490 (0.1502)  time: 0.0703  data: 0.0002  max mem: 2598
[14:11:47.697967] Epoch: [86]  [194/195]  eta: 0:00:00  lr: 0.000401  loss: 0.1510 (0.1503)  time: 0.0692  data: 0.0001  max mem: 2598
[14:11:47.750906] Epoch: [86] Total time: 0:00:14 (0.0724 s / it)
[14:11:47.750975] Averaged stats: lr: 0.000401  loss: 0.1510 (0.1503)
[14:11:47.753471] log_dir: ./output_dir/mae0.3
[14:11:48.172981] Epoch: [87]  [  0/195]  eta: 0:01:21  lr: 0.000401  loss: 0.1482 (0.1482)  time: 0.4180  data: 0.3048  max mem: 2598
[14:11:49.600060] Epoch: [87]  [ 20/195]  eta: 0:00:15  lr: 0.000400  loss: 0.1459 (0.1474)  time: 0.0713  data: 0.0001  max mem: 2598
[14:11:50.984135] Epoch: [87]  [ 40/195]  eta: 0:00:12  lr: 0.000400  loss: 0.1476 (0.1484)  time: 0.0692  data: 0.0001  max mem: 2598
[14:11:52.383379] Epoch: [87]  [ 60/195]  eta: 0:00:10  lr: 0.000400  loss: 0.1483 (0.1486)  time: 0.0699  data: 0.0001  max mem: 2598
[14:11:53.772484] Epoch: [87]  [ 80/195]  eta: 0:00:08  lr: 0.000399  loss: 0.1503 (0.1492)  time: 0.0694  data: 0.0001  max mem: 2598
[14:11:55.152785] Epoch: [87]  [100/195]  eta: 0:00:06  lr: 0.000399  loss: 0.1487 (0.1493)  time: 0.0690  data: 0.0001  max mem: 2598
[14:11:56.538450] Epoch: [87]  [120/195]  eta: 0:00:05  lr: 0.000398  loss: 0.1496 (0.1494)  time: 0.0693  data: 0.0001  max mem: 2598
[14:11:57.913859] Epoch: [87]  [140/195]  eta: 0:00:03  lr: 0.000398  loss: 0.1489 (0.1494)  time: 0.0687  data: 0.0001  max mem: 2598
[14:11:59.291410] Epoch: [87]  [160/195]  eta: 0:00:02  lr: 0.000398  loss: 0.1493 (0.1496)  time: 0.0688  data: 0.0001  max mem: 2598
[14:12:00.664492] Epoch: [87]  [180/195]  eta: 0:00:01  lr: 0.000397  loss: 0.1501 (0.1496)  time: 0.0686  data: 0.0001  max mem: 2598
[14:12:01.616635] Epoch: [87]  [194/195]  eta: 0:00:00  lr: 0.000397  loss: 0.1477 (0.1495)  time: 0.0681  data: 0.0001  max mem: 2598
[14:12:01.660967] Epoch: [87] Total time: 0:00:13 (0.0713 s / it)
[14:12:01.661039] Averaged stats: lr: 0.000397  loss: 0.1477 (0.1495)
[14:12:01.663328] log_dir: ./output_dir/mae0.3
[14:12:02.097185] Epoch: [88]  [  0/195]  eta: 0:01:24  lr: 0.000397  loss: 0.1595 (0.1595)  time: 0.4323  data: 0.2940  max mem: 2598
[14:12:03.581731] Epoch: [88]  [ 20/195]  eta: 0:00:15  lr: 0.000397  loss: 0.1472 (0.1480)  time: 0.0742  data: 0.0002  max mem: 2598
[14:12:04.990642] Epoch: [88]  [ 40/195]  eta: 0:00:12  lr: 0.000396  loss: 0.1483 (0.1488)  time: 0.0704  data: 0.0001  max mem: 2598
[14:12:06.402008] Epoch: [88]  [ 60/195]  eta: 0:00:10  lr: 0.000396  loss: 0.1469 (0.1489)  time: 0.0705  data: 0.0001  max mem: 2598
[14:12:07.814804] Epoch: [88]  [ 80/195]  eta: 0:00:08  lr: 0.000395  loss: 0.1492 (0.1488)  time: 0.0706  data: 0.0001  max mem: 2598
[14:12:09.221053] Epoch: [88]  [100/195]  eta: 0:00:07  lr: 0.000395  loss: 0.1505 (0.1490)  time: 0.0703  data: 0.0001  max mem: 2598
[14:12:10.629660] Epoch: [88]  [120/195]  eta: 0:00:05  lr: 0.000394  loss: 0.1500 (0.1491)  time: 0.0704  data: 0.0001  max mem: 2598
[14:12:12.034724] Epoch: [88]  [140/195]  eta: 0:00:04  lr: 0.000394  loss: 0.1457 (0.1487)  time: 0.0702  data: 0.0001  max mem: 2598
[14:12:13.448481] Epoch: [88]  [160/195]  eta: 0:00:02  lr: 0.000394  loss: 0.1483 (0.1487)  time: 0.0706  data: 0.0001  max mem: 2598
[14:12:14.856852] Epoch: [88]  [180/195]  eta: 0:00:01  lr: 0.000393  loss: 0.1496 (0.1488)  time: 0.0704  data: 0.0001  max mem: 2598
[14:12:15.825633] Epoch: [88]  [194/195]  eta: 0:00:00  lr: 0.000393  loss: 0.1504 (0.1488)  time: 0.0692  data: 0.0001  max mem: 2598
[14:12:15.870842] Epoch: [88] Total time: 0:00:14 (0.0729 s / it)
[14:12:15.870918] Averaged stats: lr: 0.000393  loss: 0.1504 (0.1488)
[14:12:15.873686] log_dir: ./output_dir/mae0.3
[14:12:16.248365] Epoch: [89]  [  0/195]  eta: 0:01:12  lr: 0.000393  loss: 0.1495 (0.1495)  time: 0.3740  data: 0.3015  max mem: 2598
[14:12:17.633317] Epoch: [89]  [ 20/195]  eta: 0:00:14  lr: 0.000393  loss: 0.1463 (0.1466)  time: 0.0692  data: 0.0001  max mem: 2598
[14:12:19.003408] Epoch: [89]  [ 40/195]  eta: 0:00:11  lr: 0.000392  loss: 0.1483 (0.1474)  time: 0.0685  data: 0.0001  max mem: 2598
[14:12:20.377860] Epoch: [89]  [ 60/195]  eta: 0:00:09  lr: 0.000392  loss: 0.1468 (0.1474)  time: 0.0687  data: 0.0001  max mem: 2598
[14:12:21.762702] Epoch: [89]  [ 80/195]  eta: 0:00:08  lr: 0.000391  loss: 0.1494 (0.1480)  time: 0.0692  data: 0.0001  max mem: 2598
[14:12:23.128700] Epoch: [89]  [100/195]  eta: 0:00:06  lr: 0.000391  loss: 0.1468 (0.1477)  time: 0.0683  data: 0.0001  max mem: 2598
[14:12:24.493203] Epoch: [89]  [120/195]  eta: 0:00:05  lr: 0.000390  loss: 0.1482 (0.1478)  time: 0.0682  data: 0.0001  max mem: 2598
[14:12:25.867243] Epoch: [89]  [140/195]  eta: 0:00:03  lr: 0.000390  loss: 0.1463 (0.1478)  time: 0.0687  data: 0.0001  max mem: 2598
[14:12:27.231764] Epoch: [89]  [160/195]  eta: 0:00:02  lr: 0.000390  loss: 0.1493 (0.1480)  time: 0.0682  data: 0.0001  max mem: 2598
[14:12:28.607836] Epoch: [89]  [180/195]  eta: 0:00:01  lr: 0.000389  loss: 0.1487 (0.1480)  time: 0.0688  data: 0.0001  max mem: 2598
[14:12:29.560548] Epoch: [89]  [194/195]  eta: 0:00:00  lr: 0.000389  loss: 0.1473 (0.1479)  time: 0.0680  data: 0.0001  max mem: 2598
[14:12:29.605467] Epoch: [89] Total time: 0:00:13 (0.0704 s / it)
[14:12:29.605536] Averaged stats: lr: 0.000389  loss: 0.1473 (0.1479)
[14:12:29.607868] log_dir: ./output_dir/mae0.3
[14:12:30.018109] Epoch: [90]  [  0/195]  eta: 0:01:19  lr: 0.000389  loss: 0.1500 (0.1500)  time: 0.4087  data: 0.2908  max mem: 2598
[14:12:31.500645] Epoch: [90]  [ 20/195]  eta: 0:00:15  lr: 0.000388  loss: 0.1449 (0.1463)  time: 0.0741  data: 0.0002  max mem: 2598
[14:12:32.876239] Epoch: [90]  [ 40/195]  eta: 0:00:12  lr: 0.000388  loss: 0.1459 (0.1467)  time: 0.0687  data: 0.0001  max mem: 2598
[14:12:34.249592] Epoch: [90]  [ 60/195]  eta: 0:00:10  lr: 0.000388  loss: 0.1486 (0.1474)  time: 0.0686  data: 0.0001  max mem: 2598
[14:12:35.625144] Epoch: [90]  [ 80/195]  eta: 0:00:08  lr: 0.000387  loss: 0.1475 (0.1477)  time: 0.0687  data: 0.0001  max mem: 2598
[14:12:37.013998] Epoch: [90]  [100/195]  eta: 0:00:06  lr: 0.000387  loss: 0.1476 (0.1479)  time: 0.0694  data: 0.0001  max mem: 2598
[14:12:38.406063] Epoch: [90]  [120/195]  eta: 0:00:05  lr: 0.000386  loss: 0.1463 (0.1478)  time: 0.0696  data: 0.0001  max mem: 2598
[14:12:39.806139] Epoch: [90]  [140/195]  eta: 0:00:03  lr: 0.000386  loss: 0.1474 (0.1479)  time: 0.0700  data: 0.0001  max mem: 2598
[14:12:41.185052] Epoch: [90]  [160/195]  eta: 0:00:02  lr: 0.000386  loss: 0.1494 (0.1482)  time: 0.0689  data: 0.0001  max mem: 2598
[14:12:42.556183] Epoch: [90]  [180/195]  eta: 0:00:01  lr: 0.000385  loss: 0.1503 (0.1484)  time: 0.0685  data: 0.0001  max mem: 2598
[14:12:43.508768] Epoch: [90]  [194/195]  eta: 0:00:00  lr: 0.000385  loss: 0.1492 (0.1484)  time: 0.0681  data: 0.0001  max mem: 2598
[14:12:43.579791] Epoch: [90] Total time: 0:00:13 (0.0717 s / it)
[14:12:43.579859] Averaged stats: lr: 0.000385  loss: 0.1492 (0.1484)
[14:12:43.582162] log_dir: ./output_dir/mae0.3
[14:12:43.954520] Epoch: [91]  [  0/195]  eta: 0:01:12  lr: 0.000385  loss: 0.1509 (0.1509)  time: 0.3709  data: 0.2694  max mem: 2598
[14:12:45.378964] Epoch: [91]  [ 20/195]  eta: 0:00:14  lr: 0.000384  loss: 0.1467 (0.1483)  time: 0.0712  data: 0.0001  max mem: 2598
[14:12:46.789652] Epoch: [91]  [ 40/195]  eta: 0:00:12  lr: 0.000384  loss: 0.1490 (0.1482)  time: 0.0705  data: 0.0001  max mem: 2598
[14:12:48.200935] Epoch: [91]  [ 60/195]  eta: 0:00:10  lr: 0.000384  loss: 0.1456 (0.1479)  time: 0.0705  data: 0.0001  max mem: 2598
[14:12:49.595371] Epoch: [91]  [ 80/195]  eta: 0:00:08  lr: 0.000383  loss: 0.1471 (0.1480)  time: 0.0697  data: 0.0001  max mem: 2598
[14:12:51.003633] Epoch: [91]  [100/195]  eta: 0:00:06  lr: 0.000383  loss: 0.1460 (0.1479)  time: 0.0704  data: 0.0001  max mem: 2598
[14:12:52.400829] Epoch: [91]  [120/195]  eta: 0:00:05  lr: 0.000382  loss: 0.1479 (0.1482)  time: 0.0698  data: 0.0001  max mem: 2598
[14:12:53.796640] Epoch: [91]  [140/195]  eta: 0:00:03  lr: 0.000382  loss: 0.1461 (0.1480)  time: 0.0697  data: 0.0001  max mem: 2598
[14:12:55.198767] Epoch: [91]  [160/195]  eta: 0:00:02  lr: 0.000381  loss: 0.1465 (0.1479)  time: 0.0700  data: 0.0001  max mem: 2598
[14:12:56.588682] Epoch: [91]  [180/195]  eta: 0:00:01  lr: 0.000381  loss: 0.1472 (0.1479)  time: 0.0695  data: 0.0001  max mem: 2598
[14:12:57.551341] Epoch: [91]  [194/195]  eta: 0:00:00  lr: 0.000381  loss: 0.1465 (0.1480)  time: 0.0687  data: 0.0001  max mem: 2598
[14:12:57.601361] Epoch: [91] Total time: 0:00:14 (0.0719 s / it)
[14:12:57.601451] Averaged stats: lr: 0.000381  loss: 0.1465 (0.1480)
[14:12:57.604774] log_dir: ./output_dir/mae0.3
[14:12:58.009736] Epoch: [92]  [  0/195]  eta: 0:01:18  lr: 0.000381  loss: 0.1538 (0.1538)  time: 0.4040  data: 0.3264  max mem: 2598
[14:12:59.469217] Epoch: [92]  [ 20/195]  eta: 0:00:15  lr: 0.000380  loss: 0.1445 (0.1460)  time: 0.0729  data: 0.0002  max mem: 2598
[14:13:00.874077] Epoch: [92]  [ 40/195]  eta: 0:00:12  lr: 0.000380  loss: 0.1479 (0.1467)  time: 0.0702  data: 0.0001  max mem: 2598
[14:13:02.271747] Epoch: [92]  [ 60/195]  eta: 0:00:10  lr: 0.000379  loss: 0.1442 (0.1463)  time: 0.0698  data: 0.0001  max mem: 2598
[14:13:03.664941] Epoch: [92]  [ 80/195]  eta: 0:00:08  lr: 0.000379  loss: 0.1473 (0.1467)  time: 0.0696  data: 0.0001  max mem: 2598
[14:13:05.050906] Epoch: [92]  [100/195]  eta: 0:00:06  lr: 0.000378  loss: 0.1455 (0.1466)  time: 0.0693  data: 0.0001  max mem: 2598
[14:13:06.433509] Epoch: [92]  [120/195]  eta: 0:00:05  lr: 0.000378  loss: 0.1450 (0.1465)  time: 0.0691  data: 0.0001  max mem: 2598
[14:13:07.816644] Epoch: [92]  [140/195]  eta: 0:00:03  lr: 0.000378  loss: 0.1452 (0.1464)  time: 0.0691  data: 0.0001  max mem: 2598
[14:13:09.217716] Epoch: [92]  [160/195]  eta: 0:00:02  lr: 0.000377  loss: 0.1470 (0.1464)  time: 0.0700  data: 0.0001  max mem: 2598
[14:13:10.600110] Epoch: [92]  [180/195]  eta: 0:00:01  lr: 0.000377  loss: 0.1470 (0.1467)  time: 0.0691  data: 0.0002  max mem: 2598
[14:13:11.556849] Epoch: [92]  [194/195]  eta: 0:00:00  lr: 0.000376  loss: 0.1480 (0.1468)  time: 0.0683  data: 0.0001  max mem: 2598
[14:13:11.626875] Epoch: [92] Total time: 0:00:14 (0.0719 s / it)
[14:13:11.626948] Averaged stats: lr: 0.000376  loss: 0.1480 (0.1468)
[14:13:11.629686] log_dir: ./output_dir/mae0.3
[14:13:12.067345] Epoch: [93]  [  0/195]  eta: 0:01:25  lr: 0.000376  loss: 0.1444 (0.1444)  time: 0.4363  data: 0.3163  max mem: 2598
[14:13:13.510526] Epoch: [93]  [ 20/195]  eta: 0:00:15  lr: 0.000376  loss: 0.1494 (0.1479)  time: 0.0721  data: 0.0002  max mem: 2598
[14:13:14.908573] Epoch: [93]  [ 40/195]  eta: 0:00:12  lr: 0.000376  loss: 0.1487 (0.1479)  time: 0.0699  data: 0.0001  max mem: 2598
[14:13:16.275957] Epoch: [93]  [ 60/195]  eta: 0:00:10  lr: 0.000375  loss: 0.1464 (0.1478)  time: 0.0683  data: 0.0001  max mem: 2598
[14:13:17.653129] Epoch: [93]  [ 80/195]  eta: 0:00:08  lr: 0.000375  loss: 0.1463 (0.1481)  time: 0.0688  data: 0.0001  max mem: 2598
[14:13:19.029583] Epoch: [93]  [100/195]  eta: 0:00:06  lr: 0.000374  loss: 0.1455 (0.1478)  time: 0.0688  data: 0.0001  max mem: 2598
[14:13:20.432378] Epoch: [93]  [120/195]  eta: 0:00:05  lr: 0.000374  loss: 0.1440 (0.1474)  time: 0.0701  data: 0.0001  max mem: 2598
[14:13:21.834592] Epoch: [93]  [140/195]  eta: 0:00:03  lr: 0.000373  loss: 0.1465 (0.1473)  time: 0.0701  data: 0.0002  max mem: 2598
[14:13:23.239737] Epoch: [93]  [160/195]  eta: 0:00:02  lr: 0.000373  loss: 0.1483 (0.1474)  time: 0.0702  data: 0.0001  max mem: 2598
[14:13:24.639667] Epoch: [93]  [180/195]  eta: 0:00:01  lr: 0.000372  loss: 0.1457 (0.1473)  time: 0.0700  data: 0.0001  max mem: 2598
[14:13:25.606650] Epoch: [93]  [194/195]  eta: 0:00:00  lr: 0.000372  loss: 0.1466 (0.1472)  time: 0.0691  data: 0.0001  max mem: 2598
[14:13:25.674865] Epoch: [93] Total time: 0:00:14 (0.0720 s / it)
[14:13:25.674938] Averaged stats: lr: 0.000372  loss: 0.1466 (0.1472)
[14:13:25.677423] log_dir: ./output_dir/mae0.3
[14:13:26.094578] Epoch: [94]  [  0/195]  eta: 0:01:21  lr: 0.000372  loss: 0.1481 (0.1481)  time: 0.4158  data: 0.2939  max mem: 2598
[14:13:27.562959] Epoch: [94]  [ 20/195]  eta: 0:00:15  lr: 0.000372  loss: 0.1473 (0.1455)  time: 0.0734  data: 0.0002  max mem: 2598
[14:13:28.983420] Epoch: [94]  [ 40/195]  eta: 0:00:12  lr: 0.000371  loss: 0.1499 (0.1472)  time: 0.0710  data: 0.0001  max mem: 2598
[14:13:30.390074] Epoch: [94]  [ 60/195]  eta: 0:00:10  lr: 0.000371  loss: 0.1472 (0.1473)  time: 0.0703  data: 0.0001  max mem: 2598
[14:13:31.792277] Epoch: [94]  [ 80/195]  eta: 0:00:08  lr: 0.000370  loss: 0.1440 (0.1468)  time: 0.0701  data: 0.0001  max mem: 2598
[14:13:33.201033] Epoch: [94]  [100/195]  eta: 0:00:07  lr: 0.000370  loss: 0.1470 (0.1469)  time: 0.0704  data: 0.0001  max mem: 2598
[14:13:34.616339] Epoch: [94]  [120/195]  eta: 0:00:05  lr: 0.000370  loss: 0.1452 (0.1469)  time: 0.0707  data: 0.0001  max mem: 2598
[14:13:36.026728] Epoch: [94]  [140/195]  eta: 0:00:04  lr: 0.000369  loss: 0.1473 (0.1470)  time: 0.0705  data: 0.0001  max mem: 2598
[14:13:37.439825] Epoch: [94]  [160/195]  eta: 0:00:02  lr: 0.000369  loss: 0.1472 (0.1471)  time: 0.0706  data: 0.0001  max mem: 2598
[14:13:38.847123] Epoch: [94]  [180/195]  eta: 0:00:01  lr: 0.000368  loss: 0.1463 (0.1471)  time: 0.0703  data: 0.0001  max mem: 2598
[14:13:39.816291] Epoch: [94]  [194/195]  eta: 0:00:00  lr: 0.000368  loss: 0.1455 (0.1470)  time: 0.0693  data: 0.0001  max mem: 2598
[14:13:39.864155] Epoch: [94] Total time: 0:00:14 (0.0728 s / it)
[14:13:39.864243] Averaged stats: lr: 0.000368  loss: 0.1455 (0.1470)
[14:13:39.867609] log_dir: ./output_dir/mae0.3
[14:13:40.342324] Epoch: [95]  [  0/195]  eta: 0:01:32  lr: 0.000368  loss: 0.1520 (0.1520)  time: 0.4731  data: 0.3554  max mem: 2598
[14:13:41.794764] Epoch: [95]  [ 20/195]  eta: 0:00:16  lr: 0.000367  loss: 0.1445 (0.1436)  time: 0.0726  data: 0.0002  max mem: 2598
[14:13:43.199939] Epoch: [95]  [ 40/195]  eta: 0:00:12  lr: 0.000367  loss: 0.1454 (0.1453)  time: 0.0702  data: 0.0001  max mem: 2598
[14:13:44.649467] Epoch: [95]  [ 60/195]  eta: 0:00:10  lr: 0.000367  loss: 0.1434 (0.1448)  time: 0.0724  data: 0.0001  max mem: 2598
[14:13:46.072121] Epoch: [95]  [ 80/195]  eta: 0:00:08  lr: 0.000366  loss: 0.1483 (0.1457)  time: 0.0711  data: 0.0002  max mem: 2598
[14:13:47.527472] Epoch: [95]  [100/195]  eta: 0:00:07  lr: 0.000366  loss: 0.1449 (0.1455)  time: 0.0727  data: 0.0002  max mem: 2598
[14:13:48.979794] Epoch: [95]  [120/195]  eta: 0:00:05  lr: 0.000365  loss: 0.1461 (0.1456)  time: 0.0726  data: 0.0002  max mem: 2598
[14:13:50.430240] Epoch: [95]  [140/195]  eta: 0:00:04  lr: 0.000365  loss: 0.1477 (0.1459)  time: 0.0725  data: 0.0002  max mem: 2598
[14:13:51.847818] Epoch: [95]  [160/195]  eta: 0:00:02  lr: 0.000364  loss: 0.1471 (0.1461)  time: 0.0708  data: 0.0001  max mem: 2598
[14:13:53.268707] Epoch: [95]  [180/195]  eta: 0:00:01  lr: 0.000364  loss: 0.1463 (0.1461)  time: 0.0710  data: 0.0001  max mem: 2598
[14:13:54.234848] Epoch: [95]  [194/195]  eta: 0:00:00  lr: 0.000364  loss: 0.1460 (0.1462)  time: 0.0690  data: 0.0001  max mem: 2598
[14:13:54.280959] Epoch: [95] Total time: 0:00:14 (0.0739 s / it)
[14:13:54.281025] Averaged stats: lr: 0.000364  loss: 0.1460 (0.1462)
[14:13:54.283289] log_dir: ./output_dir/mae0.3
[14:13:54.653934] Epoch: [96]  [  0/195]  eta: 0:01:12  lr: 0.000363  loss: 0.1484 (0.1484)  time: 0.3695  data: 0.2876  max mem: 2598
[14:13:56.079942] Epoch: [96]  [ 20/195]  eta: 0:00:14  lr: 0.000363  loss: 0.1424 (0.1451)  time: 0.0713  data: 0.0001  max mem: 2598
[14:13:57.515411] Epoch: [96]  [ 40/195]  eta: 0:00:12  lr: 0.000363  loss: 0.1425 (0.1445)  time: 0.0717  data: 0.0001  max mem: 2598
[14:13:58.923921] Epoch: [96]  [ 60/195]  eta: 0:00:10  lr: 0.000362  loss: 0.1446 (0.1449)  time: 0.0704  data: 0.0001  max mem: 2598
[14:14:00.334655] Epoch: [96]  [ 80/195]  eta: 0:00:08  lr: 0.000362  loss: 0.1428 (0.1449)  time: 0.0705  data: 0.0001  max mem: 2598
[14:14:01.742247] Epoch: [96]  [100/195]  eta: 0:00:07  lr: 0.000361  loss: 0.1460 (0.1448)  time: 0.0703  data: 0.0001  max mem: 2598
[14:14:03.162196] Epoch: [96]  [120/195]  eta: 0:00:05  lr: 0.000361  loss: 0.1440 (0.1450)  time: 0.0710  data: 0.0001  max mem: 2598
[14:14:04.574309] Epoch: [96]  [140/195]  eta: 0:00:04  lr: 0.000360  loss: 0.1441 (0.1450)  time: 0.0706  data: 0.0001  max mem: 2598
[14:14:05.987634] Epoch: [96]  [160/195]  eta: 0:00:02  lr: 0.000360  loss: 0.1447 (0.1451)  time: 0.0706  data: 0.0001  max mem: 2598
[14:14:07.388412] Epoch: [96]  [180/195]  eta: 0:00:01  lr: 0.000359  loss: 0.1437 (0.1449)  time: 0.0700  data: 0.0001  max mem: 2598
[14:14:08.341114] Epoch: [96]  [194/195]  eta: 0:00:00  lr: 0.000359  loss: 0.1420 (0.1449)  time: 0.0681  data: 0.0001  max mem: 2598
[14:14:08.391617] Epoch: [96] Total time: 0:00:14 (0.0724 s / it)
[14:14:08.391682] Averaged stats: lr: 0.000359  loss: 0.1420 (0.1449)
[14:14:08.393982] log_dir: ./output_dir/mae0.3
[14:14:08.776675] Epoch: [97]  [  0/195]  eta: 0:01:14  lr: 0.000359  loss: 0.1432 (0.1432)  time: 0.3808  data: 0.2723  max mem: 2598
[14:14:10.217009] Epoch: [97]  [ 20/195]  eta: 0:00:15  lr: 0.000359  loss: 0.1408 (0.1431)  time: 0.0719  data: 0.0001  max mem: 2598
[14:14:11.619338] Epoch: [97]  [ 40/195]  eta: 0:00:12  lr: 0.000358  loss: 0.1452 (0.1447)  time: 0.0701  data: 0.0001  max mem: 2598
[14:14:13.023955] Epoch: [97]  [ 60/195]  eta: 0:00:10  lr: 0.000358  loss: 0.1453 (0.1452)  time: 0.0702  data: 0.0001  max mem: 2598
[14:14:14.431928] Epoch: [97]  [ 80/195]  eta: 0:00:08  lr: 0.000357  loss: 0.1426 (0.1450)  time: 0.0703  data: 0.0002  max mem: 2598
[14:14:15.843841] Epoch: [97]  [100/195]  eta: 0:00:06  lr: 0.000357  loss: 0.1451 (0.1450)  time: 0.0705  data: 0.0001  max mem: 2598
[14:14:17.228059] Epoch: [97]  [120/195]  eta: 0:00:05  lr: 0.000356  loss: 0.1449 (0.1448)  time: 0.0692  data: 0.0001  max mem: 2598
[14:14:18.631814] Epoch: [97]  [140/195]  eta: 0:00:03  lr: 0.000356  loss: 0.1450 (0.1451)  time: 0.0701  data: 0.0002  max mem: 2598
[14:14:20.039598] Epoch: [97]  [160/195]  eta: 0:00:02  lr: 0.000355  loss: 0.1449 (0.1453)  time: 0.0703  data: 0.0001  max mem: 2598
[14:14:21.440047] Epoch: [97]  [180/195]  eta: 0:00:01  lr: 0.000355  loss: 0.1444 (0.1454)  time: 0.0700  data: 0.0001  max mem: 2598
[14:14:22.401878] Epoch: [97]  [194/195]  eta: 0:00:00  lr: 0.000355  loss: 0.1441 (0.1453)  time: 0.0688  data: 0.0001  max mem: 2598
[14:14:22.435581] Epoch: [97] Total time: 0:00:14 (0.0720 s / it)
[14:14:22.435643] Averaged stats: lr: 0.000355  loss: 0.1441 (0.1453)
[14:14:22.437803] log_dir: ./output_dir/mae0.3
[14:14:22.814576] Epoch: [98]  [  0/195]  eta: 0:01:13  lr: 0.000355  loss: 0.1488 (0.1488)  time: 0.3758  data: 0.2946  max mem: 2598
[14:14:24.250431] Epoch: [98]  [ 20/195]  eta: 0:00:15  lr: 0.000354  loss: 0.1469 (0.1451)  time: 0.0717  data: 0.0001  max mem: 2598
[14:14:25.686024] Epoch: [98]  [ 40/195]  eta: 0:00:12  lr: 0.000354  loss: 0.1429 (0.1439)  time: 0.0717  data: 0.0001  max mem: 2598
[14:14:27.069087] Epoch: [98]  [ 60/195]  eta: 0:00:10  lr: 0.000353  loss: 0.1430 (0.1441)  time: 0.0691  data: 0.0001  max mem: 2598
[14:14:28.433055] Epoch: [98]  [ 80/195]  eta: 0:00:08  lr: 0.000353  loss: 0.1427 (0.1443)  time: 0.0682  data: 0.0001  max mem: 2598
[14:14:29.802820] Epoch: [98]  [100/195]  eta: 0:00:06  lr: 0.000352  loss: 0.1448 (0.1444)  time: 0.0685  data: 0.0001  max mem: 2598
[14:14:31.171104] Epoch: [98]  [120/195]  eta: 0:00:05  lr: 0.000352  loss: 0.1448 (0.1448)  time: 0.0684  data: 0.0001  max mem: 2598
[14:14:32.550269] Epoch: [98]  [140/195]  eta: 0:00:03  lr: 0.000351  loss: 0.1435 (0.1450)  time: 0.0689  data: 0.0001  max mem: 2598
[14:14:33.922619] Epoch: [98]  [160/195]  eta: 0:00:02  lr: 0.000351  loss: 0.1431 (0.1448)  time: 0.0686  data: 0.0001  max mem: 2598
[14:14:35.297515] Epoch: [98]  [180/195]  eta: 0:00:01  lr: 0.000351  loss: 0.1458 (0.1449)  time: 0.0687  data: 0.0001  max mem: 2598
[14:14:36.250136] Epoch: [98]  [194/195]  eta: 0:00:00  lr: 0.000350  loss: 0.1440 (0.1449)  time: 0.0681  data: 0.0001  max mem: 2598
[14:14:36.293849] Epoch: [98] Total time: 0:00:13 (0.0711 s / it)
[14:14:36.294069] Averaged stats: lr: 0.000350  loss: 0.1440 (0.1449)
[14:14:36.296358] log_dir: ./output_dir/mae0.3
[14:14:36.742709] Epoch: [99]  [  0/195]  eta: 0:01:26  lr: 0.000350  loss: 0.1349 (0.1349)  time: 0.4445  data: 0.3202  max mem: 2598
[14:14:38.170733] Epoch: [99]  [ 20/195]  eta: 0:00:15  lr: 0.000350  loss: 0.1453 (0.1442)  time: 0.0714  data: 0.0001  max mem: 2598
[14:14:39.576251] Epoch: [99]  [ 40/195]  eta: 0:00:12  lr: 0.000349  loss: 0.1433 (0.1441)  time: 0.0702  data: 0.0002  max mem: 2598
[14:14:40.986417] Epoch: [99]  [ 60/195]  eta: 0:00:10  lr: 0.000349  loss: 0.1447 (0.1446)  time: 0.0705  data: 0.0001  max mem: 2598
[14:14:42.388332] Epoch: [99]  [ 80/195]  eta: 0:00:08  lr: 0.000348  loss: 0.1459 (0.1448)  time: 0.0701  data: 0.0001  max mem: 2598
[14:14:43.776994] Epoch: [99]  [100/195]  eta: 0:00:07  lr: 0.000348  loss: 0.1434 (0.1448)  time: 0.0694  data: 0.0001  max mem: 2598
[14:14:45.164614] Epoch: [99]  [120/195]  eta: 0:00:05  lr: 0.000347  loss: 0.1443 (0.1448)  time: 0.0694  data: 0.0001  max mem: 2598
[14:14:46.550716] Epoch: [99]  [140/195]  eta: 0:00:03  lr: 0.000347  loss: 0.1439 (0.1448)  time: 0.0693  data: 0.0001  max mem: 2598
[14:14:47.943161] Epoch: [99]  [160/195]  eta: 0:00:02  lr: 0.000346  loss: 0.1471 (0.1449)  time: 0.0696  data: 0.0001  max mem: 2598
[14:14:49.324796] Epoch: [99]  [180/195]  eta: 0:00:01  lr: 0.000346  loss: 0.1435 (0.1448)  time: 0.0691  data: 0.0001  max mem: 2598
[14:14:50.275251] Epoch: [99]  [194/195]  eta: 0:00:00  lr: 0.000346  loss: 0.1442 (0.1448)  time: 0.0679  data: 0.0001  max mem: 2598
[14:14:50.322426] Epoch: [99] Total time: 0:00:14 (0.0719 s / it)
[14:14:50.322490] Averaged stats: lr: 0.000346  loss: 0.1442 (0.1448)
[14:14:50.324742] log_dir: ./output_dir/mae0.3
[14:14:50.707561] Epoch: [100]  [  0/195]  eta: 0:01:14  lr: 0.000346  loss: 0.1441 (0.1441)  time: 0.3819  data: 0.2950  max mem: 2598
[14:14:52.149407] Epoch: [100]  [ 20/195]  eta: 0:00:15  lr: 0.000345  loss: 0.1421 (0.1429)  time: 0.0721  data: 0.0001  max mem: 2598
[14:14:53.562434] Epoch: [100]  [ 40/195]  eta: 0:00:12  lr: 0.000345  loss: 0.1421 (0.1423)  time: 0.0706  data: 0.0001  max mem: 2598
[14:14:54.990871] Epoch: [100]  [ 60/195]  eta: 0:00:10  lr: 0.000344  loss: 0.1420 (0.1427)  time: 0.0714  data: 0.0001  max mem: 2598
[14:14:56.452459] Epoch: [100]  [ 80/195]  eta: 0:00:08  lr: 0.000344  loss: 0.1476 (0.1436)  time: 0.0730  data: 0.0002  max mem: 2598
[14:14:57.878701] Epoch: [100]  [100/195]  eta: 0:00:07  lr: 0.000343  loss: 0.1455 (0.1441)  time: 0.0713  data: 0.0002  max mem: 2598
[14:14:59.293326] Epoch: [100]  [120/195]  eta: 0:00:05  lr: 0.000343  loss: 0.1397 (0.1437)  time: 0.0707  data: 0.0002  max mem: 2598
[14:15:00.690675] Epoch: [100]  [140/195]  eta: 0:00:04  lr: 0.000342  loss: 0.1456 (0.1439)  time: 0.0698  data: 0.0002  max mem: 2598
[14:15:02.061451] Epoch: [100]  [160/195]  eta: 0:00:02  lr: 0.000342  loss: 0.1430 (0.1440)  time: 0.0685  data: 0.0001  max mem: 2598
[14:15:03.426565] Epoch: [100]  [180/195]  eta: 0:00:01  lr: 0.000341  loss: 0.1422 (0.1439)  time: 0.0682  data: 0.0001  max mem: 2598
[14:15:04.374114] Epoch: [100]  [194/195]  eta: 0:00:00  lr: 0.000341  loss: 0.1455 (0.1442)  time: 0.0677  data: 0.0001  max mem: 2598
[14:15:04.442404] Epoch: [100] Total time: 0:00:14 (0.0724 s / it)
[14:15:04.442466] Averaged stats: lr: 0.000341  loss: 0.1455 (0.1442)
[14:15:04.595031] log_dir: ./output_dir/mae0.3
[14:15:04.974974] Epoch: [101]  [  0/195]  eta: 0:01:13  lr: 0.000341  loss: 0.1373 (0.1373)  time: 0.3790  data: 0.3009  max mem: 2598
[14:15:06.350674] Epoch: [101]  [ 20/195]  eta: 0:00:14  lr: 0.000341  loss: 0.1454 (0.1433)  time: 0.0687  data: 0.0001  max mem: 2598
[14:15:07.725556] Epoch: [101]  [ 40/195]  eta: 0:00:11  lr: 0.000340  loss: 0.1436 (0.1437)  time: 0.0687  data: 0.0001  max mem: 2598
[14:15:09.093111] Epoch: [101]  [ 60/195]  eta: 0:00:09  lr: 0.000340  loss: 0.1422 (0.1435)  time: 0.0683  data: 0.0001  max mem: 2598
[14:15:10.469319] Epoch: [101]  [ 80/195]  eta: 0:00:08  lr: 0.000339  loss: 0.1440 (0.1436)  time: 0.0688  data: 0.0001  max mem: 2598
[14:15:11.835820] Epoch: [101]  [100/195]  eta: 0:00:06  lr: 0.000339  loss: 0.1449 (0.1437)  time: 0.0683  data: 0.0001  max mem: 2598
[14:15:13.202679] Epoch: [101]  [120/195]  eta: 0:00:05  lr: 0.000338  loss: 0.1458 (0.1438)  time: 0.0683  data: 0.0001  max mem: 2598
[14:15:14.574724] Epoch: [101]  [140/195]  eta: 0:00:03  lr: 0.000338  loss: 0.1418 (0.1436)  time: 0.0686  data: 0.0002  max mem: 2598
[14:15:15.968092] Epoch: [101]  [160/195]  eta: 0:00:02  lr: 0.000337  loss: 0.1436 (0.1437)  time: 0.0696  data: 0.0002  max mem: 2598
[14:15:17.338118] Epoch: [101]  [180/195]  eta: 0:00:01  lr: 0.000337  loss: 0.1445 (0.1436)  time: 0.0685  data: 0.0001  max mem: 2598
[14:15:18.290146] Epoch: [101]  [194/195]  eta: 0:00:00  lr: 0.000337  loss: 0.1471 (0.1438)  time: 0.0680  data: 0.0001  max mem: 2598
[14:15:18.336603] Epoch: [101] Total time: 0:00:13 (0.0705 s / it)
[14:15:18.336667] Averaged stats: lr: 0.000337  loss: 0.1471 (0.1438)
[14:15:18.338920] log_dir: ./output_dir/mae0.3
[14:15:18.721546] Epoch: [102]  [  0/195]  eta: 0:01:14  lr: 0.000337  loss: 0.1434 (0.1434)  time: 0.3817  data: 0.3065  max mem: 2598
[14:15:20.136179] Epoch: [102]  [ 20/195]  eta: 0:00:14  lr: 0.000336  loss: 0.1414 (0.1414)  time: 0.0707  data: 0.0001  max mem: 2598
[14:15:21.536341] Epoch: [102]  [ 40/195]  eta: 0:00:12  lr: 0.000336  loss: 0.1434 (0.1424)  time: 0.0700  data: 0.0001  max mem: 2598
[14:15:22.950793] Epoch: [102]  [ 60/195]  eta: 0:00:10  lr: 0.000335  loss: 0.1417 (0.1424)  time: 0.0707  data: 0.0001  max mem: 2598
[14:15:24.355843] Epoch: [102]  [ 80/195]  eta: 0:00:08  lr: 0.000335  loss: 0.1434 (0.1429)  time: 0.0702  data: 0.0001  max mem: 2598
[14:15:25.771539] Epoch: [102]  [100/195]  eta: 0:00:06  lr: 0.000334  loss: 0.1427 (0.1430)  time: 0.0707  data: 0.0001  max mem: 2598
[14:15:27.176377] Epoch: [102]  [120/195]  eta: 0:00:05  lr: 0.000334  loss: 0.1444 (0.1435)  time: 0.0702  data: 0.0001  max mem: 2598
[14:15:28.602277] Epoch: [102]  [140/195]  eta: 0:00:04  lr: 0.000333  loss: 0.1445 (0.1436)  time: 0.0713  data: 0.0002  max mem: 2598
[14:15:30.016287] Epoch: [102]  [160/195]  eta: 0:00:02  lr: 0.000333  loss: 0.1460 (0.1438)  time: 0.0707  data: 0.0001  max mem: 2598
[14:15:31.412354] Epoch: [102]  [180/195]  eta: 0:00:01  lr: 0.000332  loss: 0.1441 (0.1440)  time: 0.0698  data: 0.0001  max mem: 2598
[14:15:32.390658] Epoch: [102]  [194/195]  eta: 0:00:00  lr: 0.000332  loss: 0.1472 (0.1442)  time: 0.0695  data: 0.0001  max mem: 2598
[14:15:32.461022] Epoch: [102] Total time: 0:00:14 (0.0724 s / it)
[14:15:32.461097] Averaged stats: lr: 0.000332  loss: 0.1472 (0.1442)
[14:15:32.463629] log_dir: ./output_dir/mae0.3
[14:15:32.831261] Epoch: [103]  [  0/195]  eta: 0:01:11  lr: 0.000332  loss: 0.1456 (0.1456)  time: 0.3669  data: 0.2828  max mem: 2598
[14:15:34.211825] Epoch: [103]  [ 20/195]  eta: 0:00:14  lr: 0.000331  loss: 0.1424 (0.1433)  time: 0.0690  data: 0.0001  max mem: 2598
[14:15:35.591048] Epoch: [103]  [ 40/195]  eta: 0:00:11  lr: 0.000331  loss: 0.1425 (0.1428)  time: 0.0689  data: 0.0001  max mem: 2598
[14:15:36.962524] Epoch: [103]  [ 60/195]  eta: 0:00:09  lr: 0.000330  loss: 0.1439 (0.1431)  time: 0.0685  data: 0.0001  max mem: 2598
[14:15:38.331469] Epoch: [103]  [ 80/195]  eta: 0:00:08  lr: 0.000330  loss: 0.1434 (0.1434)  time: 0.0684  data: 0.0001  max mem: 2598
[14:15:39.702530] Epoch: [103]  [100/195]  eta: 0:00:06  lr: 0.000330  loss: 0.1419 (0.1432)  time: 0.0685  data: 0.0001  max mem: 2598
[14:15:41.074147] Epoch: [103]  [120/195]  eta: 0:00:05  lr: 0.000329  loss: 0.1425 (0.1432)  time: 0.0685  data: 0.0001  max mem: 2598
[14:15:42.445416] Epoch: [103]  [140/195]  eta: 0:00:03  lr: 0.000329  loss: 0.1393 (0.1429)  time: 0.0685  data: 0.0001  max mem: 2598
[14:15:43.816860] Epoch: [103]  [160/195]  eta: 0:00:02  lr: 0.000328  loss: 0.1424 (0.1428)  time: 0.0685  data: 0.0001  max mem: 2598
[14:15:45.188393] Epoch: [103]  [180/195]  eta: 0:00:01  lr: 0.000328  loss: 0.1410 (0.1429)  time: 0.0685  data: 0.0001  max mem: 2598
[14:15:46.139757] Epoch: [103]  [194/195]  eta: 0:00:00  lr: 0.000327  loss: 0.1451 (0.1430)  time: 0.0680  data: 0.0001  max mem: 2598
[14:15:46.185711] Epoch: [103] Total time: 0:00:13 (0.0704 s / it)
[14:15:46.185774] Averaged stats: lr: 0.000327  loss: 0.1451 (0.1430)
[14:15:46.188027] log_dir: ./output_dir/mae0.3
[14:15:46.606606] Epoch: [104]  [  0/195]  eta: 0:01:21  lr: 0.000327  loss: 0.1502 (0.1502)  time: 0.4169  data: 0.2861  max mem: 2598
[14:15:48.062325] Epoch: [104]  [ 20/195]  eta: 0:00:15  lr: 0.000327  loss: 0.1424 (0.1424)  time: 0.0727  data: 0.0002  max mem: 2598
[14:15:49.498061] Epoch: [104]  [ 40/195]  eta: 0:00:12  lr: 0.000326  loss: 0.1422 (0.1430)  time: 0.0717  data: 0.0002  max mem: 2598
[14:15:50.910234] Epoch: [104]  [ 60/195]  eta: 0:00:10  lr: 0.000326  loss: 0.1413 (0.1428)  time: 0.0706  data: 0.0002  max mem: 2598
[14:15:52.313720] Epoch: [104]  [ 80/195]  eta: 0:00:08  lr: 0.000325  loss: 0.1437 (0.1431)  time: 0.0701  data: 0.0002  max mem: 2598
[14:15:53.721312] Epoch: [104]  [100/195]  eta: 0:00:07  lr: 0.000325  loss: 0.1423 (0.1429)  time: 0.0703  data: 0.0002  max mem: 2598
[14:15:55.130956] Epoch: [104]  [120/195]  eta: 0:00:05  lr: 0.000324  loss: 0.1433 (0.1429)  time: 0.0704  data: 0.0002  max mem: 2598
[14:15:56.545483] Epoch: [104]  [140/195]  eta: 0:00:04  lr: 0.000324  loss: 0.1437 (0.1431)  time: 0.0707  data: 0.0002  max mem: 2598
[14:15:57.952985] Epoch: [104]  [160/195]  eta: 0:00:02  lr: 0.000323  loss: 0.1424 (0.1431)  time: 0.0703  data: 0.0001  max mem: 2598
[14:15:59.351299] Epoch: [104]  [180/195]  eta: 0:00:01  lr: 0.000323  loss: 0.1435 (0.1432)  time: 0.0699  data: 0.0001  max mem: 2598
[14:16:00.316827] Epoch: [104]  [194/195]  eta: 0:00:00  lr: 0.000323  loss: 0.1413 (0.1431)  time: 0.0690  data: 0.0001  max mem: 2598
[14:16:00.385384] Epoch: [104] Total time: 0:00:14 (0.0728 s / it)
[14:16:00.385454] Averaged stats: lr: 0.000323  loss: 0.1413 (0.1431)
[14:16:00.388006] log_dir: ./output_dir/mae0.3
[14:16:00.835821] Epoch: [105]  [  0/195]  eta: 0:01:27  lr: 0.000323  loss: 0.1401 (0.1401)  time: 0.4465  data: 0.3230  max mem: 2598
[14:16:02.244408] Epoch: [105]  [ 20/195]  eta: 0:00:15  lr: 0.000322  loss: 0.1408 (0.1414)  time: 0.0704  data: 0.0001  max mem: 2598
[14:16:03.628261] Epoch: [105]  [ 40/195]  eta: 0:00:12  lr: 0.000322  loss: 0.1402 (0.1411)  time: 0.0691  data: 0.0001  max mem: 2598
[14:16:05.044200] Epoch: [105]  [ 60/195]  eta: 0:00:10  lr: 0.000321  loss: 0.1429 (0.1413)  time: 0.0707  data: 0.0001  max mem: 2598
[14:16:06.451725] Epoch: [105]  [ 80/195]  eta: 0:00:08  lr: 0.000321  loss: 0.1421 (0.1417)  time: 0.0703  data: 0.0001  max mem: 2598
[14:16:07.861705] Epoch: [105]  [100/195]  eta: 0:00:07  lr: 0.000320  loss: 0.1426 (0.1416)  time: 0.0705  data: 0.0001  max mem: 2598
[14:16:09.266159] Epoch: [105]  [120/195]  eta: 0:00:05  lr: 0.000320  loss: 0.1407 (0.1416)  time: 0.0702  data: 0.0001  max mem: 2598
[14:16:10.668801] Epoch: [105]  [140/195]  eta: 0:00:04  lr: 0.000319  loss: 0.1389 (0.1415)  time: 0.0701  data: 0.0001  max mem: 2598
[14:16:12.078277] Epoch: [105]  [160/195]  eta: 0:00:02  lr: 0.000319  loss: 0.1452 (0.1419)  time: 0.0704  data: 0.0001  max mem: 2598
[14:16:13.480692] Epoch: [105]  [180/195]  eta: 0:00:01  lr: 0.000318  loss: 0.1411 (0.1420)  time: 0.0701  data: 0.0002  max mem: 2598
[14:16:14.452236] Epoch: [105]  [194/195]  eta: 0:00:00  lr: 0.000318  loss: 0.1418 (0.1421)  time: 0.0694  data: 0.0001  max mem: 2598
[14:16:14.496368] Epoch: [105] Total time: 0:00:14 (0.0724 s / it)
[14:16:14.496439] Averaged stats: lr: 0.000318  loss: 0.1418 (0.1421)
[14:16:14.498962] log_dir: ./output_dir/mae0.3
[14:16:14.937473] Epoch: [106]  [  0/195]  eta: 0:01:25  lr: 0.000318  loss: 0.1403 (0.1403)  time: 0.4365  data: 0.3007  max mem: 2598
[14:16:16.401448] Epoch: [106]  [ 20/195]  eta: 0:00:15  lr: 0.000317  loss: 0.1422 (0.1422)  time: 0.0732  data: 0.0002  max mem: 2598
[14:16:17.805271] Epoch: [106]  [ 40/195]  eta: 0:00:12  lr: 0.000317  loss: 0.1406 (0.1423)  time: 0.0701  data: 0.0001  max mem: 2598
[14:16:19.217399] Epoch: [106]  [ 60/195]  eta: 0:00:10  lr: 0.000316  loss: 0.1411 (0.1420)  time: 0.0706  data: 0.0001  max mem: 2598
[14:16:20.621205] Epoch: [106]  [ 80/195]  eta: 0:00:08  lr: 0.000316  loss: 0.1405 (0.1422)  time: 0.0701  data: 0.0001  max mem: 2598
[14:16:22.023907] Epoch: [106]  [100/195]  eta: 0:00:07  lr: 0.000315  loss: 0.1418 (0.1422)  time: 0.0701  data: 0.0001  max mem: 2598
[14:16:23.434560] Epoch: [106]  [120/195]  eta: 0:00:05  lr: 0.000315  loss: 0.1428 (0.1425)  time: 0.0705  data: 0.0001  max mem: 2598
[14:16:24.834938] Epoch: [106]  [140/195]  eta: 0:00:04  lr: 0.000314  loss: 0.1412 (0.1425)  time: 0.0700  data: 0.0002  max mem: 2598
[14:16:26.240744] Epoch: [106]  [160/195]  eta: 0:00:02  lr: 0.000314  loss: 0.1437 (0.1427)  time: 0.0703  data: 0.0001  max mem: 2598
[14:16:27.639899] Epoch: [106]  [180/195]  eta: 0:00:01  lr: 0.000313  loss: 0.1424 (0.1426)  time: 0.0699  data: 0.0001  max mem: 2598
[14:16:28.608323] Epoch: [106]  [194/195]  eta: 0:00:00  lr: 0.000313  loss: 0.1446 (0.1428)  time: 0.0692  data: 0.0001  max mem: 2598
[14:16:28.662266] Epoch: [106] Total time: 0:00:14 (0.0726 s / it)
[14:16:28.662363] Averaged stats: lr: 0.000313  loss: 0.1446 (0.1428)
[14:16:28.665481] log_dir: ./output_dir/mae0.3
[14:16:29.056690] Epoch: [107]  [  0/195]  eta: 0:01:16  lr: 0.000313  loss: 0.1463 (0.1463)  time: 0.3899  data: 0.2938  max mem: 2598
[14:16:30.491894] Epoch: [107]  [ 20/195]  eta: 0:00:15  lr: 0.000313  loss: 0.1402 (0.1428)  time: 0.0717  data: 0.0002  max mem: 2598
[14:16:31.904773] Epoch: [107]  [ 40/195]  eta: 0:00:12  lr: 0.000312  loss: 0.1406 (0.1419)  time: 0.0706  data: 0.0002  max mem: 2598
[14:16:33.324968] Epoch: [107]  [ 60/195]  eta: 0:00:10  lr: 0.000312  loss: 0.1435 (0.1424)  time: 0.0710  data: 0.0002  max mem: 2598
[14:16:34.733201] Epoch: [107]  [ 80/195]  eta: 0:00:08  lr: 0.000311  loss: 0.1401 (0.1419)  time: 0.0703  data: 0.0001  max mem: 2598
[14:16:36.144053] Epoch: [107]  [100/195]  eta: 0:00:07  lr: 0.000311  loss: 0.1407 (0.1417)  time: 0.0705  data: 0.0002  max mem: 2598
[14:16:37.559238] Epoch: [107]  [120/195]  eta: 0:00:05  lr: 0.000310  loss: 0.1420 (0.1417)  time: 0.0707  data: 0.0002  max mem: 2598
[14:16:38.971774] Epoch: [107]  [140/195]  eta: 0:00:04  lr: 0.000310  loss: 0.1413 (0.1418)  time: 0.0706  data: 0.0002  max mem: 2598
[14:16:40.377803] Epoch: [107]  [160/195]  eta: 0:00:02  lr: 0.000309  loss: 0.1404 (0.1418)  time: 0.0703  data: 0.0001  max mem: 2598
[14:16:41.782937] Epoch: [107]  [180/195]  eta: 0:00:01  lr: 0.000309  loss: 0.1425 (0.1419)  time: 0.0702  data: 0.0001  max mem: 2598
[14:16:42.753494] Epoch: [107]  [194/195]  eta: 0:00:00  lr: 0.000308  loss: 0.1423 (0.1419)  time: 0.0692  data: 0.0001  max mem: 2598
[14:16:42.834865] Epoch: [107] Total time: 0:00:14 (0.0727 s / it)
[14:16:42.834934] Averaged stats: lr: 0.000308  loss: 0.1423 (0.1419)
[14:16:42.837423] log_dir: ./output_dir/mae0.3
[14:16:43.243652] Epoch: [108]  [  0/195]  eta: 0:01:18  lr: 0.000308  loss: 0.1426 (0.1426)  time: 0.4048  data: 0.2830  max mem: 2598
[14:16:44.690781] Epoch: [108]  [ 20/195]  eta: 0:00:15  lr: 0.000308  loss: 0.1405 (0.1425)  time: 0.0723  data: 0.0001  max mem: 2598
[14:16:46.073544] Epoch: [108]  [ 40/195]  eta: 0:00:12  lr: 0.000307  loss: 0.1428 (0.1422)  time: 0.0691  data: 0.0001  max mem: 2598
[14:16:47.448174] Epoch: [108]  [ 60/195]  eta: 0:00:10  lr: 0.000307  loss: 0.1430 (0.1423)  time: 0.0687  data: 0.0001  max mem: 2598
[14:16:48.825830] Epoch: [108]  [ 80/195]  eta: 0:00:08  lr: 0.000306  loss: 0.1426 (0.1420)  time: 0.0689  data: 0.0001  max mem: 2598
[14:16:50.198994] Epoch: [108]  [100/195]  eta: 0:00:06  lr: 0.000306  loss: 0.1432 (0.1421)  time: 0.0686  data: 0.0001  max mem: 2598
[14:16:51.578117] Epoch: [108]  [120/195]  eta: 0:00:05  lr: 0.000305  loss: 0.1421 (0.1421)  time: 0.0689  data: 0.0001  max mem: 2598
[14:16:52.958892] Epoch: [108]  [140/195]  eta: 0:00:03  lr: 0.000305  loss: 0.1376 (0.1419)  time: 0.0690  data: 0.0001  max mem: 2598
[14:16:54.353855] Epoch: [108]  [160/195]  eta: 0:00:02  lr: 0.000304  loss: 0.1444 (0.1421)  time: 0.0697  data: 0.0001  max mem: 2598
[14:16:55.754110] Epoch: [108]  [180/195]  eta: 0:00:01  lr: 0.000304  loss: 0.1411 (0.1419)  time: 0.0700  data: 0.0002  max mem: 2598
[14:16:56.720849] Epoch: [108]  [194/195]  eta: 0:00:00  lr: 0.000304  loss: 0.1409 (0.1418)  time: 0.0691  data: 0.0001  max mem: 2598
[14:16:56.791354] Epoch: [108] Total time: 0:00:13 (0.0716 s / it)
[14:16:56.791536] Averaged stats: lr: 0.000304  loss: 0.1409 (0.1418)
[14:16:56.794030] log_dir: ./output_dir/mae0.3
[14:16:57.198956] Epoch: [109]  [  0/195]  eta: 0:01:18  lr: 0.000304  loss: 0.1621 (0.1621)  time: 0.4037  data: 0.2899  max mem: 2598
[14:16:58.601147] Epoch: [109]  [ 20/195]  eta: 0:00:15  lr: 0.000303  loss: 0.1413 (0.1415)  time: 0.0701  data: 0.0001  max mem: 2598
[14:16:59.968290] Epoch: [109]  [ 40/195]  eta: 0:00:11  lr: 0.000303  loss: 0.1395 (0.1402)  time: 0.0683  data: 0.0001  max mem: 2598
[14:17:01.344571] Epoch: [109]  [ 60/195]  eta: 0:00:10  lr: 0.000302  loss: 0.1439 (0.1413)  time: 0.0688  data: 0.0001  max mem: 2598
[14:17:02.710103] Epoch: [109]  [ 80/195]  eta: 0:00:08  lr: 0.000302  loss: 0.1432 (0.1415)  time: 0.0682  data: 0.0001  max mem: 2598
[14:17:04.080901] Epoch: [109]  [100/195]  eta: 0:00:06  lr: 0.000301  loss: 0.1421 (0.1416)  time: 0.0685  data: 0.0001  max mem: 2598
[14:17:05.453495] Epoch: [109]  [120/195]  eta: 0:00:05  lr: 0.000301  loss: 0.1386 (0.1413)  time: 0.0686  data: 0.0001  max mem: 2598
[14:17:06.820467] Epoch: [109]  [140/195]  eta: 0:00:03  lr: 0.000300  loss: 0.1418 (0.1414)  time: 0.0683  data: 0.0001  max mem: 2598
[14:17:08.189154] Epoch: [109]  [160/195]  eta: 0:00:02  lr: 0.000300  loss: 0.1402 (0.1415)  time: 0.0684  data: 0.0001  max mem: 2598
[14:17:09.556493] Epoch: [109]  [180/195]  eta: 0:00:01  lr: 0.000299  loss: 0.1407 (0.1415)  time: 0.0683  data: 0.0001  max mem: 2598
[14:17:10.507943] Epoch: [109]  [194/195]  eta: 0:00:00  lr: 0.000299  loss: 0.1409 (0.1416)  time: 0.0680  data: 0.0001  max mem: 2598
[14:17:10.554080] Epoch: [109] Total time: 0:00:13 (0.0706 s / it)
[14:17:10.554146] Averaged stats: lr: 0.000299  loss: 0.1409 (0.1416)
[14:17:10.556395] log_dir: ./output_dir/mae0.3
[14:17:10.926656] Epoch: [110]  [  0/195]  eta: 0:01:11  lr: 0.000299  loss: 0.1439 (0.1439)  time: 0.3692  data: 0.2857  max mem: 2598
[14:17:12.373140] Epoch: [110]  [ 20/195]  eta: 0:00:15  lr: 0.000298  loss: 0.1393 (0.1398)  time: 0.0723  data: 0.0002  max mem: 2598
[14:17:13.786867] Epoch: [110]  [ 40/195]  eta: 0:00:12  lr: 0.000298  loss: 0.1403 (0.1400)  time: 0.0707  data: 0.0001  max mem: 2598
[14:17:15.199616] Epoch: [110]  [ 60/195]  eta: 0:00:10  lr: 0.000297  loss: 0.1424 (0.1406)  time: 0.0706  data: 0.0001  max mem: 2598
[14:17:16.608677] Epoch: [110]  [ 80/195]  eta: 0:00:08  lr: 0.000297  loss: 0.1398 (0.1404)  time: 0.0704  data: 0.0001  max mem: 2598
[14:17:18.037423] Epoch: [110]  [100/195]  eta: 0:00:07  lr: 0.000296  loss: 0.1400 (0.1403)  time: 0.0713  data: 0.0002  max mem: 2598
[14:17:19.455891] Epoch: [110]  [120/195]  eta: 0:00:05  lr: 0.000296  loss: 0.1422 (0.1407)  time: 0.0709  data: 0.0002  max mem: 2598
[14:17:20.883130] Epoch: [110]  [140/195]  eta: 0:00:04  lr: 0.000295  loss: 0.1428 (0.1409)  time: 0.0713  data: 0.0001  max mem: 2598
[14:17:22.303891] Epoch: [110]  [160/195]  eta: 0:00:02  lr: 0.000295  loss: 0.1433 (0.1413)  time: 0.0710  data: 0.0001  max mem: 2598
[14:17:23.721869] Epoch: [110]  [180/195]  eta: 0:00:01  lr: 0.000294  loss: 0.1418 (0.1414)  time: 0.0709  data: 0.0002  max mem: 2598
[14:17:24.688487] Epoch: [110]  [194/195]  eta: 0:00:00  lr: 0.000294  loss: 0.1397 (0.1412)  time: 0.0690  data: 0.0001  max mem: 2598
[14:17:24.732303] Epoch: [110] Total time: 0:00:14 (0.0727 s / it)
[14:17:24.732374] Averaged stats: lr: 0.000294  loss: 0.1397 (0.1412)
[14:17:24.734894] log_dir: ./output_dir/mae0.3
[14:17:25.126804] Epoch: [111]  [  0/195]  eta: 0:01:16  lr: 0.000294  loss: 0.1353 (0.1353)  time: 0.3910  data: 0.3132  max mem: 2598
[14:17:26.518201] Epoch: [111]  [ 20/195]  eta: 0:00:14  lr: 0.000293  loss: 0.1406 (0.1403)  time: 0.0695  data: 0.0001  max mem: 2598
[14:17:27.903422] Epoch: [111]  [ 40/195]  eta: 0:00:11  lr: 0.000293  loss: 0.1427 (0.1409)  time: 0.0692  data: 0.0001  max mem: 2598
[14:17:29.287341] Epoch: [111]  [ 60/195]  eta: 0:00:10  lr: 0.000292  loss: 0.1393 (0.1403)  time: 0.0692  data: 0.0001  max mem: 2598
[14:17:30.677728] Epoch: [111]  [ 80/195]  eta: 0:00:08  lr: 0.000292  loss: 0.1417 (0.1408)  time: 0.0695  data: 0.0001  max mem: 2598
[14:17:32.062603] Epoch: [111]  [100/195]  eta: 0:00:06  lr: 0.000291  loss: 0.1397 (0.1406)  time: 0.0692  data: 0.0001  max mem: 2598
[14:17:33.444274] Epoch: [111]  [120/195]  eta: 0:00:05  lr: 0.000291  loss: 0.1405 (0.1406)  time: 0.0691  data: 0.0001  max mem: 2598
[14:17:34.820930] Epoch: [111]  [140/195]  eta: 0:00:03  lr: 0.000290  loss: 0.1401 (0.1406)  time: 0.0688  data: 0.0001  max mem: 2598
[14:17:36.200711] Epoch: [111]  [160/195]  eta: 0:00:02  lr: 0.000290  loss: 0.1397 (0.1405)  time: 0.0690  data: 0.0001  max mem: 2598
[14:17:37.575947] Epoch: [111]  [180/195]  eta: 0:00:01  lr: 0.000289  loss: 0.1418 (0.1407)  time: 0.0687  data: 0.0001  max mem: 2598
[14:17:38.530869] Epoch: [111]  [194/195]  eta: 0:00:00  lr: 0.000289  loss: 0.1414 (0.1408)  time: 0.0682  data: 0.0001  max mem: 2598
[14:17:38.578944] Epoch: [111] Total time: 0:00:13 (0.0710 s / it)
[14:17:38.579023] Averaged stats: lr: 0.000289  loss: 0.1414 (0.1408)
[14:17:38.581755] log_dir: ./output_dir/mae0.3
[14:17:39.001521] Epoch: [112]  [  0/195]  eta: 0:01:21  lr: 0.000289  loss: 0.1456 (0.1456)  time: 0.4182  data: 0.2949  max mem: 2598
[14:17:40.424172] Epoch: [112]  [ 20/195]  eta: 0:00:15  lr: 0.000289  loss: 0.1425 (0.1419)  time: 0.0711  data: 0.0001  max mem: 2598
[14:17:41.825607] Epoch: [112]  [ 40/195]  eta: 0:00:12  lr: 0.000288  loss: 0.1372 (0.1411)  time: 0.0700  data: 0.0001  max mem: 2598
[14:17:43.205016] Epoch: [112]  [ 60/195]  eta: 0:00:10  lr: 0.000288  loss: 0.1405 (0.1413)  time: 0.0689  data: 0.0001  max mem: 2598
[14:17:44.589461] Epoch: [112]  [ 80/195]  eta: 0:00:08  lr: 0.000287  loss: 0.1383 (0.1409)  time: 0.0692  data: 0.0001  max mem: 2598
[14:17:45.975812] Epoch: [112]  [100/195]  eta: 0:00:06  lr: 0.000287  loss: 0.1403 (0.1410)  time: 0.0693  data: 0.0001  max mem: 2598
[14:17:47.355394] Epoch: [112]  [120/195]  eta: 0:00:05  lr: 0.000286  loss: 0.1419 (0.1412)  time: 0.0689  data: 0.0001  max mem: 2598
[14:17:48.750572] Epoch: [112]  [140/195]  eta: 0:00:03  lr: 0.000286  loss: 0.1386 (0.1410)  time: 0.0697  data: 0.0001  max mem: 2598
[14:17:50.140680] Epoch: [112]  [160/195]  eta: 0:00:02  lr: 0.000285  loss: 0.1388 (0.1410)  time: 0.0695  data: 0.0001  max mem: 2598
[14:17:51.521437] Epoch: [112]  [180/195]  eta: 0:00:01  lr: 0.000285  loss: 0.1419 (0.1411)  time: 0.0690  data: 0.0001  max mem: 2598
[14:17:52.472563] Epoch: [112]  [194/195]  eta: 0:00:00  lr: 0.000284  loss: 0.1407 (0.1410)  time: 0.0680  data: 0.0001  max mem: 2598
[14:17:52.507610] Epoch: [112] Total time: 0:00:13 (0.0714 s / it)
[14:17:52.507680] Averaged stats: lr: 0.000284  loss: 0.1407 (0.1410)
[14:17:52.509951] log_dir: ./output_dir/mae0.3
[14:17:52.897246] Epoch: [113]  [  0/195]  eta: 0:01:15  lr: 0.000284  loss: 0.1419 (0.1419)  time: 0.3863  data: 0.2982  max mem: 2598
[14:17:54.314497] Epoch: [113]  [ 20/195]  eta: 0:00:15  lr: 0.000284  loss: 0.1342 (0.1391)  time: 0.0708  data: 0.0001  max mem: 2598
[14:17:55.702910] Epoch: [113]  [ 40/195]  eta: 0:00:12  lr: 0.000283  loss: 0.1414 (0.1401)  time: 0.0694  data: 0.0001  max mem: 2598
[14:17:57.110991] Epoch: [113]  [ 60/195]  eta: 0:00:10  lr: 0.000283  loss: 0.1414 (0.1408)  time: 0.0704  data: 0.0001  max mem: 2598
[14:17:58.520741] Epoch: [113]  [ 80/195]  eta: 0:00:08  lr: 0.000282  loss: 0.1385 (0.1407)  time: 0.0704  data: 0.0001  max mem: 2598
[14:17:59.924504] Epoch: [113]  [100/195]  eta: 0:00:06  lr: 0.000282  loss: 0.1399 (0.1405)  time: 0.0702  data: 0.0001  max mem: 2598
[14:18:01.331766] Epoch: [113]  [120/195]  eta: 0:00:05  lr: 0.000281  loss: 0.1409 (0.1407)  time: 0.0703  data: 0.0002  max mem: 2598
[14:18:02.740225] Epoch: [113]  [140/195]  eta: 0:00:03  lr: 0.000281  loss: 0.1378 (0.1404)  time: 0.0704  data: 0.0001  max mem: 2598
[14:18:04.126341] Epoch: [113]  [160/195]  eta: 0:00:02  lr: 0.000280  loss: 0.1418 (0.1407)  time: 0.0693  data: 0.0001  max mem: 2598
[14:18:05.499367] Epoch: [113]  [180/195]  eta: 0:00:01  lr: 0.000280  loss: 0.1383 (0.1406)  time: 0.0686  data: 0.0001  max mem: 2598
[14:18:06.449207] Epoch: [113]  [194/195]  eta: 0:00:00  lr: 0.000279  loss: 0.1395 (0.1407)  time: 0.0678  data: 0.0001  max mem: 2598
[14:18:06.499060] Epoch: [113] Total time: 0:00:13 (0.0717 s / it)
[14:18:06.499125] Averaged stats: lr: 0.000279  loss: 0.1395 (0.1407)
[14:18:06.501363] log_dir: ./output_dir/mae0.3
[14:18:06.881593] Epoch: [114]  [  0/195]  eta: 0:01:13  lr: 0.000279  loss: 0.1442 (0.1442)  time: 0.3790  data: 0.3006  max mem: 2598
[14:18:08.311415] Epoch: [114]  [ 20/195]  eta: 0:00:15  lr: 0.000279  loss: 0.1404 (0.1398)  time: 0.0715  data: 0.0002  max mem: 2598
[14:18:09.713409] Epoch: [114]  [ 40/195]  eta: 0:00:12  lr: 0.000278  loss: 0.1370 (0.1389)  time: 0.0701  data: 0.0001  max mem: 2598
[14:18:11.126074] Epoch: [114]  [ 60/195]  eta: 0:00:10  lr: 0.000278  loss: 0.1397 (0.1397)  time: 0.0706  data: 0.0002  max mem: 2598
[14:18:12.550725] Epoch: [114]  [ 80/195]  eta: 0:00:08  lr: 0.000277  loss: 0.1394 (0.1396)  time: 0.0712  data: 0.0002  max mem: 2598
[14:18:13.953678] Epoch: [114]  [100/195]  eta: 0:00:07  lr: 0.000277  loss: 0.1388 (0.1396)  time: 0.0701  data: 0.0001  max mem: 2598
[14:18:15.354191] Epoch: [114]  [120/195]  eta: 0:00:05  lr: 0.000276  loss: 0.1394 (0.1396)  time: 0.0700  data: 0.0001  max mem: 2598
[14:18:16.751596] Epoch: [114]  [140/195]  eta: 0:00:03  lr: 0.000276  loss: 0.1425 (0.1401)  time: 0.0698  data: 0.0001  max mem: 2598
[14:18:18.125953] Epoch: [114]  [160/195]  eta: 0:00:02  lr: 0.000275  loss: 0.1413 (0.1403)  time: 0.0687  data: 0.0001  max mem: 2598
[14:18:19.492413] Epoch: [114]  [180/195]  eta: 0:00:01  lr: 0.000275  loss: 0.1385 (0.1401)  time: 0.0683  data: 0.0001  max mem: 2598
[14:18:20.443741] Epoch: [114]  [194/195]  eta: 0:00:00  lr: 0.000275  loss: 0.1378 (0.1401)  time: 0.0680  data: 0.0001  max mem: 2598
[14:18:20.489614] Epoch: [114] Total time: 0:00:13 (0.0717 s / it)
[14:18:20.489691] Averaged stats: lr: 0.000275  loss: 0.1378 (0.1401)
[14:18:20.491893] log_dir: ./output_dir/mae0.3
[14:18:20.901352] Epoch: [115]  [  0/195]  eta: 0:01:19  lr: 0.000275  loss: 0.1390 (0.1390)  time: 0.4078  data: 0.2889  max mem: 2598
[14:18:22.333235] Epoch: [115]  [ 20/195]  eta: 0:00:15  lr: 0.000274  loss: 0.1389 (0.1406)  time: 0.0716  data: 0.0002  max mem: 2598
[14:18:23.714538] Epoch: [115]  [ 40/195]  eta: 0:00:12  lr: 0.000274  loss: 0.1371 (0.1394)  time: 0.0690  data: 0.0001  max mem: 2598
[14:18:25.090870] Epoch: [115]  [ 60/195]  eta: 0:00:10  lr: 0.000273  loss: 0.1385 (0.1392)  time: 0.0688  data: 0.0001  max mem: 2598
[14:18:26.459753] Epoch: [115]  [ 80/195]  eta: 0:00:08  lr: 0.000272  loss: 0.1386 (0.1394)  time: 0.0684  data: 0.0001  max mem: 2598
[14:18:27.840941] Epoch: [115]  [100/195]  eta: 0:00:06  lr: 0.000272  loss: 0.1399 (0.1395)  time: 0.0690  data: 0.0001  max mem: 2598
[14:18:29.217028] Epoch: [115]  [120/195]  eta: 0:00:05  lr: 0.000271  loss: 0.1423 (0.1400)  time: 0.0688  data: 0.0001  max mem: 2598
[14:18:30.590657] Epoch: [115]  [140/195]  eta: 0:00:03  lr: 0.000271  loss: 0.1395 (0.1401)  time: 0.0686  data: 0.0001  max mem: 2598
[14:18:31.967147] Epoch: [115]  [160/195]  eta: 0:00:02  lr: 0.000270  loss: 0.1407 (0.1402)  time: 0.0688  data: 0.0001  max mem: 2598
[14:18:33.333689] Epoch: [115]  [180/195]  eta: 0:00:01  lr: 0.000270  loss: 0.1382 (0.1402)  time: 0.0683  data: 0.0001  max mem: 2598
[14:18:34.285432] Epoch: [115]  [194/195]  eta: 0:00:00  lr: 0.000270  loss: 0.1380 (0.1402)  time: 0.0679  data: 0.0001  max mem: 2598
[14:18:34.345753] Epoch: [115] Total time: 0:00:13 (0.0710 s / it)
[14:18:34.345815] Averaged stats: lr: 0.000270  loss: 0.1380 (0.1402)
[14:18:34.348083] log_dir: ./output_dir/mae0.3
[14:18:34.712306] Epoch: [116]  [  0/195]  eta: 0:01:10  lr: 0.000270  loss: 0.1424 (0.1424)  time: 0.3632  data: 0.2787  max mem: 2598
[14:18:36.124633] Epoch: [116]  [ 20/195]  eta: 0:00:14  lr: 0.000269  loss: 0.1389 (0.1389)  time: 0.0706  data: 0.0001  max mem: 2598
[14:18:37.496993] Epoch: [116]  [ 40/195]  eta: 0:00:11  lr: 0.000269  loss: 0.1374 (0.1391)  time: 0.0686  data: 0.0001  max mem: 2598
[14:18:38.866715] Epoch: [116]  [ 60/195]  eta: 0:00:09  lr: 0.000268  loss: 0.1403 (0.1392)  time: 0.0684  data: 0.0001  max mem: 2598
[14:18:40.263485] Epoch: [116]  [ 80/195]  eta: 0:00:08  lr: 0.000268  loss: 0.1393 (0.1395)  time: 0.0698  data: 0.0001  max mem: 2598
[14:18:41.670905] Epoch: [116]  [100/195]  eta: 0:00:06  lr: 0.000267  loss: 0.1419 (0.1397)  time: 0.0703  data: 0.0001  max mem: 2598
[14:18:43.077771] Epoch: [116]  [120/195]  eta: 0:00:05  lr: 0.000267  loss: 0.1409 (0.1398)  time: 0.0703  data: 0.0001  max mem: 2598
[14:18:44.470654] Epoch: [116]  [140/195]  eta: 0:00:03  lr: 0.000266  loss: 0.1376 (0.1396)  time: 0.0696  data: 0.0001  max mem: 2598
[14:18:45.853572] Epoch: [116]  [160/195]  eta: 0:00:02  lr: 0.000266  loss: 0.1416 (0.1397)  time: 0.0691  data: 0.0001  max mem: 2598
[14:18:47.230626] Epoch: [116]  [180/195]  eta: 0:00:01  lr: 0.000265  loss: 0.1391 (0.1398)  time: 0.0688  data: 0.0001  max mem: 2598
[14:18:48.181170] Epoch: [116]  [194/195]  eta: 0:00:00  lr: 0.000265  loss: 0.1402 (0.1396)  time: 0.0680  data: 0.0001  max mem: 2598
[14:18:48.251809] Epoch: [116] Total time: 0:00:13 (0.0713 s / it)
[14:18:48.251883] Averaged stats: lr: 0.000265  loss: 0.1402 (0.1396)
[14:18:48.254194] log_dir: ./output_dir/mae0.3
[14:18:48.712933] Epoch: [117]  [  0/195]  eta: 0:01:29  lr: 0.000265  loss: 0.1425 (0.1425)  time: 0.4571  data: 0.3378  max mem: 2598
[14:18:50.159630] Epoch: [117]  [ 20/195]  eta: 0:00:15  lr: 0.000264  loss: 0.1390 (0.1394)  time: 0.0723  data: 0.0002  max mem: 2598
[14:18:51.563540] Epoch: [117]  [ 40/195]  eta: 0:00:12  lr: 0.000264  loss: 0.1393 (0.1390)  time: 0.0702  data: 0.0001  max mem: 2598
[14:18:52.973114] Epoch: [117]  [ 60/195]  eta: 0:00:10  lr: 0.000263  loss: 0.1373 (0.1388)  time: 0.0704  data: 0.0001  max mem: 2598
[14:18:54.386894] Epoch: [117]  [ 80/195]  eta: 0:00:08  lr: 0.000263  loss: 0.1382 (0.1388)  time: 0.0706  data: 0.0001  max mem: 2598
[14:18:55.802618] Epoch: [117]  [100/195]  eta: 0:00:07  lr: 0.000262  loss: 0.1422 (0.1393)  time: 0.0707  data: 0.0001  max mem: 2598
[14:18:57.201331] Epoch: [117]  [120/195]  eta: 0:00:05  lr: 0.000262  loss: 0.1388 (0.1397)  time: 0.0699  data: 0.0001  max mem: 2598
[14:18:58.625280] Epoch: [117]  [140/195]  eta: 0:00:04  lr: 0.000261  loss: 0.1367 (0.1395)  time: 0.0712  data: 0.0001  max mem: 2598
[14:19:00.036773] Epoch: [117]  [160/195]  eta: 0:00:02  lr: 0.000261  loss: 0.1417 (0.1397)  time: 0.0705  data: 0.0001  max mem: 2598
[14:19:01.434368] Epoch: [117]  [180/195]  eta: 0:00:01  lr: 0.000260  loss: 0.1402 (0.1398)  time: 0.0698  data: 0.0001  max mem: 2598
[14:19:02.400301] Epoch: [117]  [194/195]  eta: 0:00:00  lr: 0.000260  loss: 0.1389 (0.1398)  time: 0.0690  data: 0.0001  max mem: 2598
[14:19:02.451295] Epoch: [117] Total time: 0:00:14 (0.0728 s / it)
[14:19:02.451380] Averaged stats: lr: 0.000260  loss: 0.1389 (0.1398)
[14:19:02.454711] log_dir: ./output_dir/mae0.3
[14:19:02.864683] Epoch: [118]  [  0/195]  eta: 0:01:19  lr: 0.000260  loss: 0.1344 (0.1344)  time: 0.4081  data: 0.2952  max mem: 2598
[14:19:04.284937] Epoch: [118]  [ 20/195]  eta: 0:00:15  lr: 0.000259  loss: 0.1361 (0.1368)  time: 0.0710  data: 0.0001  max mem: 2598
[14:19:05.685149] Epoch: [118]  [ 40/195]  eta: 0:00:12  lr: 0.000259  loss: 0.1366 (0.1370)  time: 0.0700  data: 0.0001  max mem: 2598
[14:19:07.082932] Epoch: [118]  [ 60/195]  eta: 0:00:10  lr: 0.000258  loss: 0.1414 (0.1383)  time: 0.0698  data: 0.0001  max mem: 2598
[14:19:08.484940] Epoch: [118]  [ 80/195]  eta: 0:00:08  lr: 0.000258  loss: 0.1388 (0.1389)  time: 0.0701  data: 0.0001  max mem: 2598
[14:19:09.882387] Epoch: [118]  [100/195]  eta: 0:00:06  lr: 0.000257  loss: 0.1396 (0.1390)  time: 0.0698  data: 0.0001  max mem: 2598
[14:19:11.280691] Epoch: [118]  [120/195]  eta: 0:00:05  lr: 0.000257  loss: 0.1394 (0.1393)  time: 0.0699  data: 0.0001  max mem: 2598
[14:19:12.643175] Epoch: [118]  [140/195]  eta: 0:00:03  lr: 0.000256  loss: 0.1371 (0.1391)  time: 0.0681  data: 0.0001  max mem: 2598
[14:19:13.999981] Epoch: [118]  [160/195]  eta: 0:00:02  lr: 0.000256  loss: 0.1402 (0.1392)  time: 0.0678  data: 0.0001  max mem: 2598
[14:19:15.354980] Epoch: [118]  [180/195]  eta: 0:00:01  lr: 0.000255  loss: 0.1385 (0.1391)  time: 0.0677  data: 0.0001  max mem: 2598
[14:19:16.298231] Epoch: [118]  [194/195]  eta: 0:00:00  lr: 0.000255  loss: 0.1377 (0.1389)  time: 0.0674  data: 0.0001  max mem: 2598
[14:19:16.351041] Epoch: [118] Total time: 0:00:13 (0.0713 s / it)
[14:19:16.351212] Averaged stats: lr: 0.000255  loss: 0.1377 (0.1389)
[14:19:16.353446] log_dir: ./output_dir/mae0.3
[14:19:16.723488] Epoch: [119]  [  0/195]  eta: 0:01:11  lr: 0.000255  loss: 0.1342 (0.1342)  time: 0.3688  data: 0.2621  max mem: 2598
[14:19:18.138611] Epoch: [119]  [ 20/195]  eta: 0:00:14  lr: 0.000254  loss: 0.1390 (0.1375)  time: 0.0707  data: 0.0001  max mem: 2598
[14:19:19.546169] Epoch: [119]  [ 40/195]  eta: 0:00:12  lr: 0.000254  loss: 0.1372 (0.1379)  time: 0.0703  data: 0.0001  max mem: 2598
[14:19:20.955020] Epoch: [119]  [ 60/195]  eta: 0:00:10  lr: 0.000253  loss: 0.1387 (0.1384)  time: 0.0704  data: 0.0001  max mem: 2598
[14:19:22.361698] Epoch: [119]  [ 80/195]  eta: 0:00:08  lr: 0.000253  loss: 0.1368 (0.1385)  time: 0.0703  data: 0.0001  max mem: 2598
[14:19:23.768125] Epoch: [119]  [100/195]  eta: 0:00:06  lr: 0.000252  loss: 0.1410 (0.1389)  time: 0.0703  data: 0.0001  max mem: 2598
[14:19:25.175793] Epoch: [119]  [120/195]  eta: 0:00:05  lr: 0.000252  loss: 0.1401 (0.1391)  time: 0.0703  data: 0.0001  max mem: 2598
[14:19:26.596983] Epoch: [119]  [140/195]  eta: 0:00:03  lr: 0.000251  loss: 0.1391 (0.1392)  time: 0.0710  data: 0.0002  max mem: 2598
[14:19:28.030850] Epoch: [119]  [160/195]  eta: 0:00:02  lr: 0.000251  loss: 0.1386 (0.1392)  time: 0.0716  data: 0.0002  max mem: 2598
[14:19:29.435099] Epoch: [119]  [180/195]  eta: 0:00:01  lr: 0.000250  loss: 0.1368 (0.1391)  time: 0.0702  data: 0.0002  max mem: 2598
[14:19:30.401780] Epoch: [119]  [194/195]  eta: 0:00:00  lr: 0.000250  loss: 0.1390 (0.1391)  time: 0.0691  data: 0.0001  max mem: 2598
[14:19:30.468326] Epoch: [119] Total time: 0:00:14 (0.0724 s / it)
[14:19:30.468747] Averaged stats: lr: 0.000250  loss: 0.1390 (0.1391)
[14:19:30.471171] log_dir: ./output_dir/mae0.3
[14:19:30.867393] Epoch: [120]  [  0/195]  eta: 0:01:16  lr: 0.000250  loss: 0.1408 (0.1408)  time: 0.3945  data: 0.2757  max mem: 2598
[14:19:32.309388] Epoch: [120]  [ 20/195]  eta: 0:00:15  lr: 0.000249  loss: 0.1387 (0.1375)  time: 0.0721  data: 0.0002  max mem: 2598
[14:19:33.720220] Epoch: [120]  [ 40/195]  eta: 0:00:12  lr: 0.000249  loss: 0.1391 (0.1379)  time: 0.0705  data: 0.0001  max mem: 2598
[14:19:35.145331] Epoch: [120]  [ 60/195]  eta: 0:00:10  lr: 0.000248  loss: 0.1385 (0.1381)  time: 0.0712  data: 0.0002  max mem: 2598
[14:19:36.530055] Epoch: [120]  [ 80/195]  eta: 0:00:08  lr: 0.000248  loss: 0.1362 (0.1380)  time: 0.0692  data: 0.0001  max mem: 2598
[14:19:37.906979] Epoch: [120]  [100/195]  eta: 0:00:06  lr: 0.000247  loss: 0.1381 (0.1382)  time: 0.0688  data: 0.0001  max mem: 2598
[14:19:39.280783] Epoch: [120]  [120/195]  eta: 0:00:05  lr: 0.000247  loss: 0.1377 (0.1386)  time: 0.0687  data: 0.0001  max mem: 2598
[14:19:40.693073] Epoch: [120]  [140/195]  eta: 0:00:03  lr: 0.000246  loss: 0.1395 (0.1387)  time: 0.0706  data: 0.0001  max mem: 2598
[14:19:42.106408] Epoch: [120]  [160/195]  eta: 0:00:02  lr: 0.000246  loss: 0.1400 (0.1390)  time: 0.0706  data: 0.0001  max mem: 2598
[14:19:43.515873] Epoch: [120]  [180/195]  eta: 0:00:01  lr: 0.000245  loss: 0.1384 (0.1390)  time: 0.0704  data: 0.0001  max mem: 2598
[14:19:44.486634] Epoch: [120]  [194/195]  eta: 0:00:00  lr: 0.000245  loss: 0.1399 (0.1391)  time: 0.0693  data: 0.0001  max mem: 2598
[14:19:44.531120] Epoch: [120] Total time: 0:00:14 (0.0721 s / it)
[14:19:44.531317] Averaged stats: lr: 0.000245  loss: 0.1399 (0.1391)
[14:19:44.718201] log_dir: ./output_dir/mae0.3
[14:19:45.108915] Epoch: [121]  [  0/195]  eta: 0:01:15  lr: 0.000245  loss: 0.1420 (0.1420)  time: 0.3895  data: 0.2977  max mem: 2598
[14:19:46.564987] Epoch: [121]  [ 20/195]  eta: 0:00:15  lr: 0.000245  loss: 0.1359 (0.1368)  time: 0.0728  data: 0.0001  max mem: 2598
[14:19:47.976403] Epoch: [121]  [ 40/195]  eta: 0:00:12  lr: 0.000244  loss: 0.1392 (0.1380)  time: 0.0705  data: 0.0001  max mem: 2598
[14:19:49.369763] Epoch: [121]  [ 60/195]  eta: 0:00:10  lr: 0.000244  loss: 0.1386 (0.1383)  time: 0.0696  data: 0.0001  max mem: 2598
[14:19:50.757160] Epoch: [121]  [ 80/195]  eta: 0:00:08  lr: 0.000243  loss: 0.1384 (0.1385)  time: 0.0693  data: 0.0001  max mem: 2598
[14:19:52.146014] Epoch: [121]  [100/195]  eta: 0:00:06  lr: 0.000243  loss: 0.1369 (0.1383)  time: 0.0694  data: 0.0001  max mem: 2598
[14:19:53.543778] Epoch: [121]  [120/195]  eta: 0:00:05  lr: 0.000242  loss: 0.1381 (0.1383)  time: 0.0699  data: 0.0002  max mem: 2598
[14:19:54.951792] Epoch: [121]  [140/195]  eta: 0:00:03  lr: 0.000242  loss: 0.1392 (0.1383)  time: 0.0704  data: 0.0002  max mem: 2598
[14:19:56.358707] Epoch: [121]  [160/195]  eta: 0:00:02  lr: 0.000241  loss: 0.1376 (0.1384)  time: 0.0703  data: 0.0002  max mem: 2598
[14:19:57.764299] Epoch: [121]  [180/195]  eta: 0:00:01  lr: 0.000241  loss: 0.1381 (0.1386)  time: 0.0702  data: 0.0002  max mem: 2598
[14:19:58.729787] Epoch: [121]  [194/195]  eta: 0:00:00  lr: 0.000240  loss: 0.1385 (0.1385)  time: 0.0691  data: 0.0001  max mem: 2598
[14:19:58.780217] Epoch: [121] Total time: 0:00:14 (0.0721 s / it)
[14:19:58.780301] Averaged stats: lr: 0.000240  loss: 0.1385 (0.1385)
[14:19:58.783512] log_dir: ./output_dir/mae0.3
[14:19:59.184048] Epoch: [122]  [  0/195]  eta: 0:01:17  lr: 0.000240  loss: 0.1325 (0.1325)  time: 0.3990  data: 0.3125  max mem: 2598
[14:20:00.646524] Epoch: [122]  [ 20/195]  eta: 0:00:15  lr: 0.000240  loss: 0.1367 (0.1361)  time: 0.0731  data: 0.0002  max mem: 2598
[14:20:02.059838] Epoch: [122]  [ 40/195]  eta: 0:00:12  lr: 0.000239  loss: 0.1366 (0.1370)  time: 0.0706  data: 0.0002  max mem: 2598
[14:20:03.453264] Epoch: [122]  [ 60/195]  eta: 0:00:10  lr: 0.000239  loss: 0.1381 (0.1370)  time: 0.0696  data: 0.0001  max mem: 2598
[14:20:04.859277] Epoch: [122]  [ 80/195]  eta: 0:00:08  lr: 0.000238  loss: 0.1374 (0.1372)  time: 0.0703  data: 0.0002  max mem: 2598
[14:20:06.232792] Epoch: [122]  [100/195]  eta: 0:00:07  lr: 0.000238  loss: 0.1389 (0.1375)  time: 0.0686  data: 0.0001  max mem: 2598
[14:20:07.627457] Epoch: [122]  [120/195]  eta: 0:00:05  lr: 0.000237  loss: 0.1396 (0.1377)  time: 0.0697  data: 0.0002  max mem: 2598
[14:20:09.040475] Epoch: [122]  [140/195]  eta: 0:00:03  lr: 0.000237  loss: 0.1390 (0.1378)  time: 0.0706  data: 0.0002  max mem: 2598
[14:20:10.454359] Epoch: [122]  [160/195]  eta: 0:00:02  lr: 0.000236  loss: 0.1376 (0.1380)  time: 0.0706  data: 0.0002  max mem: 2598
[14:20:11.858335] Epoch: [122]  [180/195]  eta: 0:00:01  lr: 0.000236  loss: 0.1375 (0.1380)  time: 0.0702  data: 0.0001  max mem: 2598
[14:20:12.817619] Epoch: [122]  [194/195]  eta: 0:00:00  lr: 0.000235  loss: 0.1378 (0.1380)  time: 0.0686  data: 0.0001  max mem: 2598
[14:20:12.862799] Epoch: [122] Total time: 0:00:14 (0.0722 s / it)
[14:20:12.862960] Averaged stats: lr: 0.000235  loss: 0.1378 (0.1380)
[14:20:12.865269] log_dir: ./output_dir/mae0.3
[14:20:13.264987] Epoch: [123]  [  0/195]  eta: 0:01:17  lr: 0.000235  loss: 0.1325 (0.1325)  time: 0.3980  data: 0.2971  max mem: 2598
[14:20:14.684608] Epoch: [123]  [ 20/195]  eta: 0:00:15  lr: 0.000235  loss: 0.1376 (0.1377)  time: 0.0709  data: 0.0001  max mem: 2598
[14:20:16.077964] Epoch: [123]  [ 40/195]  eta: 0:00:12  lr: 0.000234  loss: 0.1366 (0.1379)  time: 0.0696  data: 0.0001  max mem: 2598
[14:20:17.484121] Epoch: [123]  [ 60/195]  eta: 0:00:10  lr: 0.000234  loss: 0.1380 (0.1379)  time: 0.0703  data: 0.0001  max mem: 2598
[14:20:18.902461] Epoch: [123]  [ 80/195]  eta: 0:00:08  lr: 0.000233  loss: 0.1401 (0.1383)  time: 0.0709  data: 0.0001  max mem: 2598
[14:20:20.319393] Epoch: [123]  [100/195]  eta: 0:00:07  lr: 0.000233  loss: 0.1377 (0.1384)  time: 0.0708  data: 0.0002  max mem: 2598
[14:20:21.728256] Epoch: [123]  [120/195]  eta: 0:00:05  lr: 0.000232  loss: 0.1394 (0.1387)  time: 0.0704  data: 0.0002  max mem: 2598
[14:20:23.138759] Epoch: [123]  [140/195]  eta: 0:00:04  lr: 0.000232  loss: 0.1366 (0.1385)  time: 0.0705  data: 0.0001  max mem: 2598
[14:20:24.544220] Epoch: [123]  [160/195]  eta: 0:00:02  lr: 0.000231  loss: 0.1390 (0.1387)  time: 0.0702  data: 0.0001  max mem: 2598
[14:20:25.942941] Epoch: [123]  [180/195]  eta: 0:00:01  lr: 0.000231  loss: 0.1389 (0.1388)  time: 0.0699  data: 0.0001  max mem: 2598
[14:20:26.913482] Epoch: [123]  [194/195]  eta: 0:00:00  lr: 0.000230  loss: 0.1389 (0.1388)  time: 0.0694  data: 0.0001  max mem: 2598
[14:20:26.958107] Epoch: [123] Total time: 0:00:14 (0.0723 s / it)
[14:20:26.958178] Averaged stats: lr: 0.000230  loss: 0.1389 (0.1388)
[14:20:26.960791] log_dir: ./output_dir/mae0.3
[14:20:27.352823] Epoch: [124]  [  0/195]  eta: 0:01:16  lr: 0.000230  loss: 0.1375 (0.1375)  time: 0.3902  data: 0.3072  max mem: 2598
[14:20:28.740921] Epoch: [124]  [ 20/195]  eta: 0:00:14  lr: 0.000230  loss: 0.1392 (0.1379)  time: 0.0694  data: 0.0001  max mem: 2598
[14:20:30.119717] Epoch: [124]  [ 40/195]  eta: 0:00:11  lr: 0.000229  loss: 0.1345 (0.1375)  time: 0.0689  data: 0.0001  max mem: 2598
[14:20:31.504648] Epoch: [124]  [ 60/195]  eta: 0:00:10  lr: 0.000229  loss: 0.1360 (0.1374)  time: 0.0692  data: 0.0001  max mem: 2598
[14:20:32.886116] Epoch: [124]  [ 80/195]  eta: 0:00:08  lr: 0.000228  loss: 0.1387 (0.1380)  time: 0.0690  data: 0.0001  max mem: 2598
[14:20:34.262563] Epoch: [124]  [100/195]  eta: 0:00:06  lr: 0.000228  loss: 0.1383 (0.1379)  time: 0.0688  data: 0.0001  max mem: 2598
[14:20:35.636934] Epoch: [124]  [120/195]  eta: 0:00:05  lr: 0.000227  loss: 0.1387 (0.1382)  time: 0.0687  data: 0.0001  max mem: 2598
[14:20:37.027681] Epoch: [124]  [140/195]  eta: 0:00:03  lr: 0.000227  loss: 0.1375 (0.1382)  time: 0.0695  data: 0.0001  max mem: 2598
[14:20:38.419092] Epoch: [124]  [160/195]  eta: 0:00:02  lr: 0.000226  loss: 0.1413 (0.1386)  time: 0.0695  data: 0.0001  max mem: 2598
[14:20:39.798273] Epoch: [124]  [180/195]  eta: 0:00:01  lr: 0.000226  loss: 0.1376 (0.1386)  time: 0.0689  data: 0.0001  max mem: 2598
[14:20:40.754955] Epoch: [124]  [194/195]  eta: 0:00:00  lr: 0.000226  loss: 0.1386 (0.1385)  time: 0.0682  data: 0.0001  max mem: 2598
[14:20:40.801020] Epoch: [124] Total time: 0:00:13 (0.0710 s / it)
[14:20:40.801090] Averaged stats: lr: 0.000226  loss: 0.1386 (0.1385)
[14:20:40.803358] log_dir: ./output_dir/mae0.3
[14:20:41.226405] Epoch: [125]  [  0/195]  eta: 0:01:22  lr: 0.000225  loss: 0.1354 (0.1354)  time: 0.4213  data: 0.2969  max mem: 2598
[14:20:42.659738] Epoch: [125]  [ 20/195]  eta: 0:00:15  lr: 0.000225  loss: 0.1377 (0.1376)  time: 0.0716  data: 0.0001  max mem: 2598
[14:20:44.074989] Epoch: [125]  [ 40/195]  eta: 0:00:12  lr: 0.000224  loss: 0.1394 (0.1379)  time: 0.0707  data: 0.0002  max mem: 2598
[14:20:45.492935] Epoch: [125]  [ 60/195]  eta: 0:00:10  lr: 0.000224  loss: 0.1391 (0.1389)  time: 0.0709  data: 0.0001  max mem: 2598
[14:20:46.906809] Epoch: [125]  [ 80/195]  eta: 0:00:08  lr: 0.000223  loss: 0.1374 (0.1385)  time: 0.0707  data: 0.0001  max mem: 2598
[14:20:48.319733] Epoch: [125]  [100/195]  eta: 0:00:07  lr: 0.000223  loss: 0.1394 (0.1387)  time: 0.0706  data: 0.0001  max mem: 2598
[14:20:49.732329] Epoch: [125]  [120/195]  eta: 0:00:05  lr: 0.000222  loss: 0.1392 (0.1388)  time: 0.0706  data: 0.0001  max mem: 2598
[14:20:51.125879] Epoch: [125]  [140/195]  eta: 0:00:04  lr: 0.000222  loss: 0.1386 (0.1387)  time: 0.0696  data: 0.0001  max mem: 2598
[14:20:52.528603] Epoch: [125]  [160/195]  eta: 0:00:02  lr: 0.000221  loss: 0.1354 (0.1387)  time: 0.0701  data: 0.0002  max mem: 2598
[14:20:53.905137] Epoch: [125]  [180/195]  eta: 0:00:01  lr: 0.000221  loss: 0.1369 (0.1386)  time: 0.0688  data: 0.0001  max mem: 2598
[14:20:54.856818] Epoch: [125]  [194/195]  eta: 0:00:00  lr: 0.000221  loss: 0.1389 (0.1387)  time: 0.0679  data: 0.0001  max mem: 2598
[14:20:54.903956] Epoch: [125] Total time: 0:00:14 (0.0723 s / it)
[14:20:54.904026] Averaged stats: lr: 0.000221  loss: 0.1389 (0.1387)
[14:20:54.906316] log_dir: ./output_dir/mae0.3
[14:20:55.295606] Epoch: [126]  [  0/195]  eta: 0:01:15  lr: 0.000221  loss: 0.1337 (0.1337)  time: 0.3880  data: 0.2878  max mem: 2598
[14:20:56.744015] Epoch: [126]  [ 20/195]  eta: 0:00:15  lr: 0.000220  loss: 0.1348 (0.1354)  time: 0.0724  data: 0.0002  max mem: 2598
[14:20:58.153553] Epoch: [126]  [ 40/195]  eta: 0:00:12  lr: 0.000220  loss: 0.1341 (0.1360)  time: 0.0704  data: 0.0001  max mem: 2598
[14:20:59.574955] Epoch: [126]  [ 60/195]  eta: 0:00:10  lr: 0.000219  loss: 0.1357 (0.1361)  time: 0.0710  data: 0.0001  max mem: 2598
[14:21:00.993836] Epoch: [126]  [ 80/195]  eta: 0:00:08  lr: 0.000219  loss: 0.1360 (0.1361)  time: 0.0709  data: 0.0001  max mem: 2598
[14:21:02.420996] Epoch: [126]  [100/195]  eta: 0:00:07  lr: 0.000218  loss: 0.1378 (0.1364)  time: 0.0713  data: 0.0002  max mem: 2598
[14:21:03.829513] Epoch: [126]  [120/195]  eta: 0:00:05  lr: 0.000218  loss: 0.1398 (0.1367)  time: 0.0704  data: 0.0001  max mem: 2598
[14:21:05.233329] Epoch: [126]  [140/195]  eta: 0:00:04  lr: 0.000217  loss: 0.1359 (0.1367)  time: 0.0702  data: 0.0001  max mem: 2598
[14:21:06.646767] Epoch: [126]  [160/195]  eta: 0:00:02  lr: 0.000217  loss: 0.1365 (0.1368)  time: 0.0706  data: 0.0002  max mem: 2598
[14:21:08.053415] Epoch: [126]  [180/195]  eta: 0:00:01  lr: 0.000216  loss: 0.1385 (0.1368)  time: 0.0703  data: 0.0002  max mem: 2598
[14:21:09.037794] Epoch: [126]  [194/195]  eta: 0:00:00  lr: 0.000216  loss: 0.1362 (0.1368)  time: 0.0701  data: 0.0001  max mem: 2598
[14:21:09.106514] Epoch: [126] Total time: 0:00:14 (0.0728 s / it)
[14:21:09.106586] Averaged stats: lr: 0.000216  loss: 0.1362 (0.1368)
[14:21:09.109097] log_dir: ./output_dir/mae0.3
[14:21:09.487828] Epoch: [127]  [  0/195]  eta: 0:01:13  lr: 0.000216  loss: 0.1317 (0.1317)  time: 0.3776  data: 0.2981  max mem: 2598
[14:21:10.897559] Epoch: [127]  [ 20/195]  eta: 0:00:14  lr: 0.000215  loss: 0.1347 (0.1349)  time: 0.0704  data: 0.0001  max mem: 2598
[14:21:12.305120] Epoch: [127]  [ 40/195]  eta: 0:00:12  lr: 0.000215  loss: 0.1350 (0.1361)  time: 0.0703  data: 0.0002  max mem: 2598
[14:21:13.704541] Epoch: [127]  [ 60/195]  eta: 0:00:10  lr: 0.000214  loss: 0.1370 (0.1367)  time: 0.0699  data: 0.0002  max mem: 2598
[14:21:15.106709] Epoch: [127]  [ 80/195]  eta: 0:00:08  lr: 0.000214  loss: 0.1353 (0.1365)  time: 0.0701  data: 0.0001  max mem: 2598
[14:21:16.508322] Epoch: [127]  [100/195]  eta: 0:00:06  lr: 0.000213  loss: 0.1365 (0.1364)  time: 0.0700  data: 0.0002  max mem: 2598
[14:21:17.905769] Epoch: [127]  [120/195]  eta: 0:00:05  lr: 0.000213  loss: 0.1354 (0.1364)  time: 0.0698  data: 0.0001  max mem: 2598
[14:21:19.300023] Epoch: [127]  [140/195]  eta: 0:00:03  lr: 0.000212  loss: 0.1367 (0.1365)  time: 0.0697  data: 0.0001  max mem: 2598
[14:21:20.698820] Epoch: [127]  [160/195]  eta: 0:00:02  lr: 0.000212  loss: 0.1369 (0.1367)  time: 0.0699  data: 0.0001  max mem: 2598
[14:21:22.089955] Epoch: [127]  [180/195]  eta: 0:00:01  lr: 0.000211  loss: 0.1394 (0.1370)  time: 0.0695  data: 0.0001  max mem: 2598
[14:21:23.047855] Epoch: [127]  [194/195]  eta: 0:00:00  lr: 0.000211  loss: 0.1390 (0.1371)  time: 0.0686  data: 0.0001  max mem: 2598
[14:21:23.092506] Epoch: [127] Total time: 0:00:13 (0.0717 s / it)
[14:21:23.092567] Averaged stats: lr: 0.000211  loss: 0.1390 (0.1371)
[14:21:23.094817] log_dir: ./output_dir/mae0.3
[14:21:23.464360] Epoch: [128]  [  0/195]  eta: 0:01:11  lr: 0.000211  loss: 0.1375 (0.1375)  time: 0.3686  data: 0.2895  max mem: 2598
[14:21:24.852764] Epoch: [128]  [ 20/195]  eta: 0:00:14  lr: 0.000210  loss: 0.1378 (0.1363)  time: 0.0694  data: 0.0001  max mem: 2598
[14:21:26.243243] Epoch: [128]  [ 40/195]  eta: 0:00:11  lr: 0.000210  loss: 0.1359 (0.1366)  time: 0.0695  data: 0.0001  max mem: 2598
[14:21:27.634304] Epoch: [128]  [ 60/195]  eta: 0:00:10  lr: 0.000209  loss: 0.1359 (0.1366)  time: 0.0695  data: 0.0001  max mem: 2598
[14:21:29.032200] Epoch: [128]  [ 80/195]  eta: 0:00:08  lr: 0.000209  loss: 0.1363 (0.1366)  time: 0.0699  data: 0.0001  max mem: 2598
[14:21:30.432645] Epoch: [128]  [100/195]  eta: 0:00:06  lr: 0.000208  loss: 0.1366 (0.1367)  time: 0.0700  data: 0.0001  max mem: 2598
[14:21:31.831357] Epoch: [128]  [120/195]  eta: 0:00:05  lr: 0.000208  loss: 0.1373 (0.1367)  time: 0.0699  data: 0.0001  max mem: 2598
[14:21:33.225090] Epoch: [128]  [140/195]  eta: 0:00:03  lr: 0.000207  loss: 0.1367 (0.1367)  time: 0.0697  data: 0.0001  max mem: 2598
[14:21:34.622872] Epoch: [128]  [160/195]  eta: 0:00:02  lr: 0.000207  loss: 0.1366 (0.1368)  time: 0.0698  data: 0.0001  max mem: 2598
[14:21:36.018493] Epoch: [128]  [180/195]  eta: 0:00:01  lr: 0.000206  loss: 0.1368 (0.1368)  time: 0.0697  data: 0.0001  max mem: 2598
[14:21:36.972822] Epoch: [128]  [194/195]  eta: 0:00:00  lr: 0.000206  loss: 0.1377 (0.1369)  time: 0.0684  data: 0.0001  max mem: 2598
[14:21:37.050154] Epoch: [128] Total time: 0:00:13 (0.0716 s / it)
[14:21:37.050216] Averaged stats: lr: 0.000206  loss: 0.1377 (0.1369)
[14:21:37.052352] log_dir: ./output_dir/mae0.3
[14:21:37.439392] Epoch: [129]  [  0/195]  eta: 0:01:15  lr: 0.000206  loss: 0.1410 (0.1410)  time: 0.3859  data: 0.2961  max mem: 2598
[14:21:38.897902] Epoch: [129]  [ 20/195]  eta: 0:00:15  lr: 0.000206  loss: 0.1346 (0.1364)  time: 0.0729  data: 0.0002  max mem: 2598
[14:21:40.346559] Epoch: [129]  [ 40/195]  eta: 0:00:12  lr: 0.000205  loss: 0.1364 (0.1362)  time: 0.0724  data: 0.0002  max mem: 2598
[14:21:41.782081] Epoch: [129]  [ 60/195]  eta: 0:00:10  lr: 0.000205  loss: 0.1357 (0.1361)  time: 0.0717  data: 0.0001  max mem: 2598
[14:21:43.174761] Epoch: [129]  [ 80/195]  eta: 0:00:08  lr: 0.000204  loss: 0.1378 (0.1364)  time: 0.0696  data: 0.0001  max mem: 2598
[14:21:44.569980] Epoch: [129]  [100/195]  eta: 0:00:07  lr: 0.000204  loss: 0.1372 (0.1369)  time: 0.0697  data: 0.0001  max mem: 2598
[14:21:45.946485] Epoch: [129]  [120/195]  eta: 0:00:05  lr: 0.000203  loss: 0.1341 (0.1371)  time: 0.0688  data: 0.0001  max mem: 2598
[14:21:47.319141] Epoch: [129]  [140/195]  eta: 0:00:04  lr: 0.000203  loss: 0.1348 (0.1369)  time: 0.0686  data: 0.0001  max mem: 2598
[14:21:48.697079] Epoch: [129]  [160/195]  eta: 0:00:02  lr: 0.000202  loss: 0.1361 (0.1370)  time: 0.0689  data: 0.0001  max mem: 2598
[14:21:50.075100] Epoch: [129]  [180/195]  eta: 0:00:01  lr: 0.000202  loss: 0.1363 (0.1370)  time: 0.0689  data: 0.0001  max mem: 2598
[14:21:51.026228] Epoch: [129]  [194/195]  eta: 0:00:00  lr: 0.000201  loss: 0.1373 (0.1370)  time: 0.0681  data: 0.0001  max mem: 2598
[14:21:51.072899] Epoch: [129] Total time: 0:00:14 (0.0719 s / it)
[14:21:51.072958] Averaged stats: lr: 0.000201  loss: 0.1373 (0.1370)
[14:21:51.075143] log_dir: ./output_dir/mae0.3
[14:21:51.459076] Epoch: [130]  [  0/195]  eta: 0:01:14  lr: 0.000201  loss: 0.1379 (0.1379)  time: 0.3828  data: 0.2821  max mem: 2598
[14:21:52.870232] Epoch: [130]  [ 20/195]  eta: 0:00:14  lr: 0.000201  loss: 0.1366 (0.1364)  time: 0.0705  data: 0.0001  max mem: 2598
[14:21:54.266925] Epoch: [130]  [ 40/195]  eta: 0:00:12  lr: 0.000200  loss: 0.1351 (0.1369)  time: 0.0698  data: 0.0001  max mem: 2598
[14:21:55.664183] Epoch: [130]  [ 60/195]  eta: 0:00:10  lr: 0.000200  loss: 0.1367 (0.1369)  time: 0.0698  data: 0.0001  max mem: 2598
[14:21:57.057935] Epoch: [130]  [ 80/195]  eta: 0:00:08  lr: 0.000199  loss: 0.1370 (0.1373)  time: 0.0697  data: 0.0001  max mem: 2598
[14:21:58.440796] Epoch: [130]  [100/195]  eta: 0:00:06  lr: 0.000199  loss: 0.1375 (0.1375)  time: 0.0691  data: 0.0001  max mem: 2598
[14:21:59.821420] Epoch: [130]  [120/195]  eta: 0:00:05  lr: 0.000198  loss: 0.1371 (0.1374)  time: 0.0690  data: 0.0001  max mem: 2598
[14:22:01.205001] Epoch: [130]  [140/195]  eta: 0:00:03  lr: 0.000198  loss: 0.1362 (0.1374)  time: 0.0691  data: 0.0001  max mem: 2598
[14:22:02.604661] Epoch: [130]  [160/195]  eta: 0:00:02  lr: 0.000197  loss: 0.1360 (0.1373)  time: 0.0699  data: 0.0001  max mem: 2598
[14:22:03.981225] Epoch: [130]  [180/195]  eta: 0:00:01  lr: 0.000197  loss: 0.1389 (0.1374)  time: 0.0688  data: 0.0001  max mem: 2598
[14:22:04.934867] Epoch: [130]  [194/195]  eta: 0:00:00  lr: 0.000196  loss: 0.1361 (0.1373)  time: 0.0683  data: 0.0001  max mem: 2598
[14:22:05.002993] Epoch: [130] Total time: 0:00:13 (0.0714 s / it)
[14:22:05.003062] Averaged stats: lr: 0.000196  loss: 0.1361 (0.1373)
[14:22:05.005406] log_dir: ./output_dir/mae0.3
[14:22:05.428094] Epoch: [131]  [  0/195]  eta: 0:01:22  lr: 0.000196  loss: 0.1335 (0.1335)  time: 0.4212  data: 0.2987  max mem: 2598
[14:22:06.865939] Epoch: [131]  [ 20/195]  eta: 0:00:15  lr: 0.000196  loss: 0.1342 (0.1340)  time: 0.0718  data: 0.0002  max mem: 2598
[14:22:08.283326] Epoch: [131]  [ 40/195]  eta: 0:00:12  lr: 0.000195  loss: 0.1362 (0.1352)  time: 0.0708  data: 0.0001  max mem: 2598
[14:22:09.703118] Epoch: [131]  [ 60/195]  eta: 0:00:10  lr: 0.000195  loss: 0.1374 (0.1359)  time: 0.0709  data: 0.0001  max mem: 2598
[14:22:11.111939] Epoch: [131]  [ 80/195]  eta: 0:00:08  lr: 0.000194  loss: 0.1386 (0.1365)  time: 0.0704  data: 0.0001  max mem: 2598
[14:22:12.527966] Epoch: [131]  [100/195]  eta: 0:00:07  lr: 0.000194  loss: 0.1334 (0.1363)  time: 0.0708  data: 0.0002  max mem: 2598
[14:22:13.950187] Epoch: [131]  [120/195]  eta: 0:00:05  lr: 0.000193  loss: 0.1385 (0.1366)  time: 0.0711  data: 0.0002  max mem: 2598
[14:22:15.363724] Epoch: [131]  [140/195]  eta: 0:00:04  lr: 0.000193  loss: 0.1379 (0.1367)  time: 0.0706  data: 0.0001  max mem: 2598
[14:22:16.775563] Epoch: [131]  [160/195]  eta: 0:00:02  lr: 0.000192  loss: 0.1403 (0.1369)  time: 0.0705  data: 0.0001  max mem: 2598
[14:22:18.176748] Epoch: [131]  [180/195]  eta: 0:00:01  lr: 0.000192  loss: 0.1357 (0.1368)  time: 0.0700  data: 0.0001  max mem: 2598
[14:22:19.143535] Epoch: [131]  [194/195]  eta: 0:00:00  lr: 0.000192  loss: 0.1343 (0.1367)  time: 0.0691  data: 0.0001  max mem: 2598
[14:22:19.212484] Epoch: [131] Total time: 0:00:14 (0.0729 s / it)
[14:22:19.212554] Averaged stats: lr: 0.000192  loss: 0.1343 (0.1367)
[14:22:19.215022] log_dir: ./output_dir/mae0.3
[14:22:19.665578] Epoch: [132]  [  0/195]  eta: 0:01:27  lr: 0.000192  loss: 0.1384 (0.1384)  time: 0.4487  data: 0.3132  max mem: 2598
[14:22:21.115880] Epoch: [132]  [ 20/195]  eta: 0:00:15  lr: 0.000191  loss: 0.1350 (0.1350)  time: 0.0725  data: 0.0002  max mem: 2598
[14:22:22.520485] Epoch: [132]  [ 40/195]  eta: 0:00:12  lr: 0.000191  loss: 0.1362 (0.1358)  time: 0.0702  data: 0.0001  max mem: 2598
[14:22:23.921716] Epoch: [132]  [ 60/195]  eta: 0:00:10  lr: 0.000190  loss: 0.1357 (0.1355)  time: 0.0700  data: 0.0001  max mem: 2598
[14:22:25.330286] Epoch: [132]  [ 80/195]  eta: 0:00:08  lr: 0.000190  loss: 0.1357 (0.1357)  time: 0.0704  data: 0.0001  max mem: 2598
[14:22:26.735889] Epoch: [132]  [100/195]  eta: 0:00:07  lr: 0.000189  loss: 0.1349 (0.1356)  time: 0.0702  data: 0.0001  max mem: 2598
[14:22:28.151776] Epoch: [132]  [120/195]  eta: 0:00:05  lr: 0.000189  loss: 0.1371 (0.1359)  time: 0.0708  data: 0.0001  max mem: 2598
[14:22:29.560308] Epoch: [132]  [140/195]  eta: 0:00:04  lr: 0.000188  loss: 0.1346 (0.1358)  time: 0.0704  data: 0.0001  max mem: 2598
[14:22:30.971464] Epoch: [132]  [160/195]  eta: 0:00:02  lr: 0.000188  loss: 0.1357 (0.1359)  time: 0.0705  data: 0.0001  max mem: 2598
[14:22:32.377508] Epoch: [132]  [180/195]  eta: 0:00:01  lr: 0.000187  loss: 0.1348 (0.1359)  time: 0.0703  data: 0.0001  max mem: 2598
[14:22:33.344061] Epoch: [132]  [194/195]  eta: 0:00:00  lr: 0.000187  loss: 0.1348 (0.1358)  time: 0.0692  data: 0.0001  max mem: 2598
[14:22:33.392447] Epoch: [132] Total time: 0:00:14 (0.0727 s / it)
[14:22:33.392535] Averaged stats: lr: 0.000187  loss: 0.1348 (0.1358)
[14:22:33.395805] log_dir: ./output_dir/mae0.3
[14:22:33.798285] Epoch: [133]  [  0/195]  eta: 0:01:18  lr: 0.000187  loss: 0.1406 (0.1406)  time: 0.4010  data: 0.3149  max mem: 2598
[14:22:35.210147] Epoch: [133]  [ 20/195]  eta: 0:00:15  lr: 0.000186  loss: 0.1365 (0.1348)  time: 0.0705  data: 0.0001  max mem: 2598
[14:22:36.586692] Epoch: [133]  [ 40/195]  eta: 0:00:12  lr: 0.000186  loss: 0.1339 (0.1350)  time: 0.0688  data: 0.0001  max mem: 2598
[14:22:37.958637] Epoch: [133]  [ 60/195]  eta: 0:00:10  lr: 0.000185  loss: 0.1354 (0.1349)  time: 0.0686  data: 0.0001  max mem: 2598
[14:22:39.331829] Epoch: [133]  [ 80/195]  eta: 0:00:08  lr: 0.000185  loss: 0.1338 (0.1348)  time: 0.0686  data: 0.0001  max mem: 2598
[14:22:40.704732] Epoch: [133]  [100/195]  eta: 0:00:06  lr: 0.000184  loss: 0.1362 (0.1352)  time: 0.0686  data: 0.0001  max mem: 2598
[14:22:42.076787] Epoch: [133]  [120/195]  eta: 0:00:05  lr: 0.000184  loss: 0.1346 (0.1351)  time: 0.0686  data: 0.0001  max mem: 2598
[14:22:43.449573] Epoch: [133]  [140/195]  eta: 0:00:03  lr: 0.000183  loss: 0.1356 (0.1352)  time: 0.0686  data: 0.0001  max mem: 2598
[14:22:44.835272] Epoch: [133]  [160/195]  eta: 0:00:02  lr: 0.000183  loss: 0.1377 (0.1356)  time: 0.0692  data: 0.0001  max mem: 2598
[14:22:46.210212] Epoch: [133]  [180/195]  eta: 0:00:01  lr: 0.000183  loss: 0.1358 (0.1357)  time: 0.0687  data: 0.0001  max mem: 2598
[14:22:47.166416] Epoch: [133]  [194/195]  eta: 0:00:00  lr: 0.000182  loss: 0.1373 (0.1358)  time: 0.0683  data: 0.0001  max mem: 2598
[14:22:47.210910] Epoch: [133] Total time: 0:00:13 (0.0708 s / it)
[14:22:47.210984] Averaged stats: lr: 0.000182  loss: 0.1373 (0.1358)
[14:22:47.213676] log_dir: ./output_dir/mae0.3
[14:22:47.558358] Epoch: [134]  [  0/195]  eta: 0:01:07  lr: 0.000182  loss: 0.1400 (0.1400)  time: 0.3436  data: 0.2661  max mem: 2598
[14:22:48.965760] Epoch: [134]  [ 20/195]  eta: 0:00:14  lr: 0.000182  loss: 0.1335 (0.1357)  time: 0.0703  data: 0.0001  max mem: 2598
[14:22:50.374547] Epoch: [134]  [ 40/195]  eta: 0:00:11  lr: 0.000181  loss: 0.1345 (0.1358)  time: 0.0704  data: 0.0001  max mem: 2598
[14:22:51.769244] Epoch: [134]  [ 60/195]  eta: 0:00:10  lr: 0.000181  loss: 0.1351 (0.1354)  time: 0.0697  data: 0.0001  max mem: 2598
[14:22:53.160478] Epoch: [134]  [ 80/195]  eta: 0:00:08  lr: 0.000180  loss: 0.1359 (0.1355)  time: 0.0695  data: 0.0001  max mem: 2598
[14:22:54.555151] Epoch: [134]  [100/195]  eta: 0:00:06  lr: 0.000180  loss: 0.1325 (0.1353)  time: 0.0697  data: 0.0001  max mem: 2598
[14:22:55.934974] Epoch: [134]  [120/195]  eta: 0:00:05  lr: 0.000179  loss: 0.1358 (0.1356)  time: 0.0690  data: 0.0001  max mem: 2598
[14:22:57.316692] Epoch: [134]  [140/195]  eta: 0:00:03  lr: 0.000179  loss: 0.1376 (0.1358)  time: 0.0690  data: 0.0001  max mem: 2598
[14:22:58.692215] Epoch: [134]  [160/195]  eta: 0:00:02  lr: 0.000178  loss: 0.1346 (0.1358)  time: 0.0687  data: 0.0001  max mem: 2598
[14:23:00.075131] Epoch: [134]  [180/195]  eta: 0:00:01  lr: 0.000178  loss: 0.1357 (0.1358)  time: 0.0691  data: 0.0001  max mem: 2598
[14:23:01.026414] Epoch: [134]  [194/195]  eta: 0:00:00  lr: 0.000177  loss: 0.1353 (0.1358)  time: 0.0680  data: 0.0001  max mem: 2598
[14:23:01.077048] Epoch: [134] Total time: 0:00:13 (0.0711 s / it)
[14:23:01.077115] Averaged stats: lr: 0.000177  loss: 0.1353 (0.1358)
[14:23:01.079570] log_dir: ./output_dir/mae0.3
[14:23:01.514488] Epoch: [135]  [  0/195]  eta: 0:01:24  lr: 0.000177  loss: 0.1327 (0.1327)  time: 0.4333  data: 0.3140  max mem: 2598
[14:23:02.971547] Epoch: [135]  [ 20/195]  eta: 0:00:15  lr: 0.000177  loss: 0.1325 (0.1330)  time: 0.0728  data: 0.0002  max mem: 2598
[14:23:04.380710] Epoch: [135]  [ 40/195]  eta: 0:00:12  lr: 0.000176  loss: 0.1340 (0.1339)  time: 0.0704  data: 0.0001  max mem: 2598
[14:23:05.795411] Epoch: [135]  [ 60/195]  eta: 0:00:10  lr: 0.000176  loss: 0.1339 (0.1344)  time: 0.0707  data: 0.0001  max mem: 2598
[14:23:07.196828] Epoch: [135]  [ 80/195]  eta: 0:00:08  lr: 0.000176  loss: 0.1341 (0.1346)  time: 0.0700  data: 0.0001  max mem: 2598
[14:23:08.601122] Epoch: [135]  [100/195]  eta: 0:00:07  lr: 0.000175  loss: 0.1337 (0.1345)  time: 0.0701  data: 0.0001  max mem: 2598
[14:23:10.009023] Epoch: [135]  [120/195]  eta: 0:00:05  lr: 0.000175  loss: 0.1351 (0.1348)  time: 0.0703  data: 0.0002  max mem: 2598
[14:23:11.412537] Epoch: [135]  [140/195]  eta: 0:00:04  lr: 0.000174  loss: 0.1350 (0.1348)  time: 0.0701  data: 0.0002  max mem: 2598
[14:23:12.818174] Epoch: [135]  [160/195]  eta: 0:00:02  lr: 0.000174  loss: 0.1384 (0.1353)  time: 0.0702  data: 0.0001  max mem: 2598
[14:23:14.218565] Epoch: [135]  [180/195]  eta: 0:00:01  lr: 0.000173  loss: 0.1342 (0.1352)  time: 0.0700  data: 0.0002  max mem: 2598
[14:23:15.185896] Epoch: [135]  [194/195]  eta: 0:00:00  lr: 0.000173  loss: 0.1355 (0.1352)  time: 0.0691  data: 0.0001  max mem: 2598
[14:23:15.231419] Epoch: [135] Total time: 0:00:14 (0.0726 s / it)
[14:23:15.231542] Averaged stats: lr: 0.000173  loss: 0.1355 (0.1352)
[14:23:15.234165] log_dir: ./output_dir/mae0.3
[14:23:15.639827] Epoch: [136]  [  0/195]  eta: 0:01:18  lr: 0.000173  loss: 0.1359 (0.1359)  time: 0.4049  data: 0.3135  max mem: 2598
[14:23:17.060569] Epoch: [136]  [ 20/195]  eta: 0:00:15  lr: 0.000172  loss: 0.1354 (0.1344)  time: 0.0710  data: 0.0001  max mem: 2598
[14:23:18.470118] Epoch: [136]  [ 40/195]  eta: 0:00:12  lr: 0.000172  loss: 0.1347 (0.1345)  time: 0.0704  data: 0.0001  max mem: 2598
[14:23:19.872926] Epoch: [136]  [ 60/195]  eta: 0:00:10  lr: 0.000171  loss: 0.1343 (0.1347)  time: 0.0701  data: 0.0001  max mem: 2598
[14:23:21.286019] Epoch: [136]  [ 80/195]  eta: 0:00:08  lr: 0.000171  loss: 0.1361 (0.1348)  time: 0.0706  data: 0.0001  max mem: 2598
[14:23:22.694489] Epoch: [136]  [100/195]  eta: 0:00:07  lr: 0.000170  loss: 0.1352 (0.1348)  time: 0.0704  data: 0.0001  max mem: 2598
[14:23:24.104477] Epoch: [136]  [120/195]  eta: 0:00:05  lr: 0.000170  loss: 0.1344 (0.1351)  time: 0.0705  data: 0.0001  max mem: 2598
[14:23:25.516023] Epoch: [136]  [140/195]  eta: 0:00:04  lr: 0.000169  loss: 0.1358 (0.1352)  time: 0.0705  data: 0.0001  max mem: 2598
[14:23:26.925563] Epoch: [136]  [160/195]  eta: 0:00:02  lr: 0.000169  loss: 0.1369 (0.1355)  time: 0.0704  data: 0.0001  max mem: 2598
[14:23:28.324636] Epoch: [136]  [180/195]  eta: 0:00:01  lr: 0.000168  loss: 0.1345 (0.1355)  time: 0.0699  data: 0.0001  max mem: 2598
[14:23:29.291031] Epoch: [136]  [194/195]  eta: 0:00:00  lr: 0.000168  loss: 0.1334 (0.1354)  time: 0.0691  data: 0.0001  max mem: 2598
[14:23:29.338878] Epoch: [136] Total time: 0:00:14 (0.0723 s / it)
[14:23:29.338952] Averaged stats: lr: 0.000168  loss: 0.1334 (0.1354)
[14:23:29.341581] log_dir: ./output_dir/mae0.3
[14:23:29.764163] Epoch: [137]  [  0/195]  eta: 0:01:22  lr: 0.000168  loss: 0.1407 (0.1407)  time: 0.4209  data: 0.2974  max mem: 2598
[14:23:31.203797] Epoch: [137]  [ 20/195]  eta: 0:00:15  lr: 0.000168  loss: 0.1350 (0.1342)  time: 0.0719  data: 0.0001  max mem: 2598
[14:23:32.644890] Epoch: [137]  [ 40/195]  eta: 0:00:12  lr: 0.000167  loss: 0.1345 (0.1342)  time: 0.0720  data: 0.0001  max mem: 2598
[14:23:34.087054] Epoch: [137]  [ 60/195]  eta: 0:00:10  lr: 0.000167  loss: 0.1367 (0.1345)  time: 0.0721  data: 0.0002  max mem: 2598
[14:23:35.511477] Epoch: [137]  [ 80/195]  eta: 0:00:08  lr: 0.000166  loss: 0.1364 (0.1353)  time: 0.0712  data: 0.0002  max mem: 2598
[14:23:36.942350] Epoch: [137]  [100/195]  eta: 0:00:07  lr: 0.000166  loss: 0.1350 (0.1352)  time: 0.0715  data: 0.0002  max mem: 2598
[14:23:38.350535] Epoch: [137]  [120/195]  eta: 0:00:05  lr: 0.000165  loss: 0.1379 (0.1356)  time: 0.0704  data: 0.0001  max mem: 2598
[14:23:39.742118] Epoch: [137]  [140/195]  eta: 0:00:04  lr: 0.000165  loss: 0.1373 (0.1358)  time: 0.0695  data: 0.0001  max mem: 2598
[14:23:41.132473] Epoch: [137]  [160/195]  eta: 0:00:02  lr: 0.000164  loss: 0.1340 (0.1357)  time: 0.0695  data: 0.0001  max mem: 2598
[14:23:42.537824] Epoch: [137]  [180/195]  eta: 0:00:01  lr: 0.000164  loss: 0.1348 (0.1355)  time: 0.0702  data: 0.0001  max mem: 2598
[14:23:43.503153] Epoch: [137]  [194/195]  eta: 0:00:00  lr: 0.000163  loss: 0.1352 (0.1355)  time: 0.0691  data: 0.0001  max mem: 2598
[14:23:43.552139] Epoch: [137] Total time: 0:00:14 (0.0729 s / it)
[14:23:43.552306] Averaged stats: lr: 0.000163  loss: 0.1352 (0.1355)
[14:23:43.554701] log_dir: ./output_dir/mae0.3
[14:23:43.937941] Epoch: [138]  [  0/195]  eta: 0:01:14  lr: 0.000163  loss: 0.1371 (0.1371)  time: 0.3820  data: 0.2994  max mem: 2598
[14:23:45.334603] Epoch: [138]  [ 20/195]  eta: 0:00:14  lr: 0.000163  loss: 0.1336 (0.1346)  time: 0.0698  data: 0.0001  max mem: 2598
[14:23:46.722995] Epoch: [138]  [ 40/195]  eta: 0:00:11  lr: 0.000163  loss: 0.1320 (0.1339)  time: 0.0694  data: 0.0001  max mem: 2598
[14:23:48.115122] Epoch: [138]  [ 60/195]  eta: 0:00:10  lr: 0.000162  loss: 0.1357 (0.1346)  time: 0.0696  data: 0.0001  max mem: 2598
[14:23:49.508020] Epoch: [138]  [ 80/195]  eta: 0:00:08  lr: 0.000162  loss: 0.1315 (0.1339)  time: 0.0696  data: 0.0001  max mem: 2598
[14:23:50.904758] Epoch: [138]  [100/195]  eta: 0:00:06  lr: 0.000161  loss: 0.1357 (0.1343)  time: 0.0698  data: 0.0001  max mem: 2598
[14:23:52.300095] Epoch: [138]  [120/195]  eta: 0:00:05  lr: 0.000161  loss: 0.1339 (0.1343)  time: 0.0697  data: 0.0001  max mem: 2598
[14:23:53.689946] Epoch: [138]  [140/195]  eta: 0:00:03  lr: 0.000160  loss: 0.1349 (0.1344)  time: 0.0695  data: 0.0001  max mem: 2598
[14:23:55.081785] Epoch: [138]  [160/195]  eta: 0:00:02  lr: 0.000160  loss: 0.1338 (0.1344)  time: 0.0696  data: 0.0001  max mem: 2598
[14:23:56.466045] Epoch: [138]  [180/195]  eta: 0:00:01  lr: 0.000159  loss: 0.1335 (0.1344)  time: 0.0692  data: 0.0001  max mem: 2598
[14:23:57.417752] Epoch: [138]  [194/195]  eta: 0:00:00  lr: 0.000159  loss: 0.1330 (0.1344)  time: 0.0681  data: 0.0001  max mem: 2598
[14:23:57.463915] Epoch: [138] Total time: 0:00:13 (0.0713 s / it)
[14:23:57.464723] Averaged stats: lr: 0.000159  loss: 0.1330 (0.1344)
[14:23:57.467533] log_dir: ./output_dir/mae0.3
[14:23:57.900838] Epoch: [139]  [  0/195]  eta: 0:01:24  lr: 0.000159  loss: 0.1322 (0.1322)  time: 0.4321  data: 0.3042  max mem: 2598
[14:23:59.338729] Epoch: [139]  [ 20/195]  eta: 0:00:15  lr: 0.000158  loss: 0.1344 (0.1333)  time: 0.0719  data: 0.0001  max mem: 2598
[14:24:00.712947] Epoch: [139]  [ 40/195]  eta: 0:00:12  lr: 0.000158  loss: 0.1326 (0.1334)  time: 0.0687  data: 0.0001  max mem: 2598
[14:24:02.091803] Epoch: [139]  [ 60/195]  eta: 0:00:10  lr: 0.000157  loss: 0.1357 (0.1343)  time: 0.0689  data: 0.0001  max mem: 2598
[14:24:03.474361] Epoch: [139]  [ 80/195]  eta: 0:00:08  lr: 0.000157  loss: 0.1364 (0.1344)  time: 0.0691  data: 0.0001  max mem: 2598
[14:24:04.845639] Epoch: [139]  [100/195]  eta: 0:00:06  lr: 0.000157  loss: 0.1360 (0.1346)  time: 0.0685  data: 0.0001  max mem: 2598
[14:24:06.241314] Epoch: [139]  [120/195]  eta: 0:00:05  lr: 0.000156  loss: 0.1346 (0.1346)  time: 0.0697  data: 0.0001  max mem: 2598
[14:24:07.617296] Epoch: [139]  [140/195]  eta: 0:00:03  lr: 0.000156  loss: 0.1355 (0.1349)  time: 0.0688  data: 0.0001  max mem: 2598
[14:24:08.995755] Epoch: [139]  [160/195]  eta: 0:00:02  lr: 0.000155  loss: 0.1347 (0.1350)  time: 0.0689  data: 0.0001  max mem: 2598
[14:24:10.369462] Epoch: [139]  [180/195]  eta: 0:00:01  lr: 0.000155  loss: 0.1354 (0.1352)  time: 0.0686  data: 0.0001  max mem: 2598
[14:24:11.321286] Epoch: [139]  [194/195]  eta: 0:00:00  lr: 0.000154  loss: 0.1352 (0.1352)  time: 0.0680  data: 0.0001  max mem: 2598
[14:24:11.370816] Epoch: [139] Total time: 0:00:13 (0.0713 s / it)
[14:24:11.370887] Averaged stats: lr: 0.000154  loss: 0.1352 (0.1352)
[14:24:11.373206] log_dir: ./output_dir/mae0.3
[14:24:11.803624] Epoch: [140]  [  0/195]  eta: 0:01:23  lr: 0.000154  loss: 0.1392 (0.1392)  time: 0.4288  data: 0.3019  max mem: 2598
[14:24:13.272382] Epoch: [140]  [ 20/195]  eta: 0:00:15  lr: 0.000154  loss: 0.1341 (0.1334)  time: 0.0734  data: 0.0001  max mem: 2598
[14:24:14.669743] Epoch: [140]  [ 40/195]  eta: 0:00:12  lr: 0.000153  loss: 0.1314 (0.1337)  time: 0.0698  data: 0.0001  max mem: 2598
[14:24:16.067907] Epoch: [140]  [ 60/195]  eta: 0:00:10  lr: 0.000153  loss: 0.1343 (0.1339)  time: 0.0699  data: 0.0001  max mem: 2598
[14:24:17.473596] Epoch: [140]  [ 80/195]  eta: 0:00:08  lr: 0.000152  loss: 0.1371 (0.1344)  time: 0.0702  data: 0.0001  max mem: 2598
[14:24:18.876530] Epoch: [140]  [100/195]  eta: 0:00:07  lr: 0.000152  loss: 0.1356 (0.1346)  time: 0.0701  data: 0.0001  max mem: 2598
[14:24:20.281951] Epoch: [140]  [120/195]  eta: 0:00:05  lr: 0.000152  loss: 0.1330 (0.1346)  time: 0.0702  data: 0.0002  max mem: 2598
[14:24:21.685171] Epoch: [140]  [140/195]  eta: 0:00:04  lr: 0.000151  loss: 0.1333 (0.1344)  time: 0.0701  data: 0.0001  max mem: 2598
[14:24:23.088799] Epoch: [140]  [160/195]  eta: 0:00:02  lr: 0.000151  loss: 0.1322 (0.1341)  time: 0.0701  data: 0.0002  max mem: 2598
[14:24:24.486724] Epoch: [140]  [180/195]  eta: 0:00:01  lr: 0.000150  loss: 0.1335 (0.1341)  time: 0.0699  data: 0.0002  max mem: 2598
[14:24:25.459989] Epoch: [140]  [194/195]  eta: 0:00:00  lr: 0.000150  loss: 0.1334 (0.1341)  time: 0.0694  data: 0.0001  max mem: 2598
[14:24:25.502234] Epoch: [140] Total time: 0:00:14 (0.0725 s / it)
[14:24:25.502307] Averaged stats: lr: 0.000150  loss: 0.1334 (0.1341)
[14:24:25.690890] log_dir: ./output_dir/mae0.3
[14:24:26.150615] Epoch: [141]  [  0/195]  eta: 0:01:29  lr: 0.000150  loss: 0.1328 (0.1328)  time: 0.4583  data: 0.3372  max mem: 2598
[14:24:27.585109] Epoch: [141]  [ 20/195]  eta: 0:00:15  lr: 0.000149  loss: 0.1333 (0.1332)  time: 0.0717  data: 0.0001  max mem: 2598
[14:24:28.979816] Epoch: [141]  [ 40/195]  eta: 0:00:12  lr: 0.000149  loss: 0.1311 (0.1333)  time: 0.0697  data: 0.0001  max mem: 2598
[14:24:30.375840] Epoch: [141]  [ 60/195]  eta: 0:00:10  lr: 0.000148  loss: 0.1335 (0.1333)  time: 0.0698  data: 0.0001  max mem: 2598
[14:24:31.785269] Epoch: [141]  [ 80/195]  eta: 0:00:08  lr: 0.000148  loss: 0.1329 (0.1334)  time: 0.0704  data: 0.0001  max mem: 2598
[14:24:33.188274] Epoch: [141]  [100/195]  eta: 0:00:07  lr: 0.000148  loss: 0.1345 (0.1338)  time: 0.0701  data: 0.0001  max mem: 2598
[14:24:34.601763] Epoch: [141]  [120/195]  eta: 0:00:05  lr: 0.000147  loss: 0.1346 (0.1340)  time: 0.0706  data: 0.0001  max mem: 2598
[14:24:36.010144] Epoch: [141]  [140/195]  eta: 0:00:04  lr: 0.000147  loss: 0.1353 (0.1341)  time: 0.0704  data: 0.0001  max mem: 2598
[14:24:37.423821] Epoch: [141]  [160/195]  eta: 0:00:02  lr: 0.000146  loss: 0.1369 (0.1345)  time: 0.0707  data: 0.0001  max mem: 2598
[14:24:38.827833] Epoch: [141]  [180/195]  eta: 0:00:01  lr: 0.000146  loss: 0.1315 (0.1342)  time: 0.0702  data: 0.0001  max mem: 2598
[14:24:39.797062] Epoch: [141]  [194/195]  eta: 0:00:00  lr: 0.000145  loss: 0.1337 (0.1343)  time: 0.0692  data: 0.0001  max mem: 2598
[14:24:39.841760] Epoch: [141] Total time: 0:00:14 (0.0726 s / it)
[14:24:39.842059] Averaged stats: lr: 0.000145  loss: 0.1337 (0.1343)
[14:24:39.844584] log_dir: ./output_dir/mae0.3
[14:24:40.251110] Epoch: [142]  [  0/195]  eta: 0:01:19  lr: 0.000145  loss: 0.1267 (0.1267)  time: 0.4054  data: 0.3108  max mem: 2598
[14:24:41.724562] Epoch: [142]  [ 20/195]  eta: 0:00:15  lr: 0.000145  loss: 0.1334 (0.1335)  time: 0.0736  data: 0.0002  max mem: 2598
[14:24:43.166915] Epoch: [142]  [ 40/195]  eta: 0:00:12  lr: 0.000144  loss: 0.1326 (0.1334)  time: 0.0721  data: 0.0002  max mem: 2598
[14:24:44.600327] Epoch: [142]  [ 60/195]  eta: 0:00:10  lr: 0.000144  loss: 0.1323 (0.1335)  time: 0.0716  data: 0.0001  max mem: 2598
[14:24:45.991531] Epoch: [142]  [ 80/195]  eta: 0:00:08  lr: 0.000144  loss: 0.1348 (0.1340)  time: 0.0695  data: 0.0001  max mem: 2598
[14:24:47.385809] Epoch: [142]  [100/195]  eta: 0:00:07  lr: 0.000143  loss: 0.1339 (0.1339)  time: 0.0697  data: 0.0001  max mem: 2598
[14:24:48.794866] Epoch: [142]  [120/195]  eta: 0:00:05  lr: 0.000143  loss: 0.1337 (0.1339)  time: 0.0704  data: 0.0002  max mem: 2598
[14:24:50.187149] Epoch: [142]  [140/195]  eta: 0:00:04  lr: 0.000142  loss: 0.1349 (0.1340)  time: 0.0696  data: 0.0001  max mem: 2598
[14:24:51.591642] Epoch: [142]  [160/195]  eta: 0:00:02  lr: 0.000142  loss: 0.1345 (0.1342)  time: 0.0702  data: 0.0001  max mem: 2598
[14:24:52.976419] Epoch: [142]  [180/195]  eta: 0:00:01  lr: 0.000141  loss: 0.1348 (0.1340)  time: 0.0692  data: 0.0001  max mem: 2598
[14:24:53.932000] Epoch: [142]  [194/195]  eta: 0:00:00  lr: 0.000141  loss: 0.1354 (0.1340)  time: 0.0685  data: 0.0001  max mem: 2598
[14:24:53.999525] Epoch: [142] Total time: 0:00:14 (0.0726 s / it)
[14:24:53.999593] Averaged stats: lr: 0.000141  loss: 0.1354 (0.1340)
[14:24:54.001863] log_dir: ./output_dir/mae0.3
[14:24:54.366222] Epoch: [143]  [  0/195]  eta: 0:01:10  lr: 0.000141  loss: 0.1317 (0.1317)  time: 0.3628  data: 0.2793  max mem: 2598
[14:24:55.811447] Epoch: [143]  [ 20/195]  eta: 0:00:15  lr: 0.000140  loss: 0.1343 (0.1339)  time: 0.0722  data: 0.0002  max mem: 2598
[14:24:57.197589] Epoch: [143]  [ 40/195]  eta: 0:00:12  lr: 0.000140  loss: 0.1318 (0.1333)  time: 0.0693  data: 0.0001  max mem: 2598
[14:24:58.571693] Epoch: [143]  [ 60/195]  eta: 0:00:10  lr: 0.000140  loss: 0.1338 (0.1340)  time: 0.0687  data: 0.0001  max mem: 2598
[14:24:59.946379] Epoch: [143]  [ 80/195]  eta: 0:00:08  lr: 0.000139  loss: 0.1331 (0.1338)  time: 0.0687  data: 0.0001  max mem: 2598
[14:25:01.320738] Epoch: [143]  [100/195]  eta: 0:00:06  lr: 0.000139  loss: 0.1319 (0.1336)  time: 0.0687  data: 0.0001  max mem: 2598
[14:25:02.693982] Epoch: [143]  [120/195]  eta: 0:00:05  lr: 0.000138  loss: 0.1351 (0.1339)  time: 0.0686  data: 0.0001  max mem: 2598
[14:25:04.071901] Epoch: [143]  [140/195]  eta: 0:00:03  lr: 0.000138  loss: 0.1340 (0.1340)  time: 0.0689  data: 0.0001  max mem: 2598
[14:25:05.446491] Epoch: [143]  [160/195]  eta: 0:00:02  lr: 0.000137  loss: 0.1329 (0.1340)  time: 0.0687  data: 0.0001  max mem: 2598
[14:25:06.826701] Epoch: [143]  [180/195]  eta: 0:00:01  lr: 0.000137  loss: 0.1329 (0.1339)  time: 0.0690  data: 0.0001  max mem: 2598
[14:25:07.778650] Epoch: [143]  [194/195]  eta: 0:00:00  lr: 0.000137  loss: 0.1328 (0.1337)  time: 0.0685  data: 0.0001  max mem: 2598
[14:25:07.828593] Epoch: [143] Total time: 0:00:13 (0.0709 s / it)
[14:25:07.828729] Averaged stats: lr: 0.000137  loss: 0.1328 (0.1337)
[14:25:07.831030] log_dir: ./output_dir/mae0.3
[14:25:08.258469] Epoch: [144]  [  0/195]  eta: 0:01:23  lr: 0.000137  loss: 0.1416 (0.1416)  time: 0.4258  data: 0.3034  max mem: 2598
[14:25:09.717285] Epoch: [144]  [ 20/195]  eta: 0:00:15  lr: 0.000136  loss: 0.1320 (0.1333)  time: 0.0729  data: 0.0002  max mem: 2598
[14:25:11.122050] Epoch: [144]  [ 40/195]  eta: 0:00:12  lr: 0.000136  loss: 0.1301 (0.1327)  time: 0.0702  data: 0.0001  max mem: 2598
[14:25:12.529572] Epoch: [144]  [ 60/195]  eta: 0:00:10  lr: 0.000135  loss: 0.1320 (0.1331)  time: 0.0703  data: 0.0001  max mem: 2598
[14:25:13.946465] Epoch: [144]  [ 80/195]  eta: 0:00:08  lr: 0.000135  loss: 0.1340 (0.1331)  time: 0.0708  data: 0.0002  max mem: 2598
[14:25:15.353105] Epoch: [144]  [100/195]  eta: 0:00:07  lr: 0.000134  loss: 0.1302 (0.1331)  time: 0.0703  data: 0.0001  max mem: 2598
[14:25:16.767538] Epoch: [144]  [120/195]  eta: 0:00:05  lr: 0.000134  loss: 0.1312 (0.1332)  time: 0.0707  data: 0.0001  max mem: 2598
[14:25:18.173305] Epoch: [144]  [140/195]  eta: 0:00:04  lr: 0.000133  loss: 0.1332 (0.1333)  time: 0.0702  data: 0.0001  max mem: 2598
[14:25:19.583117] Epoch: [144]  [160/195]  eta: 0:00:02  lr: 0.000133  loss: 0.1339 (0.1335)  time: 0.0705  data: 0.0001  max mem: 2598
[14:25:20.984193] Epoch: [144]  [180/195]  eta: 0:00:01  lr: 0.000132  loss: 0.1309 (0.1335)  time: 0.0700  data: 0.0001  max mem: 2598
[14:25:21.950649] Epoch: [144]  [194/195]  eta: 0:00:00  lr: 0.000132  loss: 0.1331 (0.1335)  time: 0.0691  data: 0.0001  max mem: 2598
[14:25:22.023412] Epoch: [144] Total time: 0:00:14 (0.0728 s / it)
[14:25:22.023507] Averaged stats: lr: 0.000132  loss: 0.1331 (0.1335)
[14:25:22.027068] log_dir: ./output_dir/mae0.3
[14:25:22.477085] Epoch: [145]  [  0/195]  eta: 0:01:27  lr: 0.000132  loss: 0.1358 (0.1358)  time: 0.4484  data: 0.3103  max mem: 2598
[14:25:23.971407] Epoch: [145]  [ 20/195]  eta: 0:00:16  lr: 0.000132  loss: 0.1330 (0.1334)  time: 0.0747  data: 0.0002  max mem: 2598
[14:25:25.406920] Epoch: [145]  [ 40/195]  eta: 0:00:12  lr: 0.000131  loss: 0.1298 (0.1321)  time: 0.0717  data: 0.0002  max mem: 2598
[14:25:26.816687] Epoch: [145]  [ 60/195]  eta: 0:00:10  lr: 0.000131  loss: 0.1300 (0.1317)  time: 0.0705  data: 0.0002  max mem: 2598
[14:25:28.221212] Epoch: [145]  [ 80/195]  eta: 0:00:08  lr: 0.000130  loss: 0.1333 (0.1323)  time: 0.0702  data: 0.0001  max mem: 2598
[14:25:29.637490] Epoch: [145]  [100/195]  eta: 0:00:07  lr: 0.000130  loss: 0.1308 (0.1320)  time: 0.0708  data: 0.0001  max mem: 2598
[14:25:31.099693] Epoch: [145]  [120/195]  eta: 0:00:05  lr: 0.000129  loss: 0.1342 (0.1324)  time: 0.0731  data: 0.0002  max mem: 2598
[14:25:32.530074] Epoch: [145]  [140/195]  eta: 0:00:04  lr: 0.000129  loss: 0.1324 (0.1325)  time: 0.0715  data: 0.0002  max mem: 2598
[14:25:33.955454] Epoch: [145]  [160/195]  eta: 0:00:02  lr: 0.000129  loss: 0.1341 (0.1328)  time: 0.0712  data: 0.0002  max mem: 2598
[14:25:35.368520] Epoch: [145]  [180/195]  eta: 0:00:01  lr: 0.000128  loss: 0.1347 (0.1329)  time: 0.0706  data: 0.0001  max mem: 2598
[14:25:36.337730] Epoch: [145]  [194/195]  eta: 0:00:00  lr: 0.000128  loss: 0.1331 (0.1329)  time: 0.0693  data: 0.0001  max mem: 2598
[14:25:36.398273] Epoch: [145] Total time: 0:00:14 (0.0737 s / it)
[14:25:36.398476] Averaged stats: lr: 0.000128  loss: 0.1331 (0.1329)
[14:25:36.401063] log_dir: ./output_dir/mae0.3
[14:25:36.778848] Epoch: [146]  [  0/195]  eta: 0:01:13  lr: 0.000128  loss: 0.1420 (0.1420)  time: 0.3767  data: 0.2956  max mem: 2598
[14:25:38.179242] Epoch: [146]  [ 20/195]  eta: 0:00:14  lr: 0.000127  loss: 0.1316 (0.1315)  time: 0.0700  data: 0.0001  max mem: 2598
[14:25:39.608668] Epoch: [146]  [ 40/195]  eta: 0:00:12  lr: 0.000127  loss: 0.1324 (0.1318)  time: 0.0714  data: 0.0001  max mem: 2598
[14:25:41.043763] Epoch: [146]  [ 60/195]  eta: 0:00:10  lr: 0.000127  loss: 0.1337 (0.1325)  time: 0.0717  data: 0.0001  max mem: 2598
[14:25:42.450737] Epoch: [146]  [ 80/195]  eta: 0:00:08  lr: 0.000126  loss: 0.1319 (0.1327)  time: 0.0703  data: 0.0002  max mem: 2598
[14:25:43.856203] Epoch: [146]  [100/195]  eta: 0:00:07  lr: 0.000126  loss: 0.1325 (0.1329)  time: 0.0702  data: 0.0001  max mem: 2598
[14:25:45.279285] Epoch: [146]  [120/195]  eta: 0:00:05  lr: 0.000125  loss: 0.1325 (0.1328)  time: 0.0711  data: 0.0002  max mem: 2598
[14:25:46.744178] Epoch: [146]  [140/195]  eta: 0:00:04  lr: 0.000125  loss: 0.1335 (0.1329)  time: 0.0732  data: 0.0002  max mem: 2598
[14:25:48.163263] Epoch: [146]  [160/195]  eta: 0:00:02  lr: 0.000124  loss: 0.1330 (0.1330)  time: 0.0709  data: 0.0001  max mem: 2598
[14:25:49.695559] Epoch: [146]  [180/195]  eta: 0:00:01  lr: 0.000124  loss: 0.1327 (0.1330)  time: 0.0766  data: 0.0001  max mem: 2598
[14:25:50.683607] Epoch: [146]  [194/195]  eta: 0:00:00  lr: 0.000124  loss: 0.1328 (0.1330)  time: 0.0703  data: 0.0001  max mem: 2598
[14:25:50.751209] Epoch: [146] Total time: 0:00:14 (0.0736 s / it)
[14:25:50.751302] Averaged stats: lr: 0.000124  loss: 0.1328 (0.1330)
[14:25:50.753762] log_dir: ./output_dir/mae0.3
[14:25:51.165202] Epoch: [147]  [  0/195]  eta: 0:01:20  lr: 0.000124  loss: 0.1317 (0.1317)  time: 0.4103  data: 0.3008  max mem: 2598
[14:25:52.599382] Epoch: [147]  [ 20/195]  eta: 0:00:15  lr: 0.000123  loss: 0.1323 (0.1319)  time: 0.0717  data: 0.0001  max mem: 2598
[14:25:54.003728] Epoch: [147]  [ 40/195]  eta: 0:00:12  lr: 0.000123  loss: 0.1288 (0.1314)  time: 0.0702  data: 0.0001  max mem: 2598
[14:25:55.419954] Epoch: [147]  [ 60/195]  eta: 0:00:10  lr: 0.000122  loss: 0.1317 (0.1318)  time: 0.0708  data: 0.0001  max mem: 2598
[14:25:56.824036] Epoch: [147]  [ 80/195]  eta: 0:00:08  lr: 0.000122  loss: 0.1349 (0.1327)  time: 0.0702  data: 0.0001  max mem: 2598
[14:25:58.251238] Epoch: [147]  [100/195]  eta: 0:00:07  lr: 0.000121  loss: 0.1345 (0.1331)  time: 0.0713  data: 0.0002  max mem: 2598
[14:25:59.671332] Epoch: [147]  [120/195]  eta: 0:00:05  lr: 0.000121  loss: 0.1305 (0.1329)  time: 0.0709  data: 0.0001  max mem: 2598
[14:26:01.102293] Epoch: [147]  [140/195]  eta: 0:00:04  lr: 0.000121  loss: 0.1338 (0.1330)  time: 0.0715  data: 0.0002  max mem: 2598
[14:26:02.546834] Epoch: [147]  [160/195]  eta: 0:00:02  lr: 0.000120  loss: 0.1334 (0.1331)  time: 0.0722  data: 0.0002  max mem: 2598
[14:26:03.970529] Epoch: [147]  [180/195]  eta: 0:00:01  lr: 0.000120  loss: 0.1325 (0.1332)  time: 0.0711  data: 0.0001  max mem: 2598
[14:26:04.939256] Epoch: [147]  [194/195]  eta: 0:00:00  lr: 0.000119  loss: 0.1294 (0.1330)  time: 0.0694  data: 0.0001  max mem: 2598
[14:26:04.988892] Epoch: [147] Total time: 0:00:14 (0.0730 s / it)
[14:26:04.988969] Averaged stats: lr: 0.000119  loss: 0.1294 (0.1330)
[14:26:04.991519] log_dir: ./output_dir/mae0.3
[14:26:05.362048] Epoch: [148]  [  0/195]  eta: 0:01:12  lr: 0.000119  loss: 0.1374 (0.1374)  time: 0.3696  data: 0.2907  max mem: 2598
[14:26:06.788382] Epoch: [148]  [ 20/195]  eta: 0:00:14  lr: 0.000119  loss: 0.1325 (0.1315)  time: 0.0713  data: 0.0002  max mem: 2598
[14:26:08.202731] Epoch: [148]  [ 40/195]  eta: 0:00:12  lr: 0.000119  loss: 0.1330 (0.1326)  time: 0.0707  data: 0.0002  max mem: 2598
[14:26:09.619133] Epoch: [148]  [ 60/195]  eta: 0:00:10  lr: 0.000118  loss: 0.1311 (0.1322)  time: 0.0708  data: 0.0001  max mem: 2598
[14:26:11.029539] Epoch: [148]  [ 80/195]  eta: 0:00:08  lr: 0.000118  loss: 0.1330 (0.1327)  time: 0.0705  data: 0.0001  max mem: 2598
[14:26:12.434886] Epoch: [148]  [100/195]  eta: 0:00:06  lr: 0.000117  loss: 0.1322 (0.1325)  time: 0.0702  data: 0.0001  max mem: 2598
[14:26:13.862468] Epoch: [148]  [120/195]  eta: 0:00:05  lr: 0.000117  loss: 0.1339 (0.1326)  time: 0.0713  data: 0.0001  max mem: 2598
[14:26:15.273837] Epoch: [148]  [140/195]  eta: 0:00:04  lr: 0.000116  loss: 0.1361 (0.1328)  time: 0.0705  data: 0.0002  max mem: 2598
[14:26:16.710644] Epoch: [148]  [160/195]  eta: 0:00:02  lr: 0.000116  loss: 0.1325 (0.1330)  time: 0.0718  data: 0.0002  max mem: 2598
[14:26:18.118702] Epoch: [148]  [180/195]  eta: 0:00:01  lr: 0.000116  loss: 0.1333 (0.1331)  time: 0.0704  data: 0.0001  max mem: 2598
[14:26:19.087602] Epoch: [148]  [194/195]  eta: 0:00:00  lr: 0.000115  loss: 0.1329 (0.1331)  time: 0.0693  data: 0.0001  max mem: 2598
[14:26:19.132817] Epoch: [148] Total time: 0:00:14 (0.0725 s / it)
[14:26:19.132885] Averaged stats: lr: 0.000115  loss: 0.1329 (0.1331)
[14:26:19.135364] log_dir: ./output_dir/mae0.3
[14:26:19.550934] Epoch: [149]  [  0/195]  eta: 0:01:20  lr: 0.000115  loss: 0.1351 (0.1351)  time: 0.4142  data: 0.3233  max mem: 2598
[14:26:21.019888] Epoch: [149]  [ 20/195]  eta: 0:00:15  lr: 0.000115  loss: 0.1292 (0.1318)  time: 0.0734  data: 0.0001  max mem: 2598
[14:26:22.457379] Epoch: [149]  [ 40/195]  eta: 0:00:12  lr: 0.000114  loss: 0.1291 (0.1310)  time: 0.0718  data: 0.0001  max mem: 2598
[14:26:23.934686] Epoch: [149]  [ 60/195]  eta: 0:00:10  lr: 0.000114  loss: 0.1328 (0.1314)  time: 0.0738  data: 0.0002  max mem: 2598
[14:26:25.428548] Epoch: [149]  [ 80/195]  eta: 0:00:08  lr: 0.000114  loss: 0.1328 (0.1323)  time: 0.0746  data: 0.0002  max mem: 2598
[14:26:26.911474] Epoch: [149]  [100/195]  eta: 0:00:07  lr: 0.000113  loss: 0.1285 (0.1319)  time: 0.0741  data: 0.0002  max mem: 2598
[14:26:28.307000] Epoch: [149]  [120/195]  eta: 0:00:05  lr: 0.000113  loss: 0.1330 (0.1323)  time: 0.0697  data: 0.0001  max mem: 2598
[14:26:29.707869] Epoch: [149]  [140/195]  eta: 0:00:04  lr: 0.000112  loss: 0.1321 (0.1322)  time: 0.0700  data: 0.0001  max mem: 2598
[14:26:31.105591] Epoch: [149]  [160/195]  eta: 0:00:02  lr: 0.000112  loss: 0.1307 (0.1323)  time: 0.0698  data: 0.0001  max mem: 2598
[14:26:32.500312] Epoch: [149]  [180/195]  eta: 0:00:01  lr: 0.000111  loss: 0.1307 (0.1324)  time: 0.0697  data: 0.0001  max mem: 2598
[14:26:33.454518] Epoch: [149]  [194/195]  eta: 0:00:00  lr: 0.000111  loss: 0.1328 (0.1326)  time: 0.0682  data: 0.0001  max mem: 2598
[14:26:33.513310] Epoch: [149] Total time: 0:00:14 (0.0737 s / it)
[14:26:33.513394] Averaged stats: lr: 0.000111  loss: 0.1328 (0.1326)
[14:26:33.516746] log_dir: ./output_dir/mae0.3
[14:26:33.923320] Epoch: [150]  [  0/195]  eta: 0:01:18  lr: 0.000111  loss: 0.1357 (0.1357)  time: 0.4050  data: 0.2971  max mem: 2598
[14:26:35.362648] Epoch: [150]  [ 20/195]  eta: 0:00:15  lr: 0.000111  loss: 0.1297 (0.1309)  time: 0.0719  data: 0.0002  max mem: 2598
[14:26:36.777589] Epoch: [150]  [ 40/195]  eta: 0:00:12  lr: 0.000110  loss: 0.1309 (0.1312)  time: 0.0707  data: 0.0001  max mem: 2598
[14:26:38.182109] Epoch: [150]  [ 60/195]  eta: 0:00:10  lr: 0.000110  loss: 0.1332 (0.1321)  time: 0.0702  data: 0.0001  max mem: 2598
[14:26:39.596911] Epoch: [150]  [ 80/195]  eta: 0:00:08  lr: 0.000109  loss: 0.1314 (0.1321)  time: 0.0707  data: 0.0001  max mem: 2598
[14:26:41.004761] Epoch: [150]  [100/195]  eta: 0:00:07  lr: 0.000109  loss: 0.1298 (0.1320)  time: 0.0703  data: 0.0001  max mem: 2598
[14:26:42.405151] Epoch: [150]  [120/195]  eta: 0:00:05  lr: 0.000109  loss: 0.1319 (0.1322)  time: 0.0700  data: 0.0001  max mem: 2598
[14:26:43.813040] Epoch: [150]  [140/195]  eta: 0:00:04  lr: 0.000108  loss: 0.1319 (0.1324)  time: 0.0704  data: 0.0001  max mem: 2598
[14:26:45.223956] Epoch: [150]  [160/195]  eta: 0:00:02  lr: 0.000108  loss: 0.1346 (0.1327)  time: 0.0705  data: 0.0001  max mem: 2598
[14:26:46.623504] Epoch: [150]  [180/195]  eta: 0:00:01  lr: 0.000107  loss: 0.1337 (0.1329)  time: 0.0699  data: 0.0001  max mem: 2598
[14:26:47.590744] Epoch: [150]  [194/195]  eta: 0:00:00  lr: 0.000107  loss: 0.1352 (0.1330)  time: 0.0691  data: 0.0001  max mem: 2598
[14:26:47.659122] Epoch: [150] Total time: 0:00:14 (0.0725 s / it)
[14:26:47.659191] Averaged stats: lr: 0.000107  loss: 0.1352 (0.1330)
[14:26:47.661655] log_dir: ./output_dir/mae0.3
[14:26:48.036257] Epoch: [151]  [  0/195]  eta: 0:01:12  lr: 0.000107  loss: 0.1321 (0.1321)  time: 0.3736  data: 0.2934  max mem: 2598
[14:26:49.470742] Epoch: [151]  [ 20/195]  eta: 0:00:15  lr: 0.000107  loss: 0.1302 (0.1315)  time: 0.0717  data: 0.0002  max mem: 2598
[14:26:50.917780] Epoch: [151]  [ 40/195]  eta: 0:00:12  lr: 0.000106  loss: 0.1308 (0.1317)  time: 0.0723  data: 0.0002  max mem: 2598
[14:26:52.336350] Epoch: [151]  [ 60/195]  eta: 0:00:10  lr: 0.000106  loss: 0.1313 (0.1319)  time: 0.0709  data: 0.0002  max mem: 2598
[14:26:53.760860] Epoch: [151]  [ 80/195]  eta: 0:00:08  lr: 0.000105  loss: 0.1350 (0.1327)  time: 0.0712  data: 0.0002  max mem: 2598
[14:26:55.197264] Epoch: [151]  [100/195]  eta: 0:00:07  lr: 0.000105  loss: 0.1318 (0.1323)  time: 0.0718  data: 0.0002  max mem: 2598
[14:26:56.648080] Epoch: [151]  [120/195]  eta: 0:00:05  lr: 0.000105  loss: 0.1311 (0.1324)  time: 0.0725  data: 0.0002  max mem: 2598
[14:26:58.100980] Epoch: [151]  [140/195]  eta: 0:00:04  lr: 0.000104  loss: 0.1326 (0.1324)  time: 0.0726  data: 0.0002  max mem: 2598
[14:26:59.543372] Epoch: [151]  [160/195]  eta: 0:00:02  lr: 0.000104  loss: 0.1289 (0.1323)  time: 0.0721  data: 0.0002  max mem: 2598
[14:27:01.009813] Epoch: [151]  [180/195]  eta: 0:00:01  lr: 0.000103  loss: 0.1310 (0.1323)  time: 0.0733  data: 0.0002  max mem: 2598
[14:27:02.002247] Epoch: [151]  [194/195]  eta: 0:00:00  lr: 0.000103  loss: 0.1326 (0.1323)  time: 0.0708  data: 0.0001  max mem: 2598
[14:27:02.066722] Epoch: [151] Total time: 0:00:14 (0.0739 s / it)
[14:27:02.066844] Averaged stats: lr: 0.000103  loss: 0.1326 (0.1323)
[14:27:02.071707] log_dir: ./output_dir/mae0.3
[14:27:02.520244] Epoch: [152]  [  0/195]  eta: 0:01:27  lr: 0.000103  loss: 0.1303 (0.1303)  time: 0.4468  data: 0.3636  max mem: 2598
[14:27:03.951299] Epoch: [152]  [ 20/195]  eta: 0:00:15  lr: 0.000103  loss: 0.1274 (0.1300)  time: 0.0715  data: 0.0002  max mem: 2598
[14:27:05.369361] Epoch: [152]  [ 40/195]  eta: 0:00:12  lr: 0.000102  loss: 0.1294 (0.1304)  time: 0.0709  data: 0.0002  max mem: 2598
[14:27:06.776622] Epoch: [152]  [ 60/195]  eta: 0:00:10  lr: 0.000102  loss: 0.1286 (0.1308)  time: 0.0703  data: 0.0002  max mem: 2598
[14:27:08.248781] Epoch: [152]  [ 80/195]  eta: 0:00:08  lr: 0.000101  loss: 0.1326 (0.1314)  time: 0.0736  data: 0.0002  max mem: 2598
[14:27:09.696820] Epoch: [152]  [100/195]  eta: 0:00:07  lr: 0.000101  loss: 0.1325 (0.1316)  time: 0.0724  data: 0.0002  max mem: 2598
[14:27:11.116016] Epoch: [152]  [120/195]  eta: 0:00:05  lr: 0.000101  loss: 0.1349 (0.1320)  time: 0.0709  data: 0.0002  max mem: 2598
[14:27:12.557771] Epoch: [152]  [140/195]  eta: 0:00:04  lr: 0.000100  loss: 0.1311 (0.1321)  time: 0.0720  data: 0.0002  max mem: 2598
[14:27:14.000783] Epoch: [152]  [160/195]  eta: 0:00:02  lr: 0.000100  loss: 0.1336 (0.1323)  time: 0.0721  data: 0.0002  max mem: 2598
[14:27:15.416319] Epoch: [152]  [180/195]  eta: 0:00:01  lr: 0.000099  loss: 0.1309 (0.1323)  time: 0.0707  data: 0.0002  max mem: 2598
[14:27:16.392822] Epoch: [152]  [194/195]  eta: 0:00:00  lr: 0.000099  loss: 0.1336 (0.1324)  time: 0.0698  data: 0.0001  max mem: 2598
[14:27:16.460387] Epoch: [152] Total time: 0:00:14 (0.0738 s / it)
[14:27:16.460517] Averaged stats: lr: 0.000099  loss: 0.1336 (0.1324)
[14:27:16.465462] log_dir: ./output_dir/mae0.3
[14:27:16.953349] Epoch: [153]  [  0/195]  eta: 0:01:34  lr: 0.000099  loss: 0.1420 (0.1420)  time: 0.4869  data: 0.3727  max mem: 2598
[14:27:18.388474] Epoch: [153]  [ 20/195]  eta: 0:00:16  lr: 0.000099  loss: 0.1305 (0.1310)  time: 0.0717  data: 0.0002  max mem: 2598
[14:27:19.821671] Epoch: [153]  [ 40/195]  eta: 0:00:12  lr: 0.000098  loss: 0.1289 (0.1311)  time: 0.0716  data: 0.0002  max mem: 2598
[14:27:21.206630] Epoch: [153]  [ 60/195]  eta: 0:00:10  lr: 0.000098  loss: 0.1305 (0.1310)  time: 0.0692  data: 0.0001  max mem: 2598
[14:27:22.643600] Epoch: [153]  [ 80/195]  eta: 0:00:08  lr: 0.000098  loss: 0.1304 (0.1313)  time: 0.0718  data: 0.0002  max mem: 2598
[14:27:24.067041] Epoch: [153]  [100/195]  eta: 0:00:07  lr: 0.000097  loss: 0.1295 (0.1311)  time: 0.0711  data: 0.0002  max mem: 2598
[14:27:25.502560] Epoch: [153]  [120/195]  eta: 0:00:05  lr: 0.000097  loss: 0.1311 (0.1312)  time: 0.0717  data: 0.0002  max mem: 2598
[14:27:26.936501] Epoch: [153]  [140/195]  eta: 0:00:04  lr: 0.000096  loss: 0.1318 (0.1313)  time: 0.0717  data: 0.0002  max mem: 2598
[14:27:28.376294] Epoch: [153]  [160/195]  eta: 0:00:02  lr: 0.000096  loss: 0.1336 (0.1316)  time: 0.0719  data: 0.0002  max mem: 2598
[14:27:29.820979] Epoch: [153]  [180/195]  eta: 0:00:01  lr: 0.000096  loss: 0.1289 (0.1316)  time: 0.0722  data: 0.0002  max mem: 2598
[14:27:30.807146] Epoch: [153]  [194/195]  eta: 0:00:00  lr: 0.000095  loss: 0.1310 (0.1316)  time: 0.0703  data: 0.0001  max mem: 2598
[14:27:30.879401] Epoch: [153] Total time: 0:00:14 (0.0739 s / it)
[14:27:30.879520] Averaged stats: lr: 0.000095  loss: 0.1310 (0.1316)
[14:27:30.884961] log_dir: ./output_dir/mae0.3
[14:27:31.375505] Epoch: [154]  [  0/195]  eta: 0:01:35  lr: 0.000095  loss: 0.1337 (0.1337)  time: 0.4889  data: 0.3659  max mem: 2598
[14:27:32.824901] Epoch: [154]  [ 20/195]  eta: 0:00:16  lr: 0.000095  loss: 0.1305 (0.1310)  time: 0.0724  data: 0.0002  max mem: 2598
[14:27:34.243160] Epoch: [154]  [ 40/195]  eta: 0:00:12  lr: 0.000094  loss: 0.1313 (0.1311)  time: 0.0709  data: 0.0002  max mem: 2598
[14:27:35.646088] Epoch: [154]  [ 60/195]  eta: 0:00:10  lr: 0.000094  loss: 0.1307 (0.1311)  time: 0.0701  data: 0.0001  max mem: 2598
[14:27:37.055270] Epoch: [154]  [ 80/195]  eta: 0:00:08  lr: 0.000094  loss: 0.1329 (0.1317)  time: 0.0704  data: 0.0001  max mem: 2598
[14:27:38.451860] Epoch: [154]  [100/195]  eta: 0:00:07  lr: 0.000093  loss: 0.1296 (0.1315)  time: 0.0698  data: 0.0001  max mem: 2598
[14:27:39.857055] Epoch: [154]  [120/195]  eta: 0:00:05  lr: 0.000093  loss: 0.1280 (0.1312)  time: 0.0702  data: 0.0001  max mem: 2598
[14:27:41.265104] Epoch: [154]  [140/195]  eta: 0:00:04  lr: 0.000092  loss: 0.1307 (0.1312)  time: 0.0704  data: 0.0001  max mem: 2598
[14:27:42.671827] Epoch: [154]  [160/195]  eta: 0:00:02  lr: 0.000092  loss: 0.1300 (0.1312)  time: 0.0703  data: 0.0001  max mem: 2598
[14:27:44.080945] Epoch: [154]  [180/195]  eta: 0:00:01  lr: 0.000092  loss: 0.1309 (0.1313)  time: 0.0704  data: 0.0002  max mem: 2598
[14:27:45.042596] Epoch: [154]  [194/195]  eta: 0:00:00  lr: 0.000091  loss: 0.1308 (0.1314)  time: 0.0690  data: 0.0001  max mem: 2598
[14:27:45.133673] Epoch: [154] Total time: 0:00:14 (0.0731 s / it)
[14:27:45.133797] Averaged stats: lr: 0.000091  loss: 0.1308 (0.1314)
[14:27:45.138550] log_dir: ./output_dir/mae0.3
[14:27:45.639122] Epoch: [155]  [  0/195]  eta: 0:01:37  lr: 0.000091  loss: 0.1294 (0.1294)  time: 0.4990  data: 0.3640  max mem: 2598
[14:27:47.117604] Epoch: [155]  [ 20/195]  eta: 0:00:16  lr: 0.000091  loss: 0.1290 (0.1311)  time: 0.0739  data: 0.0002  max mem: 2598
[14:27:48.553380] Epoch: [155]  [ 40/195]  eta: 0:00:12  lr: 0.000091  loss: 0.1293 (0.1303)  time: 0.0717  data: 0.0002  max mem: 2598
[14:27:49.989730] Epoch: [155]  [ 60/195]  eta: 0:00:10  lr: 0.000090  loss: 0.1324 (0.1308)  time: 0.0717  data: 0.0002  max mem: 2598
[14:27:51.408196] Epoch: [155]  [ 80/195]  eta: 0:00:08  lr: 0.000090  loss: 0.1332 (0.1311)  time: 0.0709  data: 0.0002  max mem: 2598
[14:27:52.829300] Epoch: [155]  [100/195]  eta: 0:00:07  lr: 0.000089  loss: 0.1319 (0.1315)  time: 0.0710  data: 0.0002  max mem: 2598
[14:27:54.242279] Epoch: [155]  [120/195]  eta: 0:00:05  lr: 0.000089  loss: 0.1322 (0.1315)  time: 0.0706  data: 0.0002  max mem: 2598
[14:27:55.646036] Epoch: [155]  [140/195]  eta: 0:00:04  lr: 0.000089  loss: 0.1320 (0.1316)  time: 0.0701  data: 0.0001  max mem: 2598
[14:27:57.060380] Epoch: [155]  [160/195]  eta: 0:00:02  lr: 0.000088  loss: 0.1315 (0.1317)  time: 0.0707  data: 0.0002  max mem: 2598
[14:27:58.458990] Epoch: [155]  [180/195]  eta: 0:00:01  lr: 0.000088  loss: 0.1288 (0.1315)  time: 0.0699  data: 0.0001  max mem: 2598
[14:27:59.427140] Epoch: [155]  [194/195]  eta: 0:00:00  lr: 0.000088  loss: 0.1292 (0.1316)  time: 0.0692  data: 0.0001  max mem: 2598
[14:27:59.493962] Epoch: [155] Total time: 0:00:14 (0.0736 s / it)
[14:27:59.494032] Averaged stats: lr: 0.000088  loss: 0.1292 (0.1316)
[14:27:59.496586] log_dir: ./output_dir/mae0.3
[14:27:59.894830] Epoch: [156]  [  0/195]  eta: 0:01:17  lr: 0.000088  loss: 0.1327 (0.1327)  time: 0.3971  data: 0.3016  max mem: 2598
[14:28:01.357327] Epoch: [156]  [ 20/195]  eta: 0:00:15  lr: 0.000087  loss: 0.1292 (0.1303)  time: 0.0731  data: 0.0001  max mem: 2598
[14:28:02.790190] Epoch: [156]  [ 40/195]  eta: 0:00:12  lr: 0.000087  loss: 0.1316 (0.1310)  time: 0.0716  data: 0.0002  max mem: 2598
[14:28:04.198970] Epoch: [156]  [ 60/195]  eta: 0:00:10  lr: 0.000086  loss: 0.1266 (0.1302)  time: 0.0704  data: 0.0001  max mem: 2598
[14:28:05.590013] Epoch: [156]  [ 80/195]  eta: 0:00:08  lr: 0.000086  loss: 0.1301 (0.1310)  time: 0.0695  data: 0.0001  max mem: 2598
[14:28:06.956958] Epoch: [156]  [100/195]  eta: 0:00:07  lr: 0.000086  loss: 0.1305 (0.1311)  time: 0.0683  data: 0.0001  max mem: 2598
[14:28:08.358242] Epoch: [156]  [120/195]  eta: 0:00:05  lr: 0.000085  loss: 0.1312 (0.1313)  time: 0.0700  data: 0.0002  max mem: 2598
[14:28:09.759558] Epoch: [156]  [140/195]  eta: 0:00:04  lr: 0.000085  loss: 0.1305 (0.1312)  time: 0.0700  data: 0.0002  max mem: 2598
[14:28:11.169636] Epoch: [156]  [160/195]  eta: 0:00:02  lr: 0.000085  loss: 0.1306 (0.1312)  time: 0.0705  data: 0.0002  max mem: 2598
[14:28:12.538142] Epoch: [156]  [180/195]  eta: 0:00:01  lr: 0.000084  loss: 0.1316 (0.1314)  time: 0.0684  data: 0.0001  max mem: 2598
[14:28:13.489800] Epoch: [156]  [194/195]  eta: 0:00:00  lr: 0.000084  loss: 0.1316 (0.1312)  time: 0.0680  data: 0.0001  max mem: 2598
[14:28:13.538006] Epoch: [156] Total time: 0:00:14 (0.0720 s / it)
[14:28:13.538069] Averaged stats: lr: 0.000084  loss: 0.1316 (0.1312)
[14:28:13.540372] log_dir: ./output_dir/mae0.3
[14:28:13.933315] Epoch: [157]  [  0/195]  eta: 0:01:16  lr: 0.000084  loss: 0.1372 (0.1372)  time: 0.3917  data: 0.2921  max mem: 2598
[14:28:15.373586] Epoch: [157]  [ 20/195]  eta: 0:00:15  lr: 0.000084  loss: 0.1280 (0.1303)  time: 0.0720  data: 0.0001  max mem: 2598
[14:28:16.778712] Epoch: [157]  [ 40/195]  eta: 0:00:12  lr: 0.000083  loss: 0.1294 (0.1302)  time: 0.0702  data: 0.0001  max mem: 2598
[14:28:18.186492] Epoch: [157]  [ 60/195]  eta: 0:00:10  lr: 0.000083  loss: 0.1316 (0.1306)  time: 0.0703  data: 0.0001  max mem: 2598
[14:28:19.607853] Epoch: [157]  [ 80/195]  eta: 0:00:08  lr: 0.000082  loss: 0.1318 (0.1312)  time: 0.0710  data: 0.0001  max mem: 2598
[14:28:21.019871] Epoch: [157]  [100/195]  eta: 0:00:07  lr: 0.000082  loss: 0.1314 (0.1311)  time: 0.0706  data: 0.0001  max mem: 2598
[14:28:22.428265] Epoch: [157]  [120/195]  eta: 0:00:05  lr: 0.000082  loss: 0.1298 (0.1312)  time: 0.0704  data: 0.0002  max mem: 2598
[14:28:23.846155] Epoch: [157]  [140/195]  eta: 0:00:04  lr: 0.000081  loss: 0.1276 (0.1310)  time: 0.0708  data: 0.0001  max mem: 2598
[14:28:25.268333] Epoch: [157]  [160/195]  eta: 0:00:02  lr: 0.000081  loss: 0.1304 (0.1310)  time: 0.0711  data: 0.0002  max mem: 2598
[14:28:26.671661] Epoch: [157]  [180/195]  eta: 0:00:01  lr: 0.000081  loss: 0.1307 (0.1310)  time: 0.0701  data: 0.0002  max mem: 2598
[14:28:27.638133] Epoch: [157]  [194/195]  eta: 0:00:00  lr: 0.000080  loss: 0.1294 (0.1311)  time: 0.0691  data: 0.0001  max mem: 2598
[14:28:27.687791] Epoch: [157] Total time: 0:00:14 (0.0726 s / it)
[14:28:27.687859] Averaged stats: lr: 0.000080  loss: 0.1294 (0.1311)
[14:28:27.690340] log_dir: ./output_dir/mae0.3
[14:28:28.120080] Epoch: [158]  [  0/195]  eta: 0:01:23  lr: 0.000080  loss: 0.1271 (0.1271)  time: 0.4287  data: 0.3309  max mem: 2598
[14:28:29.566841] Epoch: [158]  [ 20/195]  eta: 0:00:15  lr: 0.000080  loss: 0.1294 (0.1294)  time: 0.0723  data: 0.0001  max mem: 2598
[14:28:31.000802] Epoch: [158]  [ 40/195]  eta: 0:00:12  lr: 0.000080  loss: 0.1299 (0.1297)  time: 0.0717  data: 0.0001  max mem: 2598
[14:28:32.378957] Epoch: [158]  [ 60/195]  eta: 0:00:10  lr: 0.000079  loss: 0.1307 (0.1303)  time: 0.0689  data: 0.0001  max mem: 2598
[14:28:33.818746] Epoch: [158]  [ 80/195]  eta: 0:00:08  lr: 0.000079  loss: 0.1293 (0.1305)  time: 0.0719  data: 0.0002  max mem: 2598
[14:28:35.228190] Epoch: [158]  [100/195]  eta: 0:00:07  lr: 0.000078  loss: 0.1278 (0.1303)  time: 0.0704  data: 0.0001  max mem: 2598
[14:28:36.627127] Epoch: [158]  [120/195]  eta: 0:00:05  lr: 0.000078  loss: 0.1300 (0.1306)  time: 0.0699  data: 0.0001  max mem: 2598
[14:28:38.018478] Epoch: [158]  [140/195]  eta: 0:00:04  lr: 0.000078  loss: 0.1298 (0.1306)  time: 0.0695  data: 0.0001  max mem: 2598
[14:28:39.438487] Epoch: [158]  [160/195]  eta: 0:00:02  lr: 0.000077  loss: 0.1312 (0.1308)  time: 0.0710  data: 0.0001  max mem: 2598
[14:28:40.821665] Epoch: [158]  [180/195]  eta: 0:00:01  lr: 0.000077  loss: 0.1328 (0.1310)  time: 0.0691  data: 0.0001  max mem: 2598
[14:28:41.773568] Epoch: [158]  [194/195]  eta: 0:00:00  lr: 0.000077  loss: 0.1326 (0.1311)  time: 0.0680  data: 0.0001  max mem: 2598
[14:28:41.820249] Epoch: [158] Total time: 0:00:14 (0.0725 s / it)
[14:28:41.820317] Averaged stats: lr: 0.000077  loss: 0.1326 (0.1311)
[14:28:41.822611] log_dir: ./output_dir/mae0.3
[14:28:42.241291] Epoch: [159]  [  0/195]  eta: 0:01:21  lr: 0.000077  loss: 0.1368 (0.1368)  time: 0.4172  data: 0.2956  max mem: 2598
[14:28:43.658291] Epoch: [159]  [ 20/195]  eta: 0:00:15  lr: 0.000076  loss: 0.1280 (0.1302)  time: 0.0708  data: 0.0001  max mem: 2598
[14:28:45.035301] Epoch: [159]  [ 40/195]  eta: 0:00:12  lr: 0.000076  loss: 0.1309 (0.1308)  time: 0.0688  data: 0.0001  max mem: 2598
[14:28:46.410253] Epoch: [159]  [ 60/195]  eta: 0:00:10  lr: 0.000076  loss: 0.1321 (0.1313)  time: 0.0687  data: 0.0001  max mem: 2598
[14:28:47.785770] Epoch: [159]  [ 80/195]  eta: 0:00:08  lr: 0.000075  loss: 0.1287 (0.1311)  time: 0.0687  data: 0.0001  max mem: 2598
[14:28:49.163082] Epoch: [159]  [100/195]  eta: 0:00:06  lr: 0.000075  loss: 0.1313 (0.1310)  time: 0.0688  data: 0.0001  max mem: 2598
[14:28:50.534552] Epoch: [159]  [120/195]  eta: 0:00:05  lr: 0.000075  loss: 0.1305 (0.1310)  time: 0.0685  data: 0.0001  max mem: 2598
[14:28:51.905735] Epoch: [159]  [140/195]  eta: 0:00:03  lr: 0.000074  loss: 0.1287 (0.1308)  time: 0.0685  data: 0.0001  max mem: 2598
[14:28:53.278696] Epoch: [159]  [160/195]  eta: 0:00:02  lr: 0.000074  loss: 0.1327 (0.1308)  time: 0.0686  data: 0.0001  max mem: 2598
[14:28:54.644238] Epoch: [159]  [180/195]  eta: 0:00:01  lr: 0.000073  loss: 0.1310 (0.1309)  time: 0.0682  data: 0.0001  max mem: 2598
[14:28:55.597603] Epoch: [159]  [194/195]  eta: 0:00:00  lr: 0.000073  loss: 0.1310 (0.1310)  time: 0.0680  data: 0.0001  max mem: 2598
[14:28:55.665866] Epoch: [159] Total time: 0:00:13 (0.0710 s / it)
[14:28:55.665957] Averaged stats: lr: 0.000073  loss: 0.1310 (0.1310)
[14:28:55.668494] log_dir: ./output_dir/mae0.3
[14:28:56.085875] Epoch: [160]  [  0/195]  eta: 0:01:21  lr: 0.000073  loss: 0.1320 (0.1320)  time: 0.4159  data: 0.3149  max mem: 2598
[14:28:57.521956] Epoch: [160]  [ 20/195]  eta: 0:00:15  lr: 0.000073  loss: 0.1285 (0.1292)  time: 0.0718  data: 0.0001  max mem: 2598
[14:28:58.906388] Epoch: [160]  [ 40/195]  eta: 0:00:12  lr: 0.000073  loss: 0.1311 (0.1306)  time: 0.0692  data: 0.0001  max mem: 2598
[14:29:00.297491] Epoch: [160]  [ 60/195]  eta: 0:00:10  lr: 0.000072  loss: 0.1307 (0.1310)  time: 0.0695  data: 0.0001  max mem: 2598
[14:29:01.684538] Epoch: [160]  [ 80/195]  eta: 0:00:08  lr: 0.000072  loss: 0.1303 (0.1312)  time: 0.0693  data: 0.0001  max mem: 2598
[14:29:03.081358] Epoch: [160]  [100/195]  eta: 0:00:06  lr: 0.000071  loss: 0.1274 (0.1306)  time: 0.0698  data: 0.0001  max mem: 2598
[14:29:04.478379] Epoch: [160]  [120/195]  eta: 0:00:05  lr: 0.000071  loss: 0.1302 (0.1307)  time: 0.0698  data: 0.0001  max mem: 2598
[14:29:05.872084] Epoch: [160]  [140/195]  eta: 0:00:03  lr: 0.000071  loss: 0.1311 (0.1309)  time: 0.0696  data: 0.0001  max mem: 2598
[14:29:07.269158] Epoch: [160]  [160/195]  eta: 0:00:02  lr: 0.000070  loss: 0.1304 (0.1309)  time: 0.0698  data: 0.0001  max mem: 2598
[14:29:08.652694] Epoch: [160]  [180/195]  eta: 0:00:01  lr: 0.000070  loss: 0.1288 (0.1307)  time: 0.0691  data: 0.0001  max mem: 2598
[14:29:09.605681] Epoch: [160]  [194/195]  eta: 0:00:00  lr: 0.000070  loss: 0.1313 (0.1307)  time: 0.0680  data: 0.0001  max mem: 2598
[14:29:09.677014] Epoch: [160] Total time: 0:00:14 (0.0718 s / it)
[14:29:09.677080] Averaged stats: lr: 0.000070  loss: 0.1313 (0.1307)
[14:29:09.830088] log_dir: ./output_dir/mae0.3
[14:29:10.256083] Epoch: [161]  [  0/195]  eta: 0:01:22  lr: 0.000070  loss: 0.1340 (0.1340)  time: 0.4244  data: 0.3089  max mem: 2598
[14:29:11.724495] Epoch: [161]  [ 20/195]  eta: 0:00:15  lr: 0.000069  loss: 0.1305 (0.1309)  time: 0.0734  data: 0.0002  max mem: 2598
[14:29:13.158934] Epoch: [161]  [ 40/195]  eta: 0:00:12  lr: 0.000069  loss: 0.1292 (0.1301)  time: 0.0717  data: 0.0002  max mem: 2598
[14:29:14.560276] Epoch: [161]  [ 60/195]  eta: 0:00:10  lr: 0.000069  loss: 0.1318 (0.1306)  time: 0.0700  data: 0.0001  max mem: 2598
[14:29:15.977116] Epoch: [161]  [ 80/195]  eta: 0:00:08  lr: 0.000068  loss: 0.1296 (0.1310)  time: 0.0708  data: 0.0001  max mem: 2598
[14:29:17.412160] Epoch: [161]  [100/195]  eta: 0:00:07  lr: 0.000068  loss: 0.1321 (0.1312)  time: 0.0717  data: 0.0002  max mem: 2598
[14:29:18.804519] Epoch: [161]  [120/195]  eta: 0:00:05  lr: 0.000068  loss: 0.1292 (0.1311)  time: 0.0696  data: 0.0001  max mem: 2598
[14:29:20.206538] Epoch: [161]  [140/195]  eta: 0:00:04  lr: 0.000067  loss: 0.1301 (0.1311)  time: 0.0701  data: 0.0001  max mem: 2598
[14:29:21.607021] Epoch: [161]  [160/195]  eta: 0:00:02  lr: 0.000067  loss: 0.1310 (0.1310)  time: 0.0700  data: 0.0001  max mem: 2598
[14:29:23.013995] Epoch: [161]  [180/195]  eta: 0:00:01  lr: 0.000067  loss: 0.1307 (0.1309)  time: 0.0703  data: 0.0001  max mem: 2598
[14:29:23.981409] Epoch: [161]  [194/195]  eta: 0:00:00  lr: 0.000066  loss: 0.1285 (0.1309)  time: 0.0692  data: 0.0001  max mem: 2598
[14:29:24.031038] Epoch: [161] Total time: 0:00:14 (0.0728 s / it)
[14:29:24.031184] Averaged stats: lr: 0.000066  loss: 0.1285 (0.1309)
[14:29:24.033403] log_dir: ./output_dir/mae0.3
[14:29:24.469155] Epoch: [162]  [  0/195]  eta: 0:01:24  lr: 0.000066  loss: 0.1310 (0.1310)  time: 0.4342  data: 0.3171  max mem: 2598
[14:29:25.917546] Epoch: [162]  [ 20/195]  eta: 0:00:15  lr: 0.000066  loss: 0.1292 (0.1295)  time: 0.0724  data: 0.0001  max mem: 2598
[14:29:27.352461] Epoch: [162]  [ 40/195]  eta: 0:00:12  lr: 0.000066  loss: 0.1293 (0.1298)  time: 0.0717  data: 0.0002  max mem: 2598
[14:29:28.771615] Epoch: [162]  [ 60/195]  eta: 0:00:10  lr: 0.000065  loss: 0.1309 (0.1298)  time: 0.0709  data: 0.0001  max mem: 2598
[14:29:30.193155] Epoch: [162]  [ 80/195]  eta: 0:00:08  lr: 0.000065  loss: 0.1317 (0.1302)  time: 0.0710  data: 0.0001  max mem: 2598
[14:29:31.612013] Epoch: [162]  [100/195]  eta: 0:00:07  lr: 0.000065  loss: 0.1289 (0.1301)  time: 0.0709  data: 0.0001  max mem: 2598
[14:29:33.017638] Epoch: [162]  [120/195]  eta: 0:00:05  lr: 0.000064  loss: 0.1323 (0.1306)  time: 0.0702  data: 0.0001  max mem: 2598
[14:29:34.421717] Epoch: [162]  [140/195]  eta: 0:00:04  lr: 0.000064  loss: 0.1290 (0.1305)  time: 0.0702  data: 0.0002  max mem: 2598
[14:29:35.833957] Epoch: [162]  [160/195]  eta: 0:00:02  lr: 0.000064  loss: 0.1315 (0.1306)  time: 0.0706  data: 0.0001  max mem: 2598
[14:29:37.234616] Epoch: [162]  [180/195]  eta: 0:00:01  lr: 0.000063  loss: 0.1289 (0.1304)  time: 0.0700  data: 0.0001  max mem: 2598
[14:29:38.216465] Epoch: [162]  [194/195]  eta: 0:00:00  lr: 0.000063  loss: 0.1309 (0.1306)  time: 0.0698  data: 0.0001  max mem: 2598
[14:29:38.263836] Epoch: [162] Total time: 0:00:14 (0.0730 s / it)
[14:29:38.263908] Averaged stats: lr: 0.000063  loss: 0.1309 (0.1306)
[14:29:38.266439] log_dir: ./output_dir/mae0.3
[14:29:38.685513] Epoch: [163]  [  0/195]  eta: 0:01:21  lr: 0.000063  loss: 0.1268 (0.1268)  time: 0.4175  data: 0.3176  max mem: 2598
[14:29:40.125539] Epoch: [163]  [ 20/195]  eta: 0:00:15  lr: 0.000063  loss: 0.1301 (0.1298)  time: 0.0720  data: 0.0002  max mem: 2598
[14:29:41.546359] Epoch: [163]  [ 40/195]  eta: 0:00:12  lr: 0.000062  loss: 0.1294 (0.1295)  time: 0.0710  data: 0.0001  max mem: 2598
[14:29:42.953052] Epoch: [163]  [ 60/195]  eta: 0:00:10  lr: 0.000062  loss: 0.1294 (0.1298)  time: 0.0703  data: 0.0001  max mem: 2598
[14:29:44.372273] Epoch: [163]  [ 80/195]  eta: 0:00:08  lr: 0.000062  loss: 0.1316 (0.1301)  time: 0.0709  data: 0.0001  max mem: 2598
[14:29:45.778675] Epoch: [163]  [100/195]  eta: 0:00:07  lr: 0.000061  loss: 0.1292 (0.1298)  time: 0.0703  data: 0.0001  max mem: 2598
[14:29:47.188594] Epoch: [163]  [120/195]  eta: 0:00:05  lr: 0.000061  loss: 0.1311 (0.1301)  time: 0.0705  data: 0.0001  max mem: 2598
[14:29:48.600670] Epoch: [163]  [140/195]  eta: 0:00:04  lr: 0.000061  loss: 0.1282 (0.1300)  time: 0.0706  data: 0.0001  max mem: 2598
[14:29:50.007012] Epoch: [163]  [160/195]  eta: 0:00:02  lr: 0.000060  loss: 0.1303 (0.1301)  time: 0.0703  data: 0.0001  max mem: 2598
[14:29:51.404776] Epoch: [163]  [180/195]  eta: 0:00:01  lr: 0.000060  loss: 0.1320 (0.1303)  time: 0.0699  data: 0.0001  max mem: 2598
[14:29:52.370967] Epoch: [163]  [194/195]  eta: 0:00:00  lr: 0.000060  loss: 0.1295 (0.1302)  time: 0.0691  data: 0.0001  max mem: 2598
[14:29:52.439922] Epoch: [163] Total time: 0:00:14 (0.0727 s / it)
[14:29:52.439994] Averaged stats: lr: 0.000060  loss: 0.1295 (0.1302)
[14:29:52.442805] log_dir: ./output_dir/mae0.3
[14:29:52.860472] Epoch: [164]  [  0/195]  eta: 0:01:21  lr: 0.000060  loss: 0.1248 (0.1248)  time: 0.4164  data: 0.2931  max mem: 2598
[14:29:54.346422] Epoch: [164]  [ 20/195]  eta: 0:00:15  lr: 0.000060  loss: 0.1266 (0.1268)  time: 0.0742  data: 0.0002  max mem: 2598
[14:29:55.772147] Epoch: [164]  [ 40/195]  eta: 0:00:12  lr: 0.000059  loss: 0.1290 (0.1287)  time: 0.0712  data: 0.0001  max mem: 2598
[14:29:57.179935] Epoch: [164]  [ 60/195]  eta: 0:00:10  lr: 0.000059  loss: 0.1287 (0.1289)  time: 0.0703  data: 0.0001  max mem: 2598
[14:29:58.587572] Epoch: [164]  [ 80/195]  eta: 0:00:08  lr: 0.000059  loss: 0.1297 (0.1294)  time: 0.0703  data: 0.0001  max mem: 2598
[14:29:59.999769] Epoch: [164]  [100/195]  eta: 0:00:07  lr: 0.000058  loss: 0.1301 (0.1298)  time: 0.0706  data: 0.0001  max mem: 2598
[14:30:01.414896] Epoch: [164]  [120/195]  eta: 0:00:05  lr: 0.000058  loss: 0.1323 (0.1302)  time: 0.0707  data: 0.0001  max mem: 2598
[14:30:02.826219] Epoch: [164]  [140/195]  eta: 0:00:04  lr: 0.000058  loss: 0.1310 (0.1303)  time: 0.0705  data: 0.0001  max mem: 2598
[14:30:04.225691] Epoch: [164]  [160/195]  eta: 0:00:02  lr: 0.000057  loss: 0.1315 (0.1305)  time: 0.0699  data: 0.0001  max mem: 2598
[14:30:05.619549] Epoch: [164]  [180/195]  eta: 0:00:01  lr: 0.000057  loss: 0.1321 (0.1307)  time: 0.0697  data: 0.0001  max mem: 2598
[14:30:06.586433] Epoch: [164]  [194/195]  eta: 0:00:00  lr: 0.000057  loss: 0.1322 (0.1306)  time: 0.0691  data: 0.0001  max mem: 2598
[14:30:06.639049] Epoch: [164] Total time: 0:00:14 (0.0728 s / it)
[14:30:06.639130] Averaged stats: lr: 0.000057  loss: 0.1322 (0.1306)
[14:30:06.642259] log_dir: ./output_dir/mae0.3
[14:30:07.075386] Epoch: [165]  [  0/195]  eta: 0:01:24  lr: 0.000057  loss: 0.1413 (0.1413)  time: 0.4316  data: 0.3001  max mem: 2598
[14:30:08.500271] Epoch: [165]  [ 20/195]  eta: 0:00:15  lr: 0.000056  loss: 0.1289 (0.1302)  time: 0.0712  data: 0.0001  max mem: 2598
[14:30:09.900241] Epoch: [165]  [ 40/195]  eta: 0:00:12  lr: 0.000056  loss: 0.1281 (0.1304)  time: 0.0700  data: 0.0001  max mem: 2598
[14:30:11.310182] Epoch: [165]  [ 60/195]  eta: 0:00:10  lr: 0.000056  loss: 0.1293 (0.1300)  time: 0.0705  data: 0.0001  max mem: 2598
[14:30:12.714516] Epoch: [165]  [ 80/195]  eta: 0:00:08  lr: 0.000055  loss: 0.1290 (0.1300)  time: 0.0702  data: 0.0001  max mem: 2598
[14:30:14.120703] Epoch: [165]  [100/195]  eta: 0:00:07  lr: 0.000055  loss: 0.1300 (0.1299)  time: 0.0703  data: 0.0001  max mem: 2598
[14:30:15.519352] Epoch: [165]  [120/195]  eta: 0:00:05  lr: 0.000055  loss: 0.1296 (0.1300)  time: 0.0699  data: 0.0001  max mem: 2598
[14:30:16.928443] Epoch: [165]  [140/195]  eta: 0:00:04  lr: 0.000055  loss: 0.1311 (0.1303)  time: 0.0704  data: 0.0001  max mem: 2598
[14:30:18.335408] Epoch: [165]  [160/195]  eta: 0:00:02  lr: 0.000054  loss: 0.1283 (0.1302)  time: 0.0703  data: 0.0001  max mem: 2598
[14:30:19.733214] Epoch: [165]  [180/195]  eta: 0:00:01  lr: 0.000054  loss: 0.1314 (0.1303)  time: 0.0698  data: 0.0001  max mem: 2598
[14:30:20.699146] Epoch: [165]  [194/195]  eta: 0:00:00  lr: 0.000054  loss: 0.1312 (0.1304)  time: 0.0690  data: 0.0001  max mem: 2598
[14:30:20.746307] Epoch: [165] Total time: 0:00:14 (0.0723 s / it)
[14:30:20.746379] Averaged stats: lr: 0.000054  loss: 0.1312 (0.1304)
[14:30:20.748920] log_dir: ./output_dir/mae0.3
[14:30:21.177648] Epoch: [166]  [  0/195]  eta: 0:01:23  lr: 0.000054  loss: 0.1307 (0.1307)  time: 0.4275  data: 0.3055  max mem: 2598
[14:30:22.648407] Epoch: [166]  [ 20/195]  eta: 0:00:15  lr: 0.000053  loss: 0.1293 (0.1286)  time: 0.0735  data: 0.0002  max mem: 2598
[14:30:24.020966] Epoch: [166]  [ 40/195]  eta: 0:00:12  lr: 0.000053  loss: 0.1281 (0.1286)  time: 0.0686  data: 0.0001  max mem: 2598
[14:30:25.391887] Epoch: [166]  [ 60/195]  eta: 0:00:10  lr: 0.000053  loss: 0.1318 (0.1296)  time: 0.0685  data: 0.0001  max mem: 2598
[14:30:26.814673] Epoch: [166]  [ 80/195]  eta: 0:00:08  lr: 0.000052  loss: 0.1280 (0.1295)  time: 0.0711  data: 0.0001  max mem: 2598
[14:30:28.191700] Epoch: [166]  [100/195]  eta: 0:00:06  lr: 0.000052  loss: 0.1308 (0.1299)  time: 0.0688  data: 0.0001  max mem: 2598
[14:30:29.576522] Epoch: [166]  [120/195]  eta: 0:00:05  lr: 0.000052  loss: 0.1299 (0.1301)  time: 0.0692  data: 0.0001  max mem: 2598
[14:30:30.955176] Epoch: [166]  [140/195]  eta: 0:00:03  lr: 0.000052  loss: 0.1314 (0.1302)  time: 0.0689  data: 0.0001  max mem: 2598
[14:30:32.325606] Epoch: [166]  [160/195]  eta: 0:00:02  lr: 0.000051  loss: 0.1297 (0.1302)  time: 0.0685  data: 0.0002  max mem: 2598
[14:30:33.694887] Epoch: [166]  [180/195]  eta: 0:00:01  lr: 0.000051  loss: 0.1304 (0.1302)  time: 0.0684  data: 0.0001  max mem: 2598
[14:30:34.645802] Epoch: [166]  [194/195]  eta: 0:00:00  lr: 0.000051  loss: 0.1329 (0.1303)  time: 0.0679  data: 0.0001  max mem: 2598
[14:30:34.689063] Epoch: [166] Total time: 0:00:13 (0.0715 s / it)
[14:30:34.689124] Averaged stats: lr: 0.000051  loss: 0.1329 (0.1303)
[14:30:34.691368] log_dir: ./output_dir/mae0.3
[14:30:35.075149] Epoch: [167]  [  0/195]  eta: 0:01:14  lr: 0.000051  loss: 0.1356 (0.1356)  time: 0.3826  data: 0.3037  max mem: 2598
[14:30:36.473694] Epoch: [167]  [ 20/195]  eta: 0:00:14  lr: 0.000050  loss: 0.1283 (0.1288)  time: 0.0699  data: 0.0001  max mem: 2598
[14:30:37.873215] Epoch: [167]  [ 40/195]  eta: 0:00:12  lr: 0.000050  loss: 0.1280 (0.1292)  time: 0.0699  data: 0.0001  max mem: 2598
[14:30:39.243176] Epoch: [167]  [ 60/195]  eta: 0:00:10  lr: 0.000050  loss: 0.1283 (0.1289)  time: 0.0685  data: 0.0001  max mem: 2598
[14:30:40.610342] Epoch: [167]  [ 80/195]  eta: 0:00:08  lr: 0.000049  loss: 0.1308 (0.1296)  time: 0.0683  data: 0.0001  max mem: 2598
[14:30:41.975032] Epoch: [167]  [100/195]  eta: 0:00:06  lr: 0.000049  loss: 0.1284 (0.1295)  time: 0.0682  data: 0.0001  max mem: 2598
[14:30:43.344645] Epoch: [167]  [120/195]  eta: 0:00:05  lr: 0.000049  loss: 0.1295 (0.1297)  time: 0.0684  data: 0.0001  max mem: 2598
[14:30:44.716204] Epoch: [167]  [140/195]  eta: 0:00:03  lr: 0.000049  loss: 0.1287 (0.1296)  time: 0.0685  data: 0.0001  max mem: 2598
[14:30:46.106694] Epoch: [167]  [160/195]  eta: 0:00:02  lr: 0.000048  loss: 0.1307 (0.1297)  time: 0.0695  data: 0.0001  max mem: 2598
[14:30:47.503567] Epoch: [167]  [180/195]  eta: 0:00:01  lr: 0.000048  loss: 0.1272 (0.1297)  time: 0.0698  data: 0.0002  max mem: 2598
[14:30:48.471129] Epoch: [167]  [194/195]  eta: 0:00:00  lr: 0.000048  loss: 0.1283 (0.1297)  time: 0.0691  data: 0.0001  max mem: 2598
[14:30:48.530544] Epoch: [167] Total time: 0:00:13 (0.0710 s / it)
[14:30:48.530617] Averaged stats: lr: 0.000048  loss: 0.1283 (0.1297)
[14:30:48.533350] log_dir: ./output_dir/mae0.3
[14:30:48.927766] Epoch: [168]  [  0/195]  eta: 0:01:16  lr: 0.000048  loss: 0.1346 (0.1346)  time: 0.3933  data: 0.3146  max mem: 2598
[14:30:50.363971] Epoch: [168]  [ 20/195]  eta: 0:00:15  lr: 0.000047  loss: 0.1290 (0.1281)  time: 0.0718  data: 0.0001  max mem: 2598
[14:30:51.779235] Epoch: [168]  [ 40/195]  eta: 0:00:12  lr: 0.000047  loss: 0.1307 (0.1294)  time: 0.0707  data: 0.0001  max mem: 2598
[14:30:53.204628] Epoch: [168]  [ 60/195]  eta: 0:00:10  lr: 0.000047  loss: 0.1283 (0.1295)  time: 0.0712  data: 0.0001  max mem: 2598
[14:30:54.612752] Epoch: [168]  [ 80/195]  eta: 0:00:08  lr: 0.000047  loss: 0.1283 (0.1295)  time: 0.0704  data: 0.0001  max mem: 2598
[14:30:56.013913] Epoch: [168]  [100/195]  eta: 0:00:07  lr: 0.000046  loss: 0.1283 (0.1293)  time: 0.0700  data: 0.0001  max mem: 2598
[14:30:57.421417] Epoch: [168]  [120/195]  eta: 0:00:05  lr: 0.000046  loss: 0.1291 (0.1294)  time: 0.0703  data: 0.0001  max mem: 2598
[14:30:58.831851] Epoch: [168]  [140/195]  eta: 0:00:04  lr: 0.000046  loss: 0.1290 (0.1295)  time: 0.0705  data: 0.0002  max mem: 2598
[14:31:00.237609] Epoch: [168]  [160/195]  eta: 0:00:02  lr: 0.000045  loss: 0.1290 (0.1296)  time: 0.0702  data: 0.0002  max mem: 2598
[14:31:01.610235] Epoch: [168]  [180/195]  eta: 0:00:01  lr: 0.000045  loss: 0.1297 (0.1296)  time: 0.0686  data: 0.0001  max mem: 2598
[14:31:02.562314] Epoch: [168]  [194/195]  eta: 0:00:00  lr: 0.000045  loss: 0.1302 (0.1295)  time: 0.0679  data: 0.0001  max mem: 2598
[14:31:02.629960] Epoch: [168] Total time: 0:00:14 (0.0723 s / it)
[14:31:02.630035] Averaged stats: lr: 0.000045  loss: 0.1302 (0.1295)
[14:31:02.632204] log_dir: ./output_dir/mae0.3
[14:31:03.037035] Epoch: [169]  [  0/195]  eta: 0:01:18  lr: 0.000045  loss: 0.1234 (0.1234)  time: 0.4032  data: 0.2829  max mem: 2598
[14:31:04.477611] Epoch: [169]  [ 20/195]  eta: 0:00:15  lr: 0.000045  loss: 0.1274 (0.1277)  time: 0.0720  data: 0.0002  max mem: 2598
[14:31:05.864991] Epoch: [169]  [ 40/195]  eta: 0:00:12  lr: 0.000044  loss: 0.1270 (0.1275)  time: 0.0693  data: 0.0001  max mem: 2598
[14:31:07.270689] Epoch: [169]  [ 60/195]  eta: 0:00:10  lr: 0.000044  loss: 0.1297 (0.1283)  time: 0.0702  data: 0.0001  max mem: 2598
[14:31:08.692530] Epoch: [169]  [ 80/195]  eta: 0:00:08  lr: 0.000044  loss: 0.1303 (0.1290)  time: 0.0710  data: 0.0002  max mem: 2598
[14:31:10.108600] Epoch: [169]  [100/195]  eta: 0:00:07  lr: 0.000043  loss: 0.1286 (0.1289)  time: 0.0708  data: 0.0002  max mem: 2598
[14:31:11.552316] Epoch: [169]  [120/195]  eta: 0:00:05  lr: 0.000043  loss: 0.1308 (0.1292)  time: 0.0721  data: 0.0002  max mem: 2598
[14:31:12.991338] Epoch: [169]  [140/195]  eta: 0:00:04  lr: 0.000043  loss: 0.1298 (0.1294)  time: 0.0719  data: 0.0002  max mem: 2598
[14:31:14.418426] Epoch: [169]  [160/195]  eta: 0:00:02  lr: 0.000043  loss: 0.1299 (0.1295)  time: 0.0713  data: 0.0002  max mem: 2598
[14:31:15.819765] Epoch: [169]  [180/195]  eta: 0:00:01  lr: 0.000042  loss: 0.1269 (0.1294)  time: 0.0700  data: 0.0002  max mem: 2598
[14:31:16.785572] Epoch: [169]  [194/195]  eta: 0:00:00  lr: 0.000042  loss: 0.1280 (0.1294)  time: 0.0690  data: 0.0001  max mem: 2598
[14:31:16.853437] Epoch: [169] Total time: 0:00:14 (0.0729 s / it)
[14:31:16.853513] Averaged stats: lr: 0.000042  loss: 0.1280 (0.1294)
[14:31:16.856139] log_dir: ./output_dir/mae0.3
[14:31:17.246879] Epoch: [170]  [  0/195]  eta: 0:01:15  lr: 0.000042  loss: 0.1290 (0.1290)  time: 0.3894  data: 0.3076  max mem: 2598
[14:31:18.680388] Epoch: [170]  [ 20/195]  eta: 0:00:15  lr: 0.000042  loss: 0.1286 (0.1289)  time: 0.0716  data: 0.0002  max mem: 2598
[14:31:20.071038] Epoch: [170]  [ 40/195]  eta: 0:00:12  lr: 0.000042  loss: 0.1269 (0.1282)  time: 0.0695  data: 0.0001  max mem: 2598
[14:31:21.487801] Epoch: [170]  [ 60/195]  eta: 0:00:10  lr: 0.000041  loss: 0.1279 (0.1282)  time: 0.0708  data: 0.0002  max mem: 2598
[14:31:22.924490] Epoch: [170]  [ 80/195]  eta: 0:00:08  lr: 0.000041  loss: 0.1304 (0.1288)  time: 0.0718  data: 0.0002  max mem: 2598
[14:31:24.339315] Epoch: [170]  [100/195]  eta: 0:00:07  lr: 0.000041  loss: 0.1312 (0.1291)  time: 0.0707  data: 0.0002  max mem: 2598
[14:31:25.750607] Epoch: [170]  [120/195]  eta: 0:00:05  lr: 0.000040  loss: 0.1296 (0.1292)  time: 0.0705  data: 0.0001  max mem: 2598
[14:31:27.160252] Epoch: [170]  [140/195]  eta: 0:00:04  lr: 0.000040  loss: 0.1306 (0.1295)  time: 0.0704  data: 0.0001  max mem: 2598
[14:31:28.570030] Epoch: [170]  [160/195]  eta: 0:00:02  lr: 0.000040  loss: 0.1278 (0.1294)  time: 0.0704  data: 0.0002  max mem: 2598
[14:31:29.970662] Epoch: [170]  [180/195]  eta: 0:00:01  lr: 0.000040  loss: 0.1303 (0.1295)  time: 0.0700  data: 0.0001  max mem: 2598
[14:31:30.941705] Epoch: [170]  [194/195]  eta: 0:00:00  lr: 0.000039  loss: 0.1297 (0.1295)  time: 0.0694  data: 0.0001  max mem: 2598
[14:31:30.997805] Epoch: [170] Total time: 0:00:14 (0.0725 s / it)
[14:31:30.997900] Averaged stats: lr: 0.000039  loss: 0.1297 (0.1295)
[14:31:31.001017] log_dir: ./output_dir/mae0.3
[14:31:31.457006] Epoch: [171]  [  0/195]  eta: 0:01:28  lr: 0.000039  loss: 0.1245 (0.1245)  time: 0.4549  data: 0.3666  max mem: 2598
[14:31:32.908346] Epoch: [171]  [ 20/195]  eta: 0:00:15  lr: 0.000039  loss: 0.1274 (0.1273)  time: 0.0725  data: 0.0002  max mem: 2598
[14:31:34.334553] Epoch: [171]  [ 40/195]  eta: 0:00:12  lr: 0.000039  loss: 0.1271 (0.1280)  time: 0.0713  data: 0.0002  max mem: 2598
[14:31:35.766105] Epoch: [171]  [ 60/195]  eta: 0:00:10  lr: 0.000039  loss: 0.1266 (0.1281)  time: 0.0715  data: 0.0002  max mem: 2598
[14:31:37.215377] Epoch: [171]  [ 80/195]  eta: 0:00:08  lr: 0.000038  loss: 0.1290 (0.1283)  time: 0.0724  data: 0.0002  max mem: 2598
[14:31:38.649808] Epoch: [171]  [100/195]  eta: 0:00:07  lr: 0.000038  loss: 0.1297 (0.1285)  time: 0.0717  data: 0.0002  max mem: 2598
[14:31:40.086840] Epoch: [171]  [120/195]  eta: 0:00:05  lr: 0.000038  loss: 0.1290 (0.1286)  time: 0.0718  data: 0.0002  max mem: 2598
[14:31:41.508221] Epoch: [171]  [140/195]  eta: 0:00:04  lr: 0.000038  loss: 0.1274 (0.1287)  time: 0.0710  data: 0.0002  max mem: 2598
[14:31:42.889555] Epoch: [171]  [160/195]  eta: 0:00:02  lr: 0.000037  loss: 0.1299 (0.1291)  time: 0.0690  data: 0.0002  max mem: 2598
[14:31:44.281500] Epoch: [171]  [180/195]  eta: 0:00:01  lr: 0.000037  loss: 0.1276 (0.1291)  time: 0.0696  data: 0.0002  max mem: 2598
[14:31:45.236573] Epoch: [171]  [194/195]  eta: 0:00:00  lr: 0.000037  loss: 0.1271 (0.1291)  time: 0.0682  data: 0.0001  max mem: 2598
[14:31:45.292769] Epoch: [171] Total time: 0:00:14 (0.0733 s / it)
[14:31:45.292867] Averaged stats: lr: 0.000037  loss: 0.1271 (0.1291)
[14:31:45.295731] log_dir: ./output_dir/mae0.3
[14:31:45.803286] Epoch: [172]  [  0/195]  eta: 0:01:38  lr: 0.000037  loss: 0.1296 (0.1296)  time: 0.5058  data: 0.3807  max mem: 2598
[14:31:47.259806] Epoch: [172]  [ 20/195]  eta: 0:00:16  lr: 0.000037  loss: 0.1285 (0.1289)  time: 0.0728  data: 0.0002  max mem: 2598
[14:31:48.695167] Epoch: [172]  [ 40/195]  eta: 0:00:12  lr: 0.000036  loss: 0.1259 (0.1283)  time: 0.0717  data: 0.0002  max mem: 2598
[14:31:50.095632] Epoch: [172]  [ 60/195]  eta: 0:00:10  lr: 0.000036  loss: 0.1303 (0.1291)  time: 0.0700  data: 0.0001  max mem: 2598
[14:31:51.476453] Epoch: [172]  [ 80/195]  eta: 0:00:08  lr: 0.000036  loss: 0.1295 (0.1294)  time: 0.0690  data: 0.0001  max mem: 2598
[14:31:52.866268] Epoch: [172]  [100/195]  eta: 0:00:07  lr: 0.000036  loss: 0.1275 (0.1291)  time: 0.0695  data: 0.0001  max mem: 2598
[14:31:54.258055] Epoch: [172]  [120/195]  eta: 0:00:05  lr: 0.000035  loss: 0.1305 (0.1293)  time: 0.0695  data: 0.0001  max mem: 2598
[14:31:55.646631] Epoch: [172]  [140/195]  eta: 0:00:04  lr: 0.000035  loss: 0.1272 (0.1290)  time: 0.0694  data: 0.0001  max mem: 2598
[14:31:57.042409] Epoch: [172]  [160/195]  eta: 0:00:02  lr: 0.000035  loss: 0.1285 (0.1291)  time: 0.0697  data: 0.0001  max mem: 2598
[14:31:58.435757] Epoch: [172]  [180/195]  eta: 0:00:01  lr: 0.000035  loss: 0.1279 (0.1291)  time: 0.0696  data: 0.0002  max mem: 2598
[14:31:59.399371] Epoch: [172]  [194/195]  eta: 0:00:00  lr: 0.000034  loss: 0.1283 (0.1292)  time: 0.0687  data: 0.0001  max mem: 2598
[14:31:59.464403] Epoch: [172] Total time: 0:00:14 (0.0727 s / it)
[14:31:59.464587] Averaged stats: lr: 0.000034  loss: 0.1283 (0.1292)
[14:31:59.468059] log_dir: ./output_dir/mae0.3
[14:31:59.936642] Epoch: [173]  [  0/195]  eta: 0:01:31  lr: 0.000034  loss: 0.1256 (0.1256)  time: 0.4672  data: 0.3362  max mem: 2598
[14:32:01.399368] Epoch: [173]  [ 20/195]  eta: 0:00:16  lr: 0.000034  loss: 0.1273 (0.1276)  time: 0.0731  data: 0.0002  max mem: 2598
[14:32:02.800568] Epoch: [173]  [ 40/195]  eta: 0:00:12  lr: 0.000034  loss: 0.1285 (0.1281)  time: 0.0700  data: 0.0001  max mem: 2598
[14:32:04.198063] Epoch: [173]  [ 60/195]  eta: 0:00:10  lr: 0.000034  loss: 0.1278 (0.1282)  time: 0.0698  data: 0.0001  max mem: 2598
[14:32:05.603327] Epoch: [173]  [ 80/195]  eta: 0:00:08  lr: 0.000033  loss: 0.1264 (0.1283)  time: 0.0702  data: 0.0002  max mem: 2598
[14:32:07.039860] Epoch: [173]  [100/195]  eta: 0:00:07  lr: 0.000033  loss: 0.1265 (0.1282)  time: 0.0718  data: 0.0002  max mem: 2598
[14:32:08.456250] Epoch: [173]  [120/195]  eta: 0:00:05  lr: 0.000033  loss: 0.1281 (0.1285)  time: 0.0708  data: 0.0002  max mem: 2598
[14:32:09.878928] Epoch: [173]  [140/195]  eta: 0:00:04  lr: 0.000033  loss: 0.1307 (0.1287)  time: 0.0711  data: 0.0002  max mem: 2598
[14:32:11.300102] Epoch: [173]  [160/195]  eta: 0:00:02  lr: 0.000032  loss: 0.1314 (0.1290)  time: 0.0710  data: 0.0001  max mem: 2598
[14:32:12.730921] Epoch: [173]  [180/195]  eta: 0:00:01  lr: 0.000032  loss: 0.1296 (0.1289)  time: 0.0715  data: 0.0002  max mem: 2598
[14:32:13.752559] Epoch: [173]  [194/195]  eta: 0:00:00  lr: 0.000032  loss: 0.1261 (0.1288)  time: 0.0729  data: 0.0001  max mem: 2598
[14:32:13.807639] Epoch: [173] Total time: 0:00:14 (0.0735 s / it)
[14:32:13.807710] Averaged stats: lr: 0.000032  loss: 0.1261 (0.1288)
[14:32:13.810208] log_dir: ./output_dir/mae0.3
[14:32:14.226316] Epoch: [174]  [  0/195]  eta: 0:01:20  lr: 0.000032  loss: 0.1320 (0.1320)  time: 0.4142  data: 0.3158  max mem: 2598
[14:32:15.690105] Epoch: [174]  [ 20/195]  eta: 0:00:15  lr: 0.000032  loss: 0.1284 (0.1301)  time: 0.0731  data: 0.0002  max mem: 2598
[14:32:17.136363] Epoch: [174]  [ 40/195]  eta: 0:00:12  lr: 0.000031  loss: 0.1290 (0.1292)  time: 0.0723  data: 0.0001  max mem: 2598
[14:32:18.556647] Epoch: [174]  [ 60/195]  eta: 0:00:10  lr: 0.000031  loss: 0.1274 (0.1289)  time: 0.0710  data: 0.0001  max mem: 2598
[14:32:19.984083] Epoch: [174]  [ 80/195]  eta: 0:00:08  lr: 0.000031  loss: 0.1306 (0.1295)  time: 0.0713  data: 0.0002  max mem: 2598
[14:32:21.387071] Epoch: [174]  [100/195]  eta: 0:00:07  lr: 0.000031  loss: 0.1291 (0.1297)  time: 0.0701  data: 0.0002  max mem: 2598
[14:32:22.793126] Epoch: [174]  [120/195]  eta: 0:00:05  lr: 0.000030  loss: 0.1301 (0.1297)  time: 0.0703  data: 0.0001  max mem: 2598
[14:32:24.219026] Epoch: [174]  [140/195]  eta: 0:00:04  lr: 0.000030  loss: 0.1295 (0.1297)  time: 0.0713  data: 0.0001  max mem: 2598
[14:32:25.641748] Epoch: [174]  [160/195]  eta: 0:00:02  lr: 0.000030  loss: 0.1311 (0.1298)  time: 0.0711  data: 0.0002  max mem: 2598
[14:32:27.075686] Epoch: [174]  [180/195]  eta: 0:00:01  lr: 0.000030  loss: 0.1295 (0.1298)  time: 0.0717  data: 0.0002  max mem: 2598
[14:32:28.065041] Epoch: [174]  [194/195]  eta: 0:00:00  lr: 0.000030  loss: 0.1290 (0.1297)  time: 0.0702  data: 0.0001  max mem: 2598
[14:32:28.114405] Epoch: [174] Total time: 0:00:14 (0.0734 s / it)
[14:32:28.114487] Averaged stats: lr: 0.000030  loss: 0.1290 (0.1297)
[14:32:28.117370] log_dir: ./output_dir/mae0.3
[14:32:28.496911] Epoch: [175]  [  0/195]  eta: 0:01:13  lr: 0.000030  loss: 0.1292 (0.1292)  time: 0.3788  data: 0.2848  max mem: 2598
[14:32:29.915275] Epoch: [175]  [ 20/195]  eta: 0:00:14  lr: 0.000029  loss: 0.1293 (0.1291)  time: 0.0709  data: 0.0001  max mem: 2598
[14:32:31.303467] Epoch: [175]  [ 40/195]  eta: 0:00:12  lr: 0.000029  loss: 0.1278 (0.1284)  time: 0.0694  data: 0.0001  max mem: 2598
[14:32:32.697633] Epoch: [175]  [ 60/195]  eta: 0:00:10  lr: 0.000029  loss: 0.1278 (0.1286)  time: 0.0697  data: 0.0001  max mem: 2598
[14:32:34.094016] Epoch: [175]  [ 80/195]  eta: 0:00:08  lr: 0.000029  loss: 0.1270 (0.1281)  time: 0.0698  data: 0.0001  max mem: 2598
[14:32:35.487684] Epoch: [175]  [100/195]  eta: 0:00:06  lr: 0.000028  loss: 0.1281 (0.1284)  time: 0.0696  data: 0.0001  max mem: 2598
[14:32:36.879355] Epoch: [175]  [120/195]  eta: 0:00:05  lr: 0.000028  loss: 0.1289 (0.1285)  time: 0.0695  data: 0.0001  max mem: 2598
[14:32:38.276946] Epoch: [175]  [140/195]  eta: 0:00:03  lr: 0.000028  loss: 0.1299 (0.1286)  time: 0.0698  data: 0.0001  max mem: 2598
[14:32:39.677739] Epoch: [175]  [160/195]  eta: 0:00:02  lr: 0.000028  loss: 0.1256 (0.1285)  time: 0.0700  data: 0.0002  max mem: 2598
[14:32:41.071792] Epoch: [175]  [180/195]  eta: 0:00:01  lr: 0.000027  loss: 0.1277 (0.1284)  time: 0.0697  data: 0.0001  max mem: 2598
[14:32:42.037620] Epoch: [175]  [194/195]  eta: 0:00:00  lr: 0.000027  loss: 0.1274 (0.1284)  time: 0.0690  data: 0.0001  max mem: 2598
[14:32:42.104791] Epoch: [175] Total time: 0:00:13 (0.0717 s / it)
[14:32:42.104855] Averaged stats: lr: 0.000027  loss: 0.1274 (0.1284)
[14:32:42.107369] log_dir: ./output_dir/mae0.3
[14:32:42.520817] Epoch: [176]  [  0/195]  eta: 0:01:20  lr: 0.000027  loss: 0.1255 (0.1255)  time: 0.4120  data: 0.3032  max mem: 2598
[14:32:43.957074] Epoch: [176]  [ 20/195]  eta: 0:00:15  lr: 0.000027  loss: 0.1255 (0.1274)  time: 0.0718  data: 0.0001  max mem: 2598
[14:32:45.395514] Epoch: [176]  [ 40/195]  eta: 0:00:12  lr: 0.000027  loss: 0.1294 (0.1288)  time: 0.0719  data: 0.0002  max mem: 2598
[14:32:46.797926] Epoch: [176]  [ 60/195]  eta: 0:00:10  lr: 0.000027  loss: 0.1272 (0.1286)  time: 0.0701  data: 0.0001  max mem: 2598
[14:32:48.221579] Epoch: [176]  [ 80/195]  eta: 0:00:08  lr: 0.000026  loss: 0.1269 (0.1288)  time: 0.0711  data: 0.0001  max mem: 2598
[14:32:49.652140] Epoch: [176]  [100/195]  eta: 0:00:07  lr: 0.000026  loss: 0.1280 (0.1285)  time: 0.0715  data: 0.0002  max mem: 2598
[14:32:51.059447] Epoch: [176]  [120/195]  eta: 0:00:05  lr: 0.000026  loss: 0.1291 (0.1287)  time: 0.0703  data: 0.0002  max mem: 2598
[14:32:52.460764] Epoch: [176]  [140/195]  eta: 0:00:04  lr: 0.000026  loss: 0.1266 (0.1285)  time: 0.0700  data: 0.0001  max mem: 2598
[14:32:53.882737] Epoch: [176]  [160/195]  eta: 0:00:02  lr: 0.000025  loss: 0.1291 (0.1288)  time: 0.0711  data: 0.0002  max mem: 2598
[14:32:55.298005] Epoch: [176]  [180/195]  eta: 0:00:01  lr: 0.000025  loss: 0.1293 (0.1289)  time: 0.0707  data: 0.0001  max mem: 2598
[14:32:56.264001] Epoch: [176]  [194/195]  eta: 0:00:00  lr: 0.000025  loss: 0.1307 (0.1290)  time: 0.0690  data: 0.0001  max mem: 2598
[14:32:56.325001] Epoch: [176] Total time: 0:00:14 (0.0729 s / it)
[14:32:56.325068] Averaged stats: lr: 0.000025  loss: 0.1307 (0.1290)
[14:32:56.327578] log_dir: ./output_dir/mae0.3
[14:32:56.765352] Epoch: [177]  [  0/195]  eta: 0:01:25  lr: 0.000025  loss: 0.1281 (0.1281)  time: 0.4362  data: 0.3146  max mem: 2598
[14:32:58.212846] Epoch: [177]  [ 20/195]  eta: 0:00:15  lr: 0.000025  loss: 0.1279 (0.1278)  time: 0.0723  data: 0.0001  max mem: 2598
[14:32:59.657031] Epoch: [177]  [ 40/195]  eta: 0:00:12  lr: 0.000025  loss: 0.1274 (0.1276)  time: 0.0722  data: 0.0002  max mem: 2598
[14:33:01.057172] Epoch: [177]  [ 60/195]  eta: 0:00:10  lr: 0.000024  loss: 0.1274 (0.1279)  time: 0.0700  data: 0.0001  max mem: 2598
[14:33:02.459370] Epoch: [177]  [ 80/195]  eta: 0:00:08  lr: 0.000024  loss: 0.1287 (0.1280)  time: 0.0701  data: 0.0001  max mem: 2598
[14:33:03.891382] Epoch: [177]  [100/195]  eta: 0:00:07  lr: 0.000024  loss: 0.1270 (0.1280)  time: 0.0716  data: 0.0002  max mem: 2598
[14:33:05.293537] Epoch: [177]  [120/195]  eta: 0:00:05  lr: 0.000024  loss: 0.1282 (0.1282)  time: 0.0701  data: 0.0001  max mem: 2598
[14:33:06.704043] Epoch: [177]  [140/195]  eta: 0:00:04  lr: 0.000024  loss: 0.1297 (0.1284)  time: 0.0705  data: 0.0001  max mem: 2598
[14:33:08.128542] Epoch: [177]  [160/195]  eta: 0:00:02  lr: 0.000023  loss: 0.1284 (0.1284)  time: 0.0712  data: 0.0001  max mem: 2598
[14:33:09.530233] Epoch: [177]  [180/195]  eta: 0:00:01  lr: 0.000023  loss: 0.1277 (0.1283)  time: 0.0700  data: 0.0001  max mem: 2598
[14:33:10.510249] Epoch: [177]  [194/195]  eta: 0:00:00  lr: 0.000023  loss: 0.1292 (0.1284)  time: 0.0697  data: 0.0001  max mem: 2598
[14:33:10.562136] Epoch: [177] Total time: 0:00:14 (0.0730 s / it)
[14:33:10.562216] Averaged stats: lr: 0.000023  loss: 0.1292 (0.1284)
[14:33:10.564662] log_dir: ./output_dir/mae0.3
[14:33:10.957727] Epoch: [178]  [  0/195]  eta: 0:01:16  lr: 0.000023  loss: 0.1287 (0.1287)  time: 0.3917  data: 0.3101  max mem: 2598
[14:33:12.397742] Epoch: [178]  [ 20/195]  eta: 0:00:15  lr: 0.000023  loss: 0.1271 (0.1277)  time: 0.0720  data: 0.0001  max mem: 2598
[14:33:13.792638] Epoch: [178]  [ 40/195]  eta: 0:00:12  lr: 0.000023  loss: 0.1257 (0.1274)  time: 0.0697  data: 0.0001  max mem: 2598
[14:33:15.181159] Epoch: [178]  [ 60/195]  eta: 0:00:10  lr: 0.000022  loss: 0.1309 (0.1281)  time: 0.0694  data: 0.0001  max mem: 2598
[14:33:16.574293] Epoch: [178]  [ 80/195]  eta: 0:00:08  lr: 0.000022  loss: 0.1290 (0.1285)  time: 0.0696  data: 0.0001  max mem: 2598
[14:33:17.965563] Epoch: [178]  [100/195]  eta: 0:00:06  lr: 0.000022  loss: 0.1271 (0.1283)  time: 0.0695  data: 0.0001  max mem: 2598
[14:33:19.356327] Epoch: [178]  [120/195]  eta: 0:00:05  lr: 0.000022  loss: 0.1268 (0.1284)  time: 0.0695  data: 0.0001  max mem: 2598
[14:33:20.750264] Epoch: [178]  [140/195]  eta: 0:00:03  lr: 0.000022  loss: 0.1290 (0.1285)  time: 0.0697  data: 0.0001  max mem: 2598
[14:33:22.142434] Epoch: [178]  [160/195]  eta: 0:00:02  lr: 0.000021  loss: 0.1295 (0.1286)  time: 0.0696  data: 0.0001  max mem: 2598
[14:33:23.521695] Epoch: [178]  [180/195]  eta: 0:00:01  lr: 0.000021  loss: 0.1293 (0.1287)  time: 0.0689  data: 0.0001  max mem: 2598
[14:33:24.472194] Epoch: [178]  [194/195]  eta: 0:00:00  lr: 0.000021  loss: 0.1275 (0.1286)  time: 0.0679  data: 0.0001  max mem: 2598
[14:33:24.525491] Epoch: [178] Total time: 0:00:13 (0.0716 s / it)
[14:33:24.525553] Averaged stats: lr: 0.000021  loss: 0.1275 (0.1286)
[14:33:24.527825] log_dir: ./output_dir/mae0.3
[14:33:24.898584] Epoch: [179]  [  0/195]  eta: 0:01:12  lr: 0.000021  loss: 0.1260 (0.1260)  time: 0.3695  data: 0.2867  max mem: 2598
[14:33:26.298096] Epoch: [179]  [ 20/195]  eta: 0:00:14  lr: 0.000021  loss: 0.1268 (0.1271)  time: 0.0699  data: 0.0001  max mem: 2598
[14:33:27.666130] Epoch: [179]  [ 40/195]  eta: 0:00:11  lr: 0.000021  loss: 0.1284 (0.1278)  time: 0.0684  data: 0.0001  max mem: 2598
[14:33:29.051875] Epoch: [179]  [ 60/195]  eta: 0:00:10  lr: 0.000020  loss: 0.1279 (0.1280)  time: 0.0693  data: 0.0001  max mem: 2598
[14:33:30.429619] Epoch: [179]  [ 80/195]  eta: 0:00:08  lr: 0.000020  loss: 0.1299 (0.1287)  time: 0.0689  data: 0.0001  max mem: 2598
[14:33:31.843327] Epoch: [179]  [100/195]  eta: 0:00:06  lr: 0.000020  loss: 0.1261 (0.1285)  time: 0.0706  data: 0.0002  max mem: 2598
[14:33:33.256113] Epoch: [179]  [120/195]  eta: 0:00:05  lr: 0.000020  loss: 0.1268 (0.1284)  time: 0.0706  data: 0.0002  max mem: 2598
[14:33:34.653151] Epoch: [179]  [140/195]  eta: 0:00:03  lr: 0.000020  loss: 0.1296 (0.1285)  time: 0.0698  data: 0.0001  max mem: 2598
[14:33:36.050355] Epoch: [179]  [160/195]  eta: 0:00:02  lr: 0.000019  loss: 0.1300 (0.1287)  time: 0.0698  data: 0.0001  max mem: 2598
[14:33:37.437391] Epoch: [179]  [180/195]  eta: 0:00:01  lr: 0.000019  loss: 0.1264 (0.1286)  time: 0.0693  data: 0.0001  max mem: 2598
[14:33:38.398485] Epoch: [179]  [194/195]  eta: 0:00:00  lr: 0.000019  loss: 0.1289 (0.1286)  time: 0.0688  data: 0.0001  max mem: 2598
[14:33:38.452298] Epoch: [179] Total time: 0:00:13 (0.0714 s / it)
[14:33:38.452367] Averaged stats: lr: 0.000019  loss: 0.1289 (0.1286)
[14:33:38.454857] log_dir: ./output_dir/mae0.3
[14:33:38.913141] Epoch: [180]  [  0/195]  eta: 0:01:29  lr: 0.000019  loss: 0.1306 (0.1306)  time: 0.4568  data: 0.3203  max mem: 2598
[14:33:40.303907] Epoch: [180]  [ 20/195]  eta: 0:00:15  lr: 0.000019  loss: 0.1289 (0.1291)  time: 0.0695  data: 0.0001  max mem: 2598
[14:33:41.676395] Epoch: [180]  [ 40/195]  eta: 0:00:12  lr: 0.000019  loss: 0.1263 (0.1276)  time: 0.0686  data: 0.0001  max mem: 2598
[14:33:43.085914] Epoch: [180]  [ 60/195]  eta: 0:00:10  lr: 0.000018  loss: 0.1289 (0.1282)  time: 0.0704  data: 0.0001  max mem: 2598
[14:33:44.490087] Epoch: [180]  [ 80/195]  eta: 0:00:08  lr: 0.000018  loss: 0.1283 (0.1286)  time: 0.0702  data: 0.0001  max mem: 2598
[14:33:45.973676] Epoch: [180]  [100/195]  eta: 0:00:07  lr: 0.000018  loss: 0.1281 (0.1287)  time: 0.0741  data: 0.0002  max mem: 2598
[14:33:47.417610] Epoch: [180]  [120/195]  eta: 0:00:05  lr: 0.000018  loss: 0.1296 (0.1290)  time: 0.0722  data: 0.0002  max mem: 2598
[14:33:48.842871] Epoch: [180]  [140/195]  eta: 0:00:04  lr: 0.000018  loss: 0.1269 (0.1287)  time: 0.0712  data: 0.0002  max mem: 2598
[14:33:50.271223] Epoch: [180]  [160/195]  eta: 0:00:02  lr: 0.000018  loss: 0.1259 (0.1285)  time: 0.0714  data: 0.0002  max mem: 2598
[14:33:51.665983] Epoch: [180]  [180/195]  eta: 0:00:01  lr: 0.000017  loss: 0.1279 (0.1285)  time: 0.0697  data: 0.0001  max mem: 2598
[14:33:52.632596] Epoch: [180]  [194/195]  eta: 0:00:00  lr: 0.000017  loss: 0.1279 (0.1285)  time: 0.0691  data: 0.0001  max mem: 2598
[14:33:52.679463] Epoch: [180] Total time: 0:00:14 (0.0729 s / it)
[14:33:52.679531] Averaged stats: lr: 0.000017  loss: 0.1279 (0.1285)
[14:33:52.867612] log_dir: ./output_dir/mae0.3
[14:33:53.295112] Epoch: [181]  [  0/195]  eta: 0:01:23  lr: 0.000017  loss: 0.1263 (0.1263)  time: 0.4264  data: 0.3340  max mem: 2598
[14:33:54.712325] Epoch: [181]  [ 20/195]  eta: 0:00:15  lr: 0.000017  loss: 0.1282 (0.1280)  time: 0.0708  data: 0.0001  max mem: 2598
[14:33:56.115381] Epoch: [181]  [ 40/195]  eta: 0:00:12  lr: 0.000017  loss: 0.1265 (0.1279)  time: 0.0701  data: 0.0001  max mem: 2598
[14:33:57.517443] Epoch: [181]  [ 60/195]  eta: 0:00:10  lr: 0.000017  loss: 0.1268 (0.1277)  time: 0.0701  data: 0.0001  max mem: 2598
[14:33:58.930038] Epoch: [181]  [ 80/195]  eta: 0:00:08  lr: 0.000016  loss: 0.1287 (0.1282)  time: 0.0706  data: 0.0001  max mem: 2598
[14:34:00.338658] Epoch: [181]  [100/195]  eta: 0:00:07  lr: 0.000016  loss: 0.1270 (0.1282)  time: 0.0704  data: 0.0001  max mem: 2598
[14:34:01.745678] Epoch: [181]  [120/195]  eta: 0:00:05  lr: 0.000016  loss: 0.1263 (0.1280)  time: 0.0703  data: 0.0002  max mem: 2598
[14:34:03.154235] Epoch: [181]  [140/195]  eta: 0:00:04  lr: 0.000016  loss: 0.1262 (0.1279)  time: 0.0704  data: 0.0002  max mem: 2598
[14:34:04.566602] Epoch: [181]  [160/195]  eta: 0:00:02  lr: 0.000016  loss: 0.1284 (0.1282)  time: 0.0706  data: 0.0002  max mem: 2598
[14:34:05.968299] Epoch: [181]  [180/195]  eta: 0:00:01  lr: 0.000016  loss: 0.1261 (0.1280)  time: 0.0700  data: 0.0001  max mem: 2598
[14:34:06.935047] Epoch: [181]  [194/195]  eta: 0:00:00  lr: 0.000015  loss: 0.1259 (0.1279)  time: 0.0691  data: 0.0001  max mem: 2598
[14:34:06.988190] Epoch: [181] Total time: 0:00:14 (0.0724 s / it)
[14:34:06.988258] Averaged stats: lr: 0.000015  loss: 0.1259 (0.1279)
[14:34:06.990765] log_dir: ./output_dir/mae0.3
[14:34:07.429632] Epoch: [182]  [  0/195]  eta: 0:01:25  lr: 0.000015  loss: 0.1404 (0.1404)  time: 0.4376  data: 0.3162  max mem: 2598
[14:34:08.875802] Epoch: [182]  [ 20/195]  eta: 0:00:15  lr: 0.000015  loss: 0.1276 (0.1292)  time: 0.0723  data: 0.0002  max mem: 2598
[14:34:10.292725] Epoch: [182]  [ 40/195]  eta: 0:00:12  lr: 0.000015  loss: 0.1264 (0.1281)  time: 0.0708  data: 0.0001  max mem: 2598
[14:34:11.690291] Epoch: [182]  [ 60/195]  eta: 0:00:10  lr: 0.000015  loss: 0.1272 (0.1283)  time: 0.0698  data: 0.0001  max mem: 2598
[14:34:13.102170] Epoch: [182]  [ 80/195]  eta: 0:00:08  lr: 0.000015  loss: 0.1281 (0.1288)  time: 0.0705  data: 0.0001  max mem: 2598
[14:34:14.509052] Epoch: [182]  [100/195]  eta: 0:00:07  lr: 0.000015  loss: 0.1272 (0.1284)  time: 0.0703  data: 0.0001  max mem: 2598
[14:34:15.898021] Epoch: [182]  [120/195]  eta: 0:00:05  lr: 0.000014  loss: 0.1315 (0.1287)  time: 0.0694  data: 0.0001  max mem: 2598
[14:34:17.280364] Epoch: [182]  [140/195]  eta: 0:00:04  lr: 0.000014  loss: 0.1288 (0.1287)  time: 0.0691  data: 0.0001  max mem: 2598
[14:34:18.664400] Epoch: [182]  [160/195]  eta: 0:00:02  lr: 0.000014  loss: 0.1280 (0.1288)  time: 0.0692  data: 0.0001  max mem: 2598
[14:34:20.045577] Epoch: [182]  [180/195]  eta: 0:00:01  lr: 0.000014  loss: 0.1270 (0.1288)  time: 0.0690  data: 0.0001  max mem: 2598
[14:34:21.004928] Epoch: [182]  [194/195]  eta: 0:00:00  lr: 0.000014  loss: 0.1258 (0.1286)  time: 0.0684  data: 0.0001  max mem: 2598
[14:34:21.050319] Epoch: [182] Total time: 0:00:14 (0.0721 s / it)
[14:34:21.050391] Averaged stats: lr: 0.000014  loss: 0.1258 (0.1286)
[14:34:21.052908] log_dir: ./output_dir/mae0.3
[14:34:21.465724] Epoch: [183]  [  0/195]  eta: 0:01:20  lr: 0.000014  loss: 0.1266 (0.1266)  time: 0.4113  data: 0.3053  max mem: 2598
[14:34:22.898958] Epoch: [183]  [ 20/195]  eta: 0:00:15  lr: 0.000014  loss: 0.1262 (0.1264)  time: 0.0716  data: 0.0002  max mem: 2598
[14:34:24.316130] Epoch: [183]  [ 40/195]  eta: 0:00:12  lr: 0.000013  loss: 0.1300 (0.1278)  time: 0.0708  data: 0.0001  max mem: 2598
[14:34:25.723352] Epoch: [183]  [ 60/195]  eta: 0:00:10  lr: 0.000013  loss: 0.1271 (0.1280)  time: 0.0703  data: 0.0001  max mem: 2598
[14:34:27.136871] Epoch: [183]  [ 80/195]  eta: 0:00:08  lr: 0.000013  loss: 0.1262 (0.1279)  time: 0.0706  data: 0.0001  max mem: 2598
[14:34:28.549849] Epoch: [183]  [100/195]  eta: 0:00:07  lr: 0.000013  loss: 0.1281 (0.1278)  time: 0.0706  data: 0.0001  max mem: 2598
[14:34:29.956088] Epoch: [183]  [120/195]  eta: 0:00:05  lr: 0.000013  loss: 0.1311 (0.1282)  time: 0.0703  data: 0.0001  max mem: 2598
[14:34:31.369816] Epoch: [183]  [140/195]  eta: 0:00:04  lr: 0.000013  loss: 0.1276 (0.1283)  time: 0.0706  data: 0.0001  max mem: 2598
[14:34:32.783188] Epoch: [183]  [160/195]  eta: 0:00:02  lr: 0.000013  loss: 0.1288 (0.1283)  time: 0.0706  data: 0.0001  max mem: 2598
[14:34:34.186635] Epoch: [183]  [180/195]  eta: 0:00:01  lr: 0.000012  loss: 0.1282 (0.1283)  time: 0.0701  data: 0.0001  max mem: 2598
[14:34:35.154824] Epoch: [183]  [194/195]  eta: 0:00:00  lr: 0.000012  loss: 0.1299 (0.1285)  time: 0.0692  data: 0.0001  max mem: 2598
[14:34:35.207461] Epoch: [183] Total time: 0:00:14 (0.0726 s / it)
[14:34:35.207530] Averaged stats: lr: 0.000012  loss: 0.1299 (0.1285)
[14:34:35.210057] log_dir: ./output_dir/mae0.3
[14:34:35.593601] Epoch: [184]  [  0/195]  eta: 0:01:14  lr: 0.000012  loss: 0.1266 (0.1266)  time: 0.3815  data: 0.3011  max mem: 2598
[14:34:37.019763] Epoch: [184]  [ 20/195]  eta: 0:00:15  lr: 0.000012  loss: 0.1271 (0.1260)  time: 0.0713  data: 0.0001  max mem: 2598
[14:34:38.415751] Epoch: [184]  [ 40/195]  eta: 0:00:12  lr: 0.000012  loss: 0.1283 (0.1274)  time: 0.0698  data: 0.0001  max mem: 2598
[14:34:39.797872] Epoch: [184]  [ 60/195]  eta: 0:00:10  lr: 0.000012  loss: 0.1272 (0.1275)  time: 0.0691  data: 0.0001  max mem: 2598
[14:34:41.190768] Epoch: [184]  [ 80/195]  eta: 0:00:08  lr: 0.000012  loss: 0.1298 (0.1279)  time: 0.0696  data: 0.0001  max mem: 2598
[14:34:42.585754] Epoch: [184]  [100/195]  eta: 0:00:06  lr: 0.000011  loss: 0.1278 (0.1279)  time: 0.0697  data: 0.0001  max mem: 2598
[14:34:43.988443] Epoch: [184]  [120/195]  eta: 0:00:05  lr: 0.000011  loss: 0.1278 (0.1280)  time: 0.0701  data: 0.0001  max mem: 2598
[14:34:45.380825] Epoch: [184]  [140/195]  eta: 0:00:03  lr: 0.000011  loss: 0.1300 (0.1282)  time: 0.0696  data: 0.0001  max mem: 2598
[14:34:46.775680] Epoch: [184]  [160/195]  eta: 0:00:02  lr: 0.000011  loss: 0.1271 (0.1281)  time: 0.0697  data: 0.0001  max mem: 2598
[14:34:48.169870] Epoch: [184]  [180/195]  eta: 0:00:01  lr: 0.000011  loss: 0.1282 (0.1283)  time: 0.0697  data: 0.0001  max mem: 2598
[14:34:49.122500] Epoch: [184]  [194/195]  eta: 0:00:00  lr: 0.000011  loss: 0.1285 (0.1283)  time: 0.0682  data: 0.0001  max mem: 2598
[14:34:49.175720] Epoch: [184] Total time: 0:00:13 (0.0716 s / it)
[14:34:49.175789] Averaged stats: lr: 0.000011  loss: 0.1285 (0.1283)
[14:34:49.178119] log_dir: ./output_dir/mae0.3
[14:34:49.568136] Epoch: [185]  [  0/195]  eta: 0:01:15  lr: 0.000011  loss: 0.1253 (0.1253)  time: 0.3884  data: 0.2734  max mem: 2598
[14:34:51.010104] Epoch: [185]  [ 20/195]  eta: 0:00:15  lr: 0.000011  loss: 0.1275 (0.1269)  time: 0.0721  data: 0.0002  max mem: 2598
[14:34:52.412205] Epoch: [185]  [ 40/195]  eta: 0:00:12  lr: 0.000010  loss: 0.1285 (0.1282)  time: 0.0701  data: 0.0001  max mem: 2598
[14:34:53.816012] Epoch: [185]  [ 60/195]  eta: 0:00:10  lr: 0.000010  loss: 0.1283 (0.1281)  time: 0.0701  data: 0.0001  max mem: 2598
[14:34:55.225443] Epoch: [185]  [ 80/195]  eta: 0:00:08  lr: 0.000010  loss: 0.1285 (0.1279)  time: 0.0704  data: 0.0001  max mem: 2598
[14:34:56.636662] Epoch: [185]  [100/195]  eta: 0:00:07  lr: 0.000010  loss: 0.1275 (0.1280)  time: 0.0705  data: 0.0001  max mem: 2598
[14:34:58.051497] Epoch: [185]  [120/195]  eta: 0:00:05  lr: 0.000010  loss: 0.1299 (0.1281)  time: 0.0707  data: 0.0001  max mem: 2598
[14:34:59.460875] Epoch: [185]  [140/195]  eta: 0:00:04  lr: 0.000010  loss: 0.1256 (0.1278)  time: 0.0704  data: 0.0001  max mem: 2598
[14:35:00.870901] Epoch: [185]  [160/195]  eta: 0:00:02  lr: 0.000010  loss: 0.1270 (0.1280)  time: 0.0705  data: 0.0001  max mem: 2598
[14:35:02.274803] Epoch: [185]  [180/195]  eta: 0:00:01  lr: 0.000009  loss: 0.1294 (0.1281)  time: 0.0701  data: 0.0001  max mem: 2598
[14:35:03.244480] Epoch: [185]  [194/195]  eta: 0:00:00  lr: 0.000009  loss: 0.1264 (0.1279)  time: 0.0693  data: 0.0001  max mem: 2598
[14:35:03.288097] Epoch: [185] Total time: 0:00:14 (0.0724 s / it)
[14:35:03.288170] Averaged stats: lr: 0.000009  loss: 0.1264 (0.1279)
[14:35:03.290700] log_dir: ./output_dir/mae0.3
[14:35:03.660775] Epoch: [186]  [  0/195]  eta: 0:01:12  lr: 0.000009  loss: 0.1214 (0.1214)  time: 0.3693  data: 0.2878  max mem: 2598
[14:35:05.048158] Epoch: [186]  [ 20/195]  eta: 0:00:14  lr: 0.000009  loss: 0.1261 (0.1260)  time: 0.0693  data: 0.0001  max mem: 2598
[14:35:06.416841] Epoch: [186]  [ 40/195]  eta: 0:00:11  lr: 0.000009  loss: 0.1264 (0.1271)  time: 0.0684  data: 0.0001  max mem: 2598
[14:35:07.786325] Epoch: [186]  [ 60/195]  eta: 0:00:09  lr: 0.000009  loss: 0.1289 (0.1275)  time: 0.0684  data: 0.0001  max mem: 2598
[14:35:09.161212] Epoch: [186]  [ 80/195]  eta: 0:00:08  lr: 0.000009  loss: 0.1275 (0.1277)  time: 0.0687  data: 0.0001  max mem: 2598
[14:35:10.530304] Epoch: [186]  [100/195]  eta: 0:00:06  lr: 0.000009  loss: 0.1260 (0.1275)  time: 0.0684  data: 0.0001  max mem: 2598
[14:35:11.899074] Epoch: [186]  [120/195]  eta: 0:00:05  lr: 0.000009  loss: 0.1273 (0.1275)  time: 0.0684  data: 0.0001  max mem: 2598
[14:35:13.267996] Epoch: [186]  [140/195]  eta: 0:00:03  lr: 0.000008  loss: 0.1303 (0.1279)  time: 0.0684  data: 0.0001  max mem: 2598
[14:35:14.647786] Epoch: [186]  [160/195]  eta: 0:00:02  lr: 0.000008  loss: 0.1291 (0.1281)  time: 0.0690  data: 0.0001  max mem: 2598
[14:35:16.024215] Epoch: [186]  [180/195]  eta: 0:00:01  lr: 0.000008  loss: 0.1264 (0.1279)  time: 0.0688  data: 0.0001  max mem: 2598
[14:35:16.975789] Epoch: [186]  [194/195]  eta: 0:00:00  lr: 0.000008  loss: 0.1271 (0.1280)  time: 0.0679  data: 0.0001  max mem: 2598
[14:35:17.019353] Epoch: [186] Total time: 0:00:13 (0.0704 s / it)
[14:35:17.019421] Averaged stats: lr: 0.000008  loss: 0.1271 (0.1280)
[14:35:17.021678] log_dir: ./output_dir/mae0.3
[14:35:17.424848] Epoch: [187]  [  0/195]  eta: 0:01:18  lr: 0.000008  loss: 0.1213 (0.1213)  time: 0.4022  data: 0.3102  max mem: 2598
[14:35:18.831558] Epoch: [187]  [ 20/195]  eta: 0:00:15  lr: 0.000008  loss: 0.1244 (0.1257)  time: 0.0703  data: 0.0002  max mem: 2598
[14:35:20.224088] Epoch: [187]  [ 40/195]  eta: 0:00:12  lr: 0.000008  loss: 0.1272 (0.1266)  time: 0.0696  data: 0.0001  max mem: 2598
[14:35:21.606472] Epoch: [187]  [ 60/195]  eta: 0:00:10  lr: 0.000008  loss: 0.1274 (0.1269)  time: 0.0691  data: 0.0001  max mem: 2598
[14:35:22.991488] Epoch: [187]  [ 80/195]  eta: 0:00:08  lr: 0.000008  loss: 0.1290 (0.1273)  time: 0.0692  data: 0.0001  max mem: 2598
[14:35:24.374656] Epoch: [187]  [100/195]  eta: 0:00:06  lr: 0.000007  loss: 0.1289 (0.1277)  time: 0.0691  data: 0.0001  max mem: 2598
[14:35:25.771113] Epoch: [187]  [120/195]  eta: 0:00:05  lr: 0.000007  loss: 0.1291 (0.1279)  time: 0.0698  data: 0.0001  max mem: 2598
[14:35:27.149508] Epoch: [187]  [140/195]  eta: 0:00:03  lr: 0.000007  loss: 0.1302 (0.1281)  time: 0.0689  data: 0.0001  max mem: 2598
[14:35:28.531089] Epoch: [187]  [160/195]  eta: 0:00:02  lr: 0.000007  loss: 0.1265 (0.1281)  time: 0.0690  data: 0.0001  max mem: 2598
[14:35:29.919704] Epoch: [187]  [180/195]  eta: 0:00:01  lr: 0.000007  loss: 0.1267 (0.1280)  time: 0.0694  data: 0.0001  max mem: 2598
[14:35:30.875738] Epoch: [187]  [194/195]  eta: 0:00:00  lr: 0.000007  loss: 0.1256 (0.1278)  time: 0.0684  data: 0.0001  max mem: 2598
[14:35:30.928328] Epoch: [187] Total time: 0:00:13 (0.0713 s / it)
[14:35:30.928815] Averaged stats: lr: 0.000007  loss: 0.1256 (0.1278)
[14:35:30.931926] log_dir: ./output_dir/mae0.3
[14:35:31.357192] Epoch: [188]  [  0/195]  eta: 0:01:22  lr: 0.000007  loss: 0.1338 (0.1338)  time: 0.4241  data: 0.3356  max mem: 2598
[14:35:32.799650] Epoch: [188]  [ 20/195]  eta: 0:00:15  lr: 0.000007  loss: 0.1232 (0.1244)  time: 0.0721  data: 0.0002  max mem: 2598
[14:35:34.230456] Epoch: [188]  [ 40/195]  eta: 0:00:12  lr: 0.000007  loss: 0.1283 (0.1260)  time: 0.0715  data: 0.0002  max mem: 2598
[14:35:35.613604] Epoch: [188]  [ 60/195]  eta: 0:00:10  lr: 0.000007  loss: 0.1280 (0.1266)  time: 0.0691  data: 0.0001  max mem: 2598
[14:35:37.012434] Epoch: [188]  [ 80/195]  eta: 0:00:08  lr: 0.000006  loss: 0.1279 (0.1270)  time: 0.0699  data: 0.0001  max mem: 2598
[14:35:38.389911] Epoch: [188]  [100/195]  eta: 0:00:07  lr: 0.000006  loss: 0.1275 (0.1271)  time: 0.0688  data: 0.0001  max mem: 2598
[14:35:39.776109] Epoch: [188]  [120/195]  eta: 0:00:05  lr: 0.000006  loss: 0.1290 (0.1274)  time: 0.0693  data: 0.0001  max mem: 2598
[14:35:41.165376] Epoch: [188]  [140/195]  eta: 0:00:03  lr: 0.000006  loss: 0.1274 (0.1276)  time: 0.0694  data: 0.0001  max mem: 2598
[14:35:42.558432] Epoch: [188]  [160/195]  eta: 0:00:02  lr: 0.000006  loss: 0.1303 (0.1280)  time: 0.0696  data: 0.0001  max mem: 2598
[14:35:43.945658] Epoch: [188]  [180/195]  eta: 0:00:01  lr: 0.000006  loss: 0.1255 (0.1278)  time: 0.0693  data: 0.0001  max mem: 2598
[14:35:44.902610] Epoch: [188]  [194/195]  eta: 0:00:00  lr: 0.000006  loss: 0.1268 (0.1277)  time: 0.0683  data: 0.0001  max mem: 2598
[14:35:44.970768] Epoch: [188] Total time: 0:00:14 (0.0720 s / it)
[14:35:44.970855] Averaged stats: lr: 0.000006  loss: 0.1268 (0.1277)
[14:35:44.973861] log_dir: ./output_dir/mae0.3
[14:35:45.422159] Epoch: [189]  [  0/195]  eta: 0:01:27  lr: 0.000006  loss: 0.1342 (0.1342)  time: 0.4469  data: 0.3258  max mem: 2598
[14:35:46.866863] Epoch: [189]  [ 20/195]  eta: 0:00:15  lr: 0.000006  loss: 0.1259 (0.1258)  time: 0.0722  data: 0.0002  max mem: 2598
[14:35:48.246457] Epoch: [189]  [ 40/195]  eta: 0:00:12  lr: 0.000006  loss: 0.1262 (0.1266)  time: 0.0689  data: 0.0001  max mem: 2598
[14:35:49.668221] Epoch: [189]  [ 60/195]  eta: 0:00:10  lr: 0.000005  loss: 0.1276 (0.1271)  time: 0.0710  data: 0.0002  max mem: 2598
[14:35:51.070881] Epoch: [189]  [ 80/195]  eta: 0:00:08  lr: 0.000005  loss: 0.1277 (0.1273)  time: 0.0701  data: 0.0001  max mem: 2598
[14:35:52.469406] Epoch: [189]  [100/195]  eta: 0:00:07  lr: 0.000005  loss: 0.1262 (0.1274)  time: 0.0699  data: 0.0001  max mem: 2598
[14:35:53.876642] Epoch: [189]  [120/195]  eta: 0:00:05  lr: 0.000005  loss: 0.1249 (0.1272)  time: 0.0703  data: 0.0001  max mem: 2598
[14:35:55.280650] Epoch: [189]  [140/195]  eta: 0:00:04  lr: 0.000005  loss: 0.1266 (0.1273)  time: 0.0702  data: 0.0001  max mem: 2598
[14:35:56.722177] Epoch: [189]  [160/195]  eta: 0:00:02  lr: 0.000005  loss: 0.1279 (0.1274)  time: 0.0720  data: 0.0002  max mem: 2598
[14:35:58.137213] Epoch: [189]  [180/195]  eta: 0:00:01  lr: 0.000005  loss: 0.1268 (0.1275)  time: 0.0707  data: 0.0002  max mem: 2598
[14:35:59.116569] Epoch: [189]  [194/195]  eta: 0:00:00  lr: 0.000005  loss: 0.1262 (0.1275)  time: 0.0699  data: 0.0001  max mem: 2598
[14:35:59.182362] Epoch: [189] Total time: 0:00:14 (0.0729 s / it)
[14:35:59.182459] Averaged stats: lr: 0.000005  loss: 0.1262 (0.1275)
[14:35:59.186099] log_dir: ./output_dir/mae0.3
[14:35:59.727560] Epoch: [190]  [  0/195]  eta: 0:01:45  lr: 0.000005  loss: 0.1234 (0.1234)  time: 0.5397  data: 0.3905  max mem: 2598
[14:36:01.171562] Epoch: [190]  [ 20/195]  eta: 0:00:16  lr: 0.000005  loss: 0.1275 (0.1271)  time: 0.0722  data: 0.0002  max mem: 2598
[14:36:02.539590] Epoch: [190]  [ 40/195]  eta: 0:00:12  lr: 0.000005  loss: 0.1269 (0.1275)  time: 0.0684  data: 0.0001  max mem: 2598
[14:36:03.910062] Epoch: [190]  [ 60/195]  eta: 0:00:10  lr: 0.000005  loss: 0.1257 (0.1270)  time: 0.0685  data: 0.0001  max mem: 2598
[14:36:05.293851] Epoch: [190]  [ 80/195]  eta: 0:00:08  lr: 0.000004  loss: 0.1266 (0.1272)  time: 0.0691  data: 0.0001  max mem: 2598
[14:36:06.701504] Epoch: [190]  [100/195]  eta: 0:00:07  lr: 0.000004  loss: 0.1256 (0.1273)  time: 0.0704  data: 0.0002  max mem: 2598
[14:36:08.105858] Epoch: [190]  [120/195]  eta: 0:00:05  lr: 0.000004  loss: 0.1283 (0.1275)  time: 0.0702  data: 0.0001  max mem: 2598
[14:36:09.511175] Epoch: [190]  [140/195]  eta: 0:00:04  lr: 0.000004  loss: 0.1262 (0.1275)  time: 0.0702  data: 0.0001  max mem: 2598
[14:36:10.923024] Epoch: [190]  [160/195]  eta: 0:00:02  lr: 0.000004  loss: 0.1285 (0.1278)  time: 0.0705  data: 0.0001  max mem: 2598
[14:36:12.322374] Epoch: [190]  [180/195]  eta: 0:00:01  lr: 0.000004  loss: 0.1259 (0.1277)  time: 0.0699  data: 0.0002  max mem: 2598
[14:36:13.291525] Epoch: [190]  [194/195]  eta: 0:00:00  lr: 0.000004  loss: 0.1262 (0.1278)  time: 0.0692  data: 0.0001  max mem: 2598
[14:36:13.363717] Epoch: [190] Total time: 0:00:14 (0.0727 s / it)
[14:36:13.363788] Averaged stats: lr: 0.000004  loss: 0.1262 (0.1278)
[14:36:13.366349] log_dir: ./output_dir/mae0.3
[14:36:13.773876] Epoch: [191]  [  0/195]  eta: 0:01:19  lr: 0.000004  loss: 0.1359 (0.1359)  time: 0.4058  data: 0.2861  max mem: 2598
[14:36:15.217945] Epoch: [191]  [ 20/195]  eta: 0:00:15  lr: 0.000004  loss: 0.1272 (0.1264)  time: 0.0722  data: 0.0001  max mem: 2598
[14:36:16.624428] Epoch: [191]  [ 40/195]  eta: 0:00:12  lr: 0.000004  loss: 0.1257 (0.1260)  time: 0.0703  data: 0.0001  max mem: 2598
[14:36:18.031787] Epoch: [191]  [ 60/195]  eta: 0:00:10  lr: 0.000004  loss: 0.1267 (0.1267)  time: 0.0703  data: 0.0001  max mem: 2598
[14:36:19.435179] Epoch: [191]  [ 80/195]  eta: 0:00:08  lr: 0.000004  loss: 0.1263 (0.1270)  time: 0.0701  data: 0.0001  max mem: 2598
[14:36:20.856584] Epoch: [191]  [100/195]  eta: 0:00:07  lr: 0.000003  loss: 0.1257 (0.1266)  time: 0.0710  data: 0.0001  max mem: 2598
[14:36:22.259821] Epoch: [191]  [120/195]  eta: 0:00:05  lr: 0.000003  loss: 0.1288 (0.1270)  time: 0.0701  data: 0.0001  max mem: 2598
[14:36:23.673140] Epoch: [191]  [140/195]  eta: 0:00:04  lr: 0.000003  loss: 0.1264 (0.1270)  time: 0.0706  data: 0.0001  max mem: 2598
[14:36:25.094908] Epoch: [191]  [160/195]  eta: 0:00:02  lr: 0.000003  loss: 0.1282 (0.1272)  time: 0.0710  data: 0.0001  max mem: 2598
[14:36:26.507977] Epoch: [191]  [180/195]  eta: 0:00:01  lr: 0.000003  loss: 0.1275 (0.1272)  time: 0.0706  data: 0.0001  max mem: 2598
[14:36:27.477670] Epoch: [191]  [194/195]  eta: 0:00:00  lr: 0.000003  loss: 0.1258 (0.1273)  time: 0.0692  data: 0.0001  max mem: 2598
[14:36:27.523706] Epoch: [191] Total time: 0:00:14 (0.0726 s / it)
[14:36:27.523878] Averaged stats: lr: 0.000003  loss: 0.1258 (0.1273)
[14:36:27.526365] log_dir: ./output_dir/mae0.3
[14:36:27.925112] Epoch: [192]  [  0/195]  eta: 0:01:17  lr: 0.000003  loss: 0.1410 (0.1410)  time: 0.3977  data: 0.3056  max mem: 2598
[14:36:29.350627] Epoch: [192]  [ 20/195]  eta: 0:00:15  lr: 0.000003  loss: 0.1255 (0.1288)  time: 0.0712  data: 0.0001  max mem: 2598
[14:36:30.726083] Epoch: [192]  [ 40/195]  eta: 0:00:12  lr: 0.000003  loss: 0.1263 (0.1285)  time: 0.0687  data: 0.0001  max mem: 2598
[14:36:32.117820] Epoch: [192]  [ 60/195]  eta: 0:00:10  lr: 0.000003  loss: 0.1286 (0.1287)  time: 0.0696  data: 0.0001  max mem: 2598
[14:36:33.500750] Epoch: [192]  [ 80/195]  eta: 0:00:08  lr: 0.000003  loss: 0.1263 (0.1284)  time: 0.0691  data: 0.0001  max mem: 2598
[14:36:34.889454] Epoch: [192]  [100/195]  eta: 0:00:06  lr: 0.000003  loss: 0.1278 (0.1284)  time: 0.0694  data: 0.0001  max mem: 2598
[14:36:36.284175] Epoch: [192]  [120/195]  eta: 0:00:05  lr: 0.000003  loss: 0.1265 (0.1283)  time: 0.0697  data: 0.0001  max mem: 2598
[14:36:37.666567] Epoch: [192]  [140/195]  eta: 0:00:03  lr: 0.000003  loss: 0.1268 (0.1281)  time: 0.0691  data: 0.0001  max mem: 2598
[14:36:39.078537] Epoch: [192]  [160/195]  eta: 0:00:02  lr: 0.000002  loss: 0.1283 (0.1282)  time: 0.0705  data: 0.0001  max mem: 2598
[14:36:40.488490] Epoch: [192]  [180/195]  eta: 0:00:01  lr: 0.000002  loss: 0.1258 (0.1281)  time: 0.0705  data: 0.0001  max mem: 2598
[14:36:41.455932] Epoch: [192]  [194/195]  eta: 0:00:00  lr: 0.000002  loss: 0.1267 (0.1281)  time: 0.0692  data: 0.0001  max mem: 2598
[14:36:41.502846] Epoch: [192] Total time: 0:00:13 (0.0717 s / it)
[14:36:41.503024] Averaged stats: lr: 0.000002  loss: 0.1267 (0.1281)
[14:36:41.505637] log_dir: ./output_dir/mae0.3
[14:36:41.958552] Epoch: [193]  [  0/195]  eta: 0:01:27  lr: 0.000002  loss: 0.1301 (0.1301)  time: 0.4512  data: 0.3050  max mem: 2598
[14:36:43.419112] Epoch: [193]  [ 20/195]  eta: 0:00:15  lr: 0.000002  loss: 0.1249 (0.1272)  time: 0.0730  data: 0.0002  max mem: 2598
[14:36:44.832320] Epoch: [193]  [ 40/195]  eta: 0:00:12  lr: 0.000002  loss: 0.1272 (0.1275)  time: 0.0706  data: 0.0001  max mem: 2598
[14:36:46.254071] Epoch: [193]  [ 60/195]  eta: 0:00:10  lr: 0.000002  loss: 0.1271 (0.1277)  time: 0.0710  data: 0.0001  max mem: 2598
[14:36:47.689004] Epoch: [193]  [ 80/195]  eta: 0:00:08  lr: 0.000002  loss: 0.1263 (0.1276)  time: 0.0717  data: 0.0001  max mem: 2598
[14:36:49.099299] Epoch: [193]  [100/195]  eta: 0:00:07  lr: 0.000002  loss: 0.1301 (0.1280)  time: 0.0705  data: 0.0001  max mem: 2598
[14:36:50.514837] Epoch: [193]  [120/195]  eta: 0:00:05  lr: 0.000002  loss: 0.1271 (0.1278)  time: 0.0707  data: 0.0001  max mem: 2598
[14:36:51.913811] Epoch: [193]  [140/195]  eta: 0:00:04  lr: 0.000002  loss: 0.1287 (0.1278)  time: 0.0699  data: 0.0001  max mem: 2598
[14:36:53.312433] Epoch: [193]  [160/195]  eta: 0:00:02  lr: 0.000002  loss: 0.1271 (0.1277)  time: 0.0699  data: 0.0001  max mem: 2598
[14:36:54.719248] Epoch: [193]  [180/195]  eta: 0:00:01  lr: 0.000002  loss: 0.1283 (0.1277)  time: 0.0703  data: 0.0001  max mem: 2598
[14:36:55.686616] Epoch: [193]  [194/195]  eta: 0:00:00  lr: 0.000002  loss: 0.1277 (0.1278)  time: 0.0692  data: 0.0001  max mem: 2598
[14:36:55.732730] Epoch: [193] Total time: 0:00:14 (0.0730 s / it)
[14:36:55.732802] Averaged stats: lr: 0.000002  loss: 0.1277 (0.1278)
[14:36:55.735331] log_dir: ./output_dir/mae0.3
[14:36:56.133682] Epoch: [194]  [  0/195]  eta: 0:01:17  lr: 0.000002  loss: 0.1269 (0.1269)  time: 0.3971  data: 0.3143  max mem: 2598
[14:36:57.552995] Epoch: [194]  [ 20/195]  eta: 0:00:15  lr: 0.000002  loss: 0.1259 (0.1276)  time: 0.0709  data: 0.0001  max mem: 2598
[14:36:58.959336] Epoch: [194]  [ 40/195]  eta: 0:00:12  lr: 0.000002  loss: 0.1266 (0.1279)  time: 0.0703  data: 0.0001  max mem: 2598
[14:37:00.373344] Epoch: [194]  [ 60/195]  eta: 0:00:10  lr: 0.000002  loss: 0.1271 (0.1281)  time: 0.0706  data: 0.0002  max mem: 2598
[14:37:01.776241] Epoch: [194]  [ 80/195]  eta: 0:00:08  lr: 0.000002  loss: 0.1278 (0.1282)  time: 0.0701  data: 0.0002  max mem: 2598
[14:37:03.174030] Epoch: [194]  [100/195]  eta: 0:00:06  lr: 0.000001  loss: 0.1273 (0.1282)  time: 0.0699  data: 0.0001  max mem: 2598
[14:37:04.571919] Epoch: [194]  [120/195]  eta: 0:00:05  lr: 0.000001  loss: 0.1296 (0.1284)  time: 0.0699  data: 0.0001  max mem: 2598
[14:37:05.962534] Epoch: [194]  [140/195]  eta: 0:00:03  lr: 0.000001  loss: 0.1281 (0.1283)  time: 0.0695  data: 0.0001  max mem: 2598
[14:37:07.329554] Epoch: [194]  [160/195]  eta: 0:00:02  lr: 0.000001  loss: 0.1275 (0.1284)  time: 0.0683  data: 0.0001  max mem: 2598
[14:37:08.693569] Epoch: [194]  [180/195]  eta: 0:00:01  lr: 0.000001  loss: 0.1274 (0.1284)  time: 0.0682  data: 0.0001  max mem: 2598
[14:37:09.646951] Epoch: [194]  [194/195]  eta: 0:00:00  lr: 0.000001  loss: 0.1261 (0.1284)  time: 0.0680  data: 0.0001  max mem: 2598
[14:37:09.715223] Epoch: [194] Total time: 0:00:13 (0.0717 s / it)
[14:37:09.715295] Averaged stats: lr: 0.000001  loss: 0.1261 (0.1284)
[14:37:09.717526] log_dir: ./output_dir/mae0.3
[14:37:10.154293] Epoch: [195]  [  0/195]  eta: 0:01:24  lr: 0.000001  loss: 0.1355 (0.1355)  time: 0.4352  data: 0.3109  max mem: 2598
[14:37:11.585312] Epoch: [195]  [ 20/195]  eta: 0:00:15  lr: 0.000001  loss: 0.1270 (0.1266)  time: 0.0715  data: 0.0001  max mem: 2598
[14:37:12.942208] Epoch: [195]  [ 40/195]  eta: 0:00:12  lr: 0.000001  loss: 0.1249 (0.1267)  time: 0.0678  data: 0.0001  max mem: 2598
[14:37:14.297172] Epoch: [195]  [ 60/195]  eta: 0:00:10  lr: 0.000001  loss: 0.1271 (0.1270)  time: 0.0677  data: 0.0001  max mem: 2598
[14:37:15.649722] Epoch: [195]  [ 80/195]  eta: 0:00:08  lr: 0.000001  loss: 0.1269 (0.1271)  time: 0.0676  data: 0.0001  max mem: 2598
[14:37:17.005885] Epoch: [195]  [100/195]  eta: 0:00:06  lr: 0.000001  loss: 0.1296 (0.1276)  time: 0.0678  data: 0.0001  max mem: 2598
[14:37:18.360710] Epoch: [195]  [120/195]  eta: 0:00:05  lr: 0.000001  loss: 0.1269 (0.1277)  time: 0.0677  data: 0.0001  max mem: 2598
[14:37:19.717941] Epoch: [195]  [140/195]  eta: 0:00:03  lr: 0.000001  loss: 0.1256 (0.1276)  time: 0.0678  data: 0.0001  max mem: 2598
[14:37:21.077404] Epoch: [195]  [160/195]  eta: 0:00:02  lr: 0.000001  loss: 0.1284 (0.1278)  time: 0.0679  data: 0.0001  max mem: 2598
[14:37:22.434268] Epoch: [195]  [180/195]  eta: 0:00:01  lr: 0.000001  loss: 0.1290 (0.1279)  time: 0.0678  data: 0.0001  max mem: 2598
[14:37:23.380272] Epoch: [195]  [194/195]  eta: 0:00:00  lr: 0.000001  loss: 0.1260 (0.1280)  time: 0.0676  data: 0.0001  max mem: 2598
[14:37:23.428487] Epoch: [195] Total time: 0:00:13 (0.0703 s / it)
[14:37:23.428559] Averaged stats: lr: 0.000001  loss: 0.1260 (0.1280)
[14:37:23.431250] log_dir: ./output_dir/mae0.3
[14:37:23.841961] Epoch: [196]  [  0/195]  eta: 0:01:19  lr: 0.000001  loss: 0.1318 (0.1318)  time: 0.4095  data: 0.2889  max mem: 2598
[14:37:25.270618] Epoch: [196]  [ 20/195]  eta: 0:00:15  lr: 0.000001  loss: 0.1284 (0.1265)  time: 0.0714  data: 0.0001  max mem: 2598
[14:37:26.678459] Epoch: [196]  [ 40/195]  eta: 0:00:12  lr: 0.000001  loss: 0.1248 (0.1264)  time: 0.0703  data: 0.0001  max mem: 2598
[14:37:28.093464] Epoch: [196]  [ 60/195]  eta: 0:00:10  lr: 0.000001  loss: 0.1259 (0.1267)  time: 0.0707  data: 0.0001  max mem: 2598
[14:37:29.506456] Epoch: [196]  [ 80/195]  eta: 0:00:08  lr: 0.000001  loss: 0.1273 (0.1267)  time: 0.0706  data: 0.0002  max mem: 2598
[14:37:30.920884] Epoch: [196]  [100/195]  eta: 0:00:07  lr: 0.000001  loss: 0.1284 (0.1269)  time: 0.0707  data: 0.0002  max mem: 2598
[14:37:32.327788] Epoch: [196]  [120/195]  eta: 0:00:05  lr: 0.000001  loss: 0.1262 (0.1270)  time: 0.0703  data: 0.0002  max mem: 2598
[14:37:33.731185] Epoch: [196]  [140/195]  eta: 0:00:04  lr: 0.000001  loss: 0.1300 (0.1273)  time: 0.0701  data: 0.0001  max mem: 2598
[14:37:35.136354] Epoch: [196]  [160/195]  eta: 0:00:02  lr: 0.000000  loss: 0.1274 (0.1274)  time: 0.0702  data: 0.0002  max mem: 2598
[14:37:36.535478] Epoch: [196]  [180/195]  eta: 0:00:01  lr: 0.000000  loss: 0.1274 (0.1276)  time: 0.0699  data: 0.0002  max mem: 2598
[14:37:37.501721] Epoch: [196]  [194/195]  eta: 0:00:00  lr: 0.000000  loss: 0.1282 (0.1277)  time: 0.0691  data: 0.0001  max mem: 2598
[14:37:37.545421] Epoch: [196] Total time: 0:00:14 (0.0724 s / it)
[14:37:37.545493] Averaged stats: lr: 0.000000  loss: 0.1282 (0.1277)
[14:37:37.547950] log_dir: ./output_dir/mae0.3
[14:37:37.941372] Epoch: [197]  [  0/195]  eta: 0:01:16  lr: 0.000000  loss: 0.1318 (0.1318)  time: 0.3921  data: 0.3047  max mem: 2598
[14:37:39.383180] Epoch: [197]  [ 20/195]  eta: 0:00:15  lr: 0.000000  loss: 0.1254 (0.1269)  time: 0.0720  data: 0.0001  max mem: 2598
[14:37:40.790240] Epoch: [197]  [ 40/195]  eta: 0:00:12  lr: 0.000000  loss: 0.1257 (0.1273)  time: 0.0703  data: 0.0001  max mem: 2598
[14:37:42.200149] Epoch: [197]  [ 60/195]  eta: 0:00:10  lr: 0.000000  loss: 0.1269 (0.1274)  time: 0.0705  data: 0.0001  max mem: 2598
[14:37:43.613530] Epoch: [197]  [ 80/195]  eta: 0:00:08  lr: 0.000000  loss: 0.1282 (0.1275)  time: 0.0706  data: 0.0001  max mem: 2598
[14:37:45.024941] Epoch: [197]  [100/195]  eta: 0:00:07  lr: 0.000000  loss: 0.1263 (0.1273)  time: 0.0705  data: 0.0001  max mem: 2598
[14:37:46.432269] Epoch: [197]  [120/195]  eta: 0:00:05  lr: 0.000000  loss: 0.1284 (0.1276)  time: 0.0703  data: 0.0001  max mem: 2598
[14:37:47.843234] Epoch: [197]  [140/195]  eta: 0:00:04  lr: 0.000000  loss: 0.1265 (0.1275)  time: 0.0705  data: 0.0001  max mem: 2598
[14:37:49.252337] Epoch: [197]  [160/195]  eta: 0:00:02  lr: 0.000000  loss: 0.1265 (0.1276)  time: 0.0704  data: 0.0001  max mem: 2598
[14:37:50.666069] Epoch: [197]  [180/195]  eta: 0:00:01  lr: 0.000000  loss: 0.1283 (0.1276)  time: 0.0706  data: 0.0001  max mem: 2598
[14:37:51.638005] Epoch: [197]  [194/195]  eta: 0:00:00  lr: 0.000000  loss: 0.1242 (0.1275)  time: 0.0695  data: 0.0001  max mem: 2598
[14:37:51.706244] Epoch: [197] Total time: 0:00:14 (0.0726 s / it)
[14:37:51.706313] Averaged stats: lr: 0.000000  loss: 0.1242 (0.1275)
[14:37:51.708805] log_dir: ./output_dir/mae0.3
[14:37:52.093686] Epoch: [198]  [  0/195]  eta: 0:01:14  lr: 0.000000  loss: 0.1170 (0.1170)  time: 0.3839  data: 0.3031  max mem: 2598
[14:37:53.525912] Epoch: [198]  [ 20/195]  eta: 0:00:15  lr: 0.000000  loss: 0.1252 (0.1262)  time: 0.0716  data: 0.0001  max mem: 2598
[14:37:54.934592] Epoch: [198]  [ 40/195]  eta: 0:00:12  lr: 0.000000  loss: 0.1285 (0.1269)  time: 0.0704  data: 0.0001  max mem: 2598
[14:37:56.343797] Epoch: [198]  [ 60/195]  eta: 0:00:10  lr: 0.000000  loss: 0.1284 (0.1268)  time: 0.0704  data: 0.0001  max mem: 2598
[14:37:57.760083] Epoch: [198]  [ 80/195]  eta: 0:00:08  lr: 0.000000  loss: 0.1283 (0.1271)  time: 0.0708  data: 0.0001  max mem: 2598
[14:37:59.169300] Epoch: [198]  [100/195]  eta: 0:00:07  lr: 0.000000  loss: 0.1264 (0.1269)  time: 0.0704  data: 0.0001  max mem: 2598
[14:38:00.579693] Epoch: [198]  [120/195]  eta: 0:00:05  lr: 0.000000  loss: 0.1271 (0.1271)  time: 0.0705  data: 0.0001  max mem: 2598
[14:38:01.985404] Epoch: [198]  [140/195]  eta: 0:00:04  lr: 0.000000  loss: 0.1273 (0.1272)  time: 0.0702  data: 0.0001  max mem: 2598
[14:38:03.375894] Epoch: [198]  [160/195]  eta: 0:00:02  lr: 0.000000  loss: 0.1274 (0.1271)  time: 0.0695  data: 0.0001  max mem: 2598
[14:38:04.744065] Epoch: [198]  [180/195]  eta: 0:00:01  lr: 0.000000  loss: 0.1271 (0.1271)  time: 0.0684  data: 0.0001  max mem: 2598
[14:38:05.693753] Epoch: [198]  [194/195]  eta: 0:00:00  lr: 0.000000  loss: 0.1272 (0.1272)  time: 0.0679  data: 0.0001  max mem: 2598
[14:38:05.741396] Epoch: [198] Total time: 0:00:14 (0.0720 s / it)
[14:38:05.741456] Averaged stats: lr: 0.000000  loss: 0.1272 (0.1272)
[14:38:05.743693] log_dir: ./output_dir/mae0.3
[14:38:06.145911] Epoch: [199]  [  0/195]  eta: 0:01:18  lr: 0.000000  loss: 0.1260 (0.1260)  time: 0.4008  data: 0.2839  max mem: 2598
[14:38:07.586036] Epoch: [199]  [ 20/195]  eta: 0:00:15  lr: 0.000000  loss: 0.1286 (0.1268)  time: 0.0720  data: 0.0002  max mem: 2598
[14:38:08.990898] Epoch: [199]  [ 40/195]  eta: 0:00:12  lr: 0.000000  loss: 0.1255 (0.1266)  time: 0.0702  data: 0.0001  max mem: 2598
[14:38:10.399451] Epoch: [199]  [ 60/195]  eta: 0:00:10  lr: 0.000000  loss: 0.1261 (0.1267)  time: 0.0704  data: 0.0001  max mem: 2598
[14:38:11.800066] Epoch: [199]  [ 80/195]  eta: 0:00:08  lr: 0.000000  loss: 0.1282 (0.1270)  time: 0.0700  data: 0.0001  max mem: 2598
[14:38:13.206621] Epoch: [199]  [100/195]  eta: 0:00:07  lr: 0.000000  loss: 0.1270 (0.1268)  time: 0.0703  data: 0.0001  max mem: 2598
[14:38:14.608243] Epoch: [199]  [120/195]  eta: 0:00:05  lr: 0.000000  loss: 0.1282 (0.1271)  time: 0.0700  data: 0.0001  max mem: 2598
[14:38:16.008868] Epoch: [199]  [140/195]  eta: 0:00:04  lr: 0.000000  loss: 0.1293 (0.1274)  time: 0.0700  data: 0.0001  max mem: 2598
[14:38:17.410980] Epoch: [199]  [160/195]  eta: 0:00:02  lr: 0.000000  loss: 0.1293 (0.1275)  time: 0.0701  data: 0.0001  max mem: 2598
[14:38:18.819370] Epoch: [199]  [180/195]  eta: 0:00:01  lr: 0.000000  loss: 0.1274 (0.1275)  time: 0.0704  data: 0.0001  max mem: 2598
[14:38:19.786070] Epoch: [199]  [194/195]  eta: 0:00:00  lr: 0.000000  loss: 0.1246 (0.1274)  time: 0.0692  data: 0.0001  max mem: 2598
[14:38:19.836091] Epoch: [199] Total time: 0:00:14 (0.0723 s / it)
[14:38:19.836163] Averaged stats: lr: 0.000000  loss: 0.1246 (0.1274)
[14:38:20.021189] Training time 0:46:59
Not using distributed mode
[15:21:27.847832] job dir: /home/wsj/mae
[15:21:27.847874] Namespace(batch_size=256,
epochs=100,
accum_iter=1,
model='deit_tiny',
weight_decay=0.0,
lr=None,
blr=0.1,
min_lr=0.0,
warmup_epochs=10,
finetune='./output_dir/mae_mk0.3/checkpoint-199.pth',
global_pool=False,
data_path='./data',
nb_classes=1000,
output_dir='./output_dir/mae_mk0.3/linear',
log_dir='./output_dir/mae_mk0.3/linear',
device='cuda',
seed=0,
resume='',
start_epoch=0,
eval=False,
dist_eval=True,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:21:28.642343] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:21:28.642452] Dataset CIFAR10
    Number of datapoints: 10000
    Root location: ./data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:21:28.642497] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x705d066be8d0>
[15:21:28.797881] Load pre-trained checkpoint from: ./output_dir/mae_mk0.3/checkpoint-199.pth
[15:21:28.803787] _IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=['mask_token', 'decoder_pos_embed', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])
[15:21:29.205762] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (head): Sequential(
    (0): BatchNorm1d(192, eps=1e-06, momentum=0.1, affine=False, track_running_stats=True)
    (1): Linear(in_features=192, out_features=1000, bias=True)
  )
)
[15:21:29.205783] number of params (M): 0.19
[15:21:29.205792] base lr: 1.00e-01
[15:21:29.205797] actual lr: 1.00e-01
[15:21:29.205800] accumulate grad iterations: 1
[15:21:29.205803] effective batch size: 256
[15:21:29.206029] LARS (
Parameter Group 0
    lr: 0.1
    momentum: 0.9
    trust_coefficient: 0.001
    weight_decay: 0.0
)
[15:21:29.206119] criterion = CrossEntropyLoss()
[15:21:29.206127] Start training for 100 epochs
[15:21:29.206783] log_dir: ./output_dir/mae_mk0.3/linear
Not using distributed mode
[15:21:32.380045] job dir: /home/wsj/mae
[15:21:32.380109] Namespace(batch_size=256,
epochs=100,
accum_iter=1,
model='deit_tiny',
weight_decay=0.0,
lr=None,
blr=0.1,
min_lr=0.0,
warmup_epochs=10,
finetune='./output_dir/mae_mk0.5/checkpoint-199.pth',
global_pool=False,
data_path='./data',
nb_classes=1000,
output_dir='./output_dir/mae_mk0.5/linear',
log_dir='./output_dir/mae_mk0.5/linear',
device='cuda',
seed=0,
resume='',
start_epoch=0,
eval=False,
dist_eval=True,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:21:33.185508] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:21:33.185620] Dataset CIFAR10
    Number of datapoints: 10000
    Root location: ./data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:21:33.185664] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f9a80486690>
[15:21:33.349901] Load pre-trained checkpoint from: ./output_dir/mae_mk0.5/checkpoint-199.pth
[15:21:33.356032] _IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=['mask_token', 'decoder_pos_embed', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])
[15:21:34.217960] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (head): Sequential(
    (0): BatchNorm1d(192, eps=1e-06, momentum=0.1, affine=False, track_running_stats=True)
    (1): Linear(in_features=192, out_features=1000, bias=True)
  )
)
[15:21:34.217981] number of params (M): 0.19
[15:21:34.217991] base lr: 1.00e-01
[15:21:34.217994] actual lr: 1.00e-01
[15:21:34.217998] accumulate grad iterations: 1
[15:21:34.218001] effective batch size: 256
[15:21:34.218212] LARS (
Parameter Group 0
    lr: 0.1
    momentum: 0.9
    trust_coefficient: 0.001
    weight_decay: 0.0
)
[15:21:34.218299] criterion = CrossEntropyLoss()
[15:21:34.218307] Start training for 100 epochs
[15:21:34.218960] log_dir: ./output_dir/mae_mk0.5/linear
Not using distributed mode
[15:21:43.726620] job dir: /home/wsj/mae
[15:21:43.726681] Namespace(batch_size=256,
epochs=100,
accum_iter=1,
model='deit_tiny',
weight_decay=0.0,
lr=None,
blr=0.1,
min_lr=0.0,
warmup_epochs=10,
finetune='./output_dir/mae_mk0.7/checkpoint-199.pth',
global_pool=False,
data_path='./data',
nb_classes=1000,
output_dir='./output_dir/mae_mk0.7/linear',
log_dir='./output_dir/mae_mk0.7/linear',
device='cuda',
seed=0,
resume='',
start_epoch=0,
eval=False,
dist_eval=True,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:21:44.678999] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:21:44.679197] Dataset CIFAR10
    Number of datapoints: 10000
    Root location: ./data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:21:44.679268] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7ef8bbb39910>
[15:21:44.929732] Load pre-trained checkpoint from: ./output_dir/mae_mk0.7/checkpoint-199.pth
[15:21:44.938569] _IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=['mask_token', 'decoder_pos_embed', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])
[15:21:46.239562] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (head): Sequential(
    (0): BatchNorm1d(192, eps=1e-06, momentum=0.1, affine=False, track_running_stats=True)
    (1): Linear(in_features=192, out_features=1000, bias=True)
  )
)
[15:21:46.239593] number of params (M): 0.19
[15:21:46.239604] base lr: 1.00e-01
[15:21:46.239609] actual lr: 1.00e-01
[15:21:46.239613] accumulate grad iterations: 1
[15:21:46.239616] effective batch size: 256
[15:21:46.239957] LARS (
Parameter Group 0
    lr: 0.1
    momentum: 0.9
    trust_coefficient: 0.001
    weight_decay: 0.0
)
[15:21:46.240080] criterion = CrossEntropyLoss()
[15:21:46.240090] Start training for 100 epochs
[15:21:46.240960] log_dir: ./output_dir/mae_mk0.7/linear
Not using distributed mode
[15:21:48.711521] job dir: /home/wsj/mae
[15:21:48.711584] Namespace(batch_size=256,
epochs=100,
accum_iter=1,
model='deit_tiny',
weight_decay=0.0,
lr=None,
blr=0.1,
min_lr=0.0,
warmup_epochs=10,
finetune='./output_dir/mae_mk0.9/checkpoint-199.pth',
global_pool=False,
data_path='./data',
nb_classes=1000,
output_dir='./output_dir/mae_mk0.9/linear',
log_dir='./output_dir/mae_mk0.9/linear',
device='cuda',
seed=0,
resume='',
start_epoch=0,
eval=False,
dist_eval=True,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:21:49.747049] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:21:49.747263] Dataset CIFAR10
    Number of datapoints: 10000
    Root location: ./data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:21:49.747335] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x74165c331910>
[15:21:49.970147] Load pre-trained checkpoint from: ./output_dir/mae_mk0.9/checkpoint-199.pth
[15:21:49.979396] _IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=['mask_token', 'decoder_pos_embed', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])
[15:21:51.704989] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (head): Sequential(
    (0): BatchNorm1d(192, eps=1e-06, momentum=0.1, affine=False, track_running_stats=True)
    (1): Linear(in_features=192, out_features=1000, bias=True)
  )
)
[15:21:51.705021] number of params (M): 0.19
[15:21:51.705039] base lr: 1.00e-01
[15:21:51.705047] actual lr: 1.00e-01
[15:21:51.705051] accumulate grad iterations: 1
[15:21:51.705054] effective batch size: 256
[15:21:51.705622] LARS (
Parameter Group 0
    lr: 0.1
    momentum: 0.9
    trust_coefficient: 0.001
    weight_decay: 0.0
)
[15:21:51.705825] criterion = CrossEntropyLoss()
[15:21:51.705839] Start training for 100 epochs
[15:21:51.706915] log_dir: ./output_dir/mae_mk0.9/linear
[15:21:30.110436] Epoch: [0]  [  0/195]  eta: 0:02:55  lr: 0.000000  loss: 6.9211 (6.9211)  time: 0.9024  data: 0.3149  max mem: 118
[15:21:31.658039] Epoch: [0]  [ 20/195]  eta: 0:00:20  lr: 0.001026  loss: 6.9181 (6.9204)  time: 0.0773  data: 0.0001  max mem: 127
[15:21:33.207581] Epoch: [0]  [ 40/195]  eta: 0:00:15  lr: 0.002051  loss: 6.8981 (6.9103)  time: 0.0774  data: 0.0001  max mem: 127
[15:21:35.365978] Epoch: [0]  [ 60/195]  eta: 0:00:13  lr: 0.003077  loss: 6.8543 (6.8931)  time: 0.1079  data: 0.0001  max mem: 127
[15:21:40.221153] Epoch: [0]  [ 80/195]  eta: 0:00:15  lr: 0.004103  loss: 6.7953 (6.8683)  time: 0.2427  data: 0.0001  max mem: 127
[15:21:45.082944] Epoch: [0]  [100/195]  eta: 0:00:14  lr: 0.005128  loss: 6.7080 (6.8369)  time: 0.2431  data: 0.0001  max mem: 127
[15:21:51.599938] Epoch: [0]  [120/195]  eta: 0:00:13  lr: 0.006154  loss: 6.6039 (6.7985)  time: 0.3258  data: 0.0001  max mem: 127
[15:22:00.908087] Epoch: [0]  [140/195]  eta: 0:00:12  lr: 0.007179  loss: 6.4744 (6.7530)  time: 0.4654  data: 0.0001  max mem: 127
[15:22:10.649737] Epoch: [0]  [160/195]  eta: 0:00:09  lr: 0.008205  loss: 6.3303 (6.7000)  time: 0.4870  data: 0.0001  max mem: 127
[15:22:20.389596] Epoch: [0]  [180/195]  eta: 0:00:04  lr: 0.009231  loss: 6.1537 (6.6406)  time: 0.4870  data: 0.0001  max mem: 127
[15:22:27.199799] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.009949  loss: 6.0331 (6.5949)  time: 0.4869  data: 0.0001  max mem: 127
[15:22:27.261043] Epoch: [0] Total time: 0:00:58 (0.2977 s / it)
[15:22:27.261129] Averaged stats: lr: 0.009949  loss: 6.0331 (6.5949)
[15:22:28.133753] Test:  [ 0/40]  eta: 0:00:32  loss: 5.9577 (5.9577)  acc1: 10.1562 (10.1562)  acc5: 62.1094 (62.1094)  time: 0.8225  data: 0.3098  max mem: 127
[15:22:32.811229] Test:  [10/40]  eta: 0:00:14  loss: 5.9550 (5.9560)  acc1: 10.1562 (9.8366)  acc5: 62.8906 (62.3580)  time: 0.4999  data: 0.0283  max mem: 127
[15:22:37.497318] Test:  [20/40]  eta: 0:00:09  loss: 5.9512 (5.9527)  acc1: 10.1562 (10.3609)  acc5: 62.5000 (62.5558)  time: 0.4681  data: 0.0001  max mem: 127
[15:22:42.176203] Test:  [30/40]  eta: 0:00:04  loss: 5.9507 (5.9515)  acc1: 10.5469 (10.4335)  acc5: 62.1094 (62.7268)  time: 0.4682  data: 0.0001  max mem: 127
[15:22:46.202820] Test:  [39/40]  eta: 0:00:00  loss: 5.9523 (5.9520)  acc1: 10.5469 (10.3900)  acc5: 61.3281 (62.3800)  time: 0.4587  data: 0.0001  max mem: 127
[15:22:46.275913] Test: Total time: 0:00:18 (0.4741 s / it)
[15:22:46.275985] * Acc@1 10.390 Acc@5 62.380 loss 5.952
[15:22:46.276163] Accuracy of the network on the 10000 test images: 10.4%
[15:22:46.276175] Max accuracy: 10.39%
[15:22:46.278934] log_dir: ./output_dir/mae_mk0.3/linear
[15:21:35.339281] Epoch: [0]  [  0/195]  eta: 0:03:38  lr: 0.000000  loss: 6.9224 (6.9224)  time: 1.1192  data: 0.3003  max mem: 118
[15:21:40.193225] Epoch: [0]  [ 20/195]  eta: 0:00:49  lr: 0.001026  loss: 6.9229 (6.9245)  time: 0.2427  data: 0.0001  max mem: 127
[15:21:45.055861] Epoch: [0]  [ 40/195]  eta: 0:00:40  lr: 0.002051  loss: 6.9032 (6.9148)  time: 0.2431  data: 0.0001  max mem: 127
[15:21:51.490249] Epoch: [0]  [ 60/195]  eta: 0:00:38  lr: 0.003077  loss: 6.8606 (6.8976)  time: 0.3217  data: 0.0001  max mem: 127
[15:22:00.890621] Epoch: [0]  [ 80/195]  eta: 0:00:37  lr: 0.004103  loss: 6.7981 (6.8729)  time: 0.4700  data: 0.0001  max mem: 127
[15:22:10.601956] Epoch: [0]  [100/195]  eta: 0:00:34  lr: 0.005128  loss: 6.7118 (6.8413)  time: 0.4855  data: 0.0001  max mem: 127
[15:22:20.339425] Epoch: [0]  [120/195]  eta: 0:00:28  lr: 0.006154  loss: 6.6099 (6.8025)  time: 0.4868  data: 0.0001  max mem: 127
[15:22:29.918120] Epoch: [0]  [140/195]  eta: 0:00:21  lr: 0.007179  loss: 6.4841 (6.7568)  time: 0.4789  data: 0.0001  max mem: 127
[15:22:39.741154] Epoch: [0]  [160/195]  eta: 0:00:14  lr: 0.008205  loss: 6.3360 (6.7045)  time: 0.4911  data: 0.0001  max mem: 127
[15:22:49.317110] Epoch: [0]  [180/195]  eta: 0:00:06  lr: 0.009231  loss: 6.1573 (6.6451)  time: 0.4788  data: 0.0001  max mem: 127
[15:22:56.121137] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.009949  loss: 6.0270 (6.5997)  time: 0.4827  data: 0.0001  max mem: 127
[15:22:56.180774] Epoch: [0] Total time: 0:01:21 (0.4203 s / it)
[15:22:56.180838] Averaged stats: lr: 0.009949  loss: 6.0270 (6.5997)
[15:22:56.941953] Test:  [ 0/40]  eta: 0:00:28  loss: 5.9220 (5.9220)  acc1: 14.4531 (14.4531)  acc5: 60.1562 (60.1562)  time: 0.7237  data: 0.2032  max mem: 127
[15:23:01.623736] Test:  [10/40]  eta: 0:00:14  loss: 5.9228 (5.9240)  acc1: 14.4531 (14.5952)  acc5: 57.8125 (58.4517)  time: 0.4914  data: 0.0186  max mem: 127
[15:23:06.295329] Test:  [20/40]  eta: 0:00:09  loss: 5.9187 (5.9212)  acc1: 15.6250 (15.6808)  acc5: 57.8125 (58.6868)  time: 0.4676  data: 0.0001  max mem: 127
[15:23:10.970280] Test:  [30/40]  eta: 0:00:04  loss: 5.9186 (5.9210)  acc1: 15.6250 (15.4990)  acc5: 59.7656 (59.2364)  time: 0.4673  data: 0.0001  max mem: 127
[15:23:14.992254] Test:  [39/40]  eta: 0:00:00  loss: 5.9250 (5.9242)  acc1: 14.8438 (15.2500)  acc5: 58.5938 (58.3700)  time: 0.4582  data: 0.0001  max mem: 127
[15:23:15.045217] Test: Total time: 0:00:18 (0.4707 s / it)
[15:23:15.045268] * Acc@1 15.250 Acc@5 58.370 loss 5.924
[15:23:15.045384] Accuracy of the network on the 10000 test images: 15.2%
[15:23:15.045391] Max accuracy: 15.25%
[15:23:15.047355] log_dir: ./output_dir/mae_mk0.5/linear
[15:21:47.754501] Epoch: [0]  [  0/195]  eta: 0:04:54  lr: 0.000000  loss: 6.9304 (6.9304)  time: 1.5126  data: 0.3281  max mem: 118
[15:21:56.000578] Epoch: [0]  [ 20/195]  eta: 0:01:21  lr: 0.001026  loss: 6.9222 (6.9233)  time: 0.4123  data: 0.0001  max mem: 127
[15:22:05.749791] Epoch: [0]  [ 40/195]  eta: 0:01:13  lr: 0.002051  loss: 6.9026 (6.9124)  time: 0.4874  data: 0.0001  max mem: 127
[15:22:15.465157] Epoch: [0]  [ 60/195]  eta: 0:01:04  lr: 0.003077  loss: 6.8580 (6.8962)  time: 0.4857  data: 0.0001  max mem: 127
[15:22:25.200659] Epoch: [0]  [ 80/195]  eta: 0:00:55  lr: 0.004103  loss: 6.7976 (6.8718)  time: 0.4867  data: 0.0001  max mem: 127
[15:22:34.835243] Epoch: [0]  [100/195]  eta: 0:00:45  lr: 0.005128  loss: 6.7121 (6.8405)  time: 0.4817  data: 0.0001  max mem: 127
[15:22:44.668939] Epoch: [0]  [120/195]  eta: 0:00:36  lr: 0.006154  loss: 6.5980 (6.8015)  time: 0.4917  data: 0.0001  max mem: 127
[15:22:54.217663] Epoch: [0]  [140/195]  eta: 0:00:26  lr: 0.007179  loss: 6.4808 (6.7553)  time: 0.4774  data: 0.0001  max mem: 127
[15:23:03.912382] Epoch: [0]  [160/195]  eta: 0:00:16  lr: 0.008205  loss: 6.3262 (6.7025)  time: 0.4847  data: 0.0001  max mem: 127
[15:23:13.736153] Epoch: [0]  [180/195]  eta: 0:00:07  lr: 0.009231  loss: 6.1645 (6.6428)  time: 0.4911  data: 0.0001  max mem: 127
[15:23:20.408151] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.009949  loss: 6.0260 (6.5970)  time: 0.4810  data: 0.0001  max mem: 127
[15:23:20.440885] Epoch: [0] Total time: 0:01:34 (0.4831 s / it)
[15:23:20.440956] Averaged stats: lr: 0.009949  loss: 6.0260 (6.5970)
[15:21:53.373379] Epoch: [0]  [  0/195]  eta: 0:05:24  lr: 0.000000  loss: 6.8947 (6.8947)  time: 1.6654  data: 0.3147  max mem: 118
[15:22:03.091759] Epoch: [0]  [ 20/195]  eta: 0:01:34  lr: 0.001026  loss: 6.8908 (6.8918)  time: 0.4859  data: 0.0001  max mem: 127
[15:22:12.817009] Epoch: [0]  [ 40/195]  eta: 0:01:19  lr: 0.002051  loss: 6.8705 (6.8806)  time: 0.4862  data: 0.0001  max mem: 127
[15:22:22.532723] Epoch: [0]  [ 60/195]  eta: 0:01:08  lr: 0.003077  loss: 6.8184 (6.8621)  time: 0.4858  data: 0.0001  max mem: 127
[15:22:32.124045] Epoch: [0]  [ 80/195]  eta: 0:00:57  lr: 0.004103  loss: 6.7634 (6.8377)  time: 0.4795  data: 0.0001  max mem: 127
[15:22:41.958717] Epoch: [0]  [100/195]  eta: 0:00:47  lr: 0.005128  loss: 6.6753 (6.8058)  time: 0.4917  data: 0.0001  max mem: 127
[15:22:51.509150] Epoch: [0]  [120/195]  eta: 0:00:37  lr: 0.006154  loss: 6.5607 (6.7663)  time: 0.4775  data: 0.0001  max mem: 127
[15:23:01.177676] Epoch: [0]  [140/195]  eta: 0:00:27  lr: 0.007179  loss: 6.4365 (6.7191)  time: 0.4834  data: 0.0001  max mem: 127
[15:23:10.995869] Epoch: [0]  [160/195]  eta: 0:00:17  lr: 0.008205  loss: 6.2735 (6.6648)  time: 0.4909  data: 0.0001  max mem: 127
[15:23:20.555532] Epoch: [0]  [180/195]  eta: 0:00:07  lr: 0.009231  loss: 6.1045 (6.6037)  time: 0.4779  data: 0.0001  max mem: 127
[15:23:27.364926] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.009949  loss: 5.9773 (6.5566)  time: 0.4840  data: 0.0001  max mem: 127
[15:23:27.399592] Epoch: [0] Total time: 0:01:35 (0.4907 s / it)
[15:23:27.399735] Averaged stats: lr: 0.009949  loss: 5.9773 (6.5566)
[15:23:21.153772] Test:  [ 0/40]  eta: 0:00:26  loss: 5.9402 (5.9402)  acc1: 19.9219 (19.9219)  acc5: 55.4688 (55.4688)  time: 0.6747  data: 0.1570  max mem: 127
[15:23:25.837786] Test:  [10/40]  eta: 0:00:14  loss: 5.9402 (5.9424)  acc1: 16.0156 (17.4006)  acc5: 56.6406 (57.3509)  time: 0.4871  data: 0.0144  max mem: 127
[15:23:30.448987] Test:  [20/40]  eta: 0:00:09  loss: 5.9426 (5.9429)  acc1: 16.0156 (17.2619)  acc5: 57.0312 (57.2359)  time: 0.4647  data: 0.0001  max mem: 127
[15:23:35.175196] Test:  [30/40]  eta: 0:00:04  loss: 5.9407 (5.9412)  acc1: 17.1875 (17.4143)  acc5: 57.8125 (57.7999)  time: 0.4668  data: 0.0001  max mem: 127
[15:23:39.243130] Test:  [39/40]  eta: 0:00:00  loss: 5.9449 (5.9439)  acc1: 16.4062 (17.0400)  acc5: 58.2031 (57.7900)  time: 0.4632  data: 0.0001  max mem: 127
[15:23:39.295526] Test: Total time: 0:00:18 (0.4704 s / it)
[15:23:39.295574] * Acc@1 17.040 Acc@5 57.790 loss 5.944
[15:23:39.295673] Accuracy of the network on the 10000 test images: 17.0%
[15:23:39.295681] Max accuracy: 17.04%
[15:23:39.297267] log_dir: ./output_dir/mae_mk0.7/linear
[15:23:28.108346] Test:  [ 0/40]  eta: 0:00:26  loss: 5.8454 (5.8454)  acc1: 27.7344 (27.7344)  acc5: 81.2500 (81.2500)  time: 0.6702  data: 0.1448  max mem: 127
[15:23:32.820831] Test:  [10/40]  eta: 0:00:14  loss: 5.8437 (5.8436)  acc1: 25.7812 (26.3849)  acc5: 81.2500 (82.2088)  time: 0.4893  data: 0.0133  max mem: 127
[15:23:37.560656] Test:  [20/40]  eta: 0:00:09  loss: 5.8396 (5.8413)  acc1: 26.1719 (26.8415)  acc5: 80.8594 (81.9382)  time: 0.4726  data: 0.0001  max mem: 127
[15:23:42.108616] Test:  [30/40]  eta: 0:00:04  loss: 5.8389 (5.8413)  acc1: 26.9531 (27.0035)  acc5: 81.2500 (81.7288)  time: 0.4643  data: 0.0001  max mem: 127
[15:23:46.143444] Test:  [39/40]  eta: 0:00:00  loss: 5.8456 (5.8428)  acc1: 26.5625 (26.9700)  acc5: 80.4688 (81.5300)  time: 0.4527  data: 0.0001  max mem: 127
[15:23:46.210567] Test: Total time: 0:00:18 (0.4693 s / it)
[15:23:46.210612] * Acc@1 26.970 Acc@5 81.530 loss 5.843
[15:23:46.210830] Accuracy of the network on the 10000 test images: 27.0%
[15:23:46.210838] Max accuracy: 26.97%
[15:23:46.212655] log_dir: ./output_dir/mae_mk0.9/linear
[15:22:47.123640] Epoch: [1]  [  0/195]  eta: 0:02:44  lr: 0.010000  loss: 5.9318 (5.9318)  time: 0.8436  data: 0.3585  max mem: 127
[15:22:56.740040] Epoch: [1]  [ 20/195]  eta: 0:01:27  lr: 0.011026  loss: 5.8105 (5.8242)  time: 0.4808  data: 0.0001  max mem: 127
[15:23:06.549130] Epoch: [1]  [ 40/195]  eta: 0:01:16  lr: 0.012051  loss: 5.5977 (5.7115)  time: 0.4904  data: 0.0001  max mem: 127
[15:23:16.185855] Epoch: [1]  [ 60/195]  eta: 0:01:06  lr: 0.013077  loss: 5.3515 (5.5959)  time: 0.4818  data: 0.0001  max mem: 127
[15:23:25.863287] Epoch: [1]  [ 80/195]  eta: 0:00:56  lr: 0.014103  loss: 5.1004 (5.4749)  time: 0.4838  data: 0.0001  max mem: 127
[15:23:35.662951] Epoch: [1]  [100/195]  eta: 0:00:46  lr: 0.015128  loss: 4.8388 (5.3505)  time: 0.4899  data: 0.0001  max mem: 127
[15:23:45.343141] Epoch: [1]  [120/195]  eta: 0:00:36  lr: 0.016154  loss: 4.5617 (5.2215)  time: 0.4840  data: 0.0001  max mem: 127
[15:23:54.917880] Epoch: [1]  [140/195]  eta: 0:00:26  lr: 0.017179  loss: 4.2890 (5.0894)  time: 0.4787  data: 0.0001  max mem: 127
[15:24:04.642767] Epoch: [1]  [160/195]  eta: 0:00:17  lr: 0.018205  loss: 4.0093 (4.9553)  time: 0.4862  data: 0.0001  max mem: 127
[15:24:14.378966] Epoch: [1]  [180/195]  eta: 0:00:07  lr: 0.019231  loss: 3.7336 (4.8214)  time: 0.4868  data: 0.0001  max mem: 127
[15:24:21.186400] Epoch: [1]  [194/195]  eta: 0:00:00  lr: 0.019949  loss: 3.5678 (4.7287)  time: 0.4863  data: 0.0001  max mem: 127
[15:24:21.273442] Epoch: [1] Total time: 0:01:34 (0.4872 s / it)
[15:24:21.274977] Averaged stats: lr: 0.019949  loss: 3.5678 (4.7287)
[15:24:22.078363] Test:  [ 0/40]  eta: 0:00:30  loss: 3.4570 (3.4570)  acc1: 16.4062 (16.4062)  acc5: 73.4375 (73.4375)  time: 0.7533  data: 0.2932  max mem: 127
[15:24:26.748985] Test:  [10/40]  eta: 0:00:14  loss: 3.4550 (3.4500)  acc1: 18.7500 (19.4957)  acc5: 75.3906 (75.9588)  time: 0.4930  data: 0.0268  max mem: 127
[15:24:31.426976] Test:  [20/40]  eta: 0:00:09  loss: 3.4347 (3.4432)  acc1: 19.9219 (20.4799)  acc5: 76.1719 (76.1905)  time: 0.4674  data: 0.0001  max mem: 127
[15:24:36.118819] Test:  [30/40]  eta: 0:00:04  loss: 3.4362 (3.4435)  acc1: 20.7031 (20.7031)  acc5: 76.5625 (76.5373)  time: 0.4684  data: 0.0001  max mem: 127
[15:24:39.935132] Test:  [39/40]  eta: 0:00:00  loss: 3.4447 (3.4446)  acc1: 20.3125 (20.4200)  acc5: 75.7812 (76.1000)  time: 0.4487  data: 0.0001  max mem: 127
[15:24:39.987109] Test: Total time: 0:00:18 (0.4666 s / it)
[15:24:39.987173] * Acc@1 20.420 Acc@5 76.100 loss 3.445
[15:24:39.987307] Accuracy of the network on the 10000 test images: 20.4%
[15:24:39.987314] Max accuracy: 20.42%
[15:24:39.989295] log_dir: ./output_dir/mae_mk0.3/linear
[15:23:15.800539] Epoch: [1]  [  0/195]  eta: 0:02:26  lr: 0.010000  loss: 5.9431 (5.9431)  time: 0.7523  data: 0.2680  max mem: 127
[15:23:25.461278] Epoch: [1]  [ 20/195]  eta: 0:01:26  lr: 0.011026  loss: 5.8133 (5.8310)  time: 0.4830  data: 0.0001  max mem: 127
[15:23:35.250087] Epoch: [1]  [ 40/195]  eta: 0:01:16  lr: 0.012051  loss: 5.6078 (5.7204)  time: 0.4894  data: 0.0001  max mem: 127
[15:23:44.963401] Epoch: [1]  [ 60/195]  eta: 0:01:06  lr: 0.013077  loss: 5.3746 (5.6053)  time: 0.4856  data: 0.0001  max mem: 127
[15:23:54.522237] Epoch: [1]  [ 80/195]  eta: 0:00:56  lr: 0.014103  loss: 5.1111 (5.4846)  time: 0.4779  data: 0.0001  max mem: 127
[15:24:04.259735] Epoch: [1]  [100/195]  eta: 0:00:46  lr: 0.015128  loss: 4.8503 (5.3597)  time: 0.4868  data: 0.0001  max mem: 127
[15:24:14.007375] Epoch: [1]  [120/195]  eta: 0:00:36  lr: 0.016154  loss: 4.5745 (5.2312)  time: 0.4873  data: 0.0001  max mem: 127
[15:24:23.617202] Epoch: [1]  [140/195]  eta: 0:00:26  lr: 0.017179  loss: 4.2882 (5.0994)  time: 0.4805  data: 0.0001  max mem: 127
[15:24:33.427996] Epoch: [1]  [160/195]  eta: 0:00:17  lr: 0.018205  loss: 4.0214 (4.9660)  time: 0.4905  data: 0.0001  max mem: 127
[15:24:43.055526] Epoch: [1]  [180/195]  eta: 0:00:07  lr: 0.019231  loss: 3.7471 (4.8326)  time: 0.4813  data: 0.0001  max mem: 127
[15:24:49.860613] Epoch: [1]  [194/195]  eta: 0:00:00  lr: 0.019949  loss: 3.5765 (4.7401)  time: 0.4833  data: 0.0001  max mem: 127
[15:24:49.941404] Epoch: [1] Total time: 0:01:34 (0.4866 s / it)
[15:24:49.941474] Averaged stats: lr: 0.019949  loss: 3.5765 (4.7401)
[15:24:50.629451] Test:  [ 0/40]  eta: 0:00:26  loss: 3.4254 (3.4254)  acc1: 20.7031 (20.7031)  acc5: 80.0781 (80.0781)  time: 0.6503  data: 0.1914  max mem: 127
[15:24:55.300984] Test:  [10/40]  eta: 0:00:14  loss: 3.4254 (3.4274)  acc1: 22.6562 (22.9403)  acc5: 77.3438 (77.1662)  time: 0.4838  data: 0.0175  max mem: 127
[15:24:59.987067] Test:  [20/40]  eta: 0:00:09  loss: 3.4166 (3.4218)  acc1: 23.8281 (23.6607)  acc5: 77.7344 (77.7716)  time: 0.4678  data: 0.0001  max mem: 127
[15:25:04.666064] Test:  [30/40]  eta: 0:00:04  loss: 3.4254 (3.4230)  acc1: 24.2188 (23.7147)  acc5: 77.7344 (77.5076)  time: 0.4682  data: 0.0001  max mem: 127
[15:25:08.486524] Test:  [39/40]  eta: 0:00:00  loss: 3.4285 (3.4271)  acc1: 23.0469 (23.2400)  acc5: 75.7812 (77.1500)  time: 0.4489  data: 0.0001  max mem: 127
[15:25:08.534218] Test: Total time: 0:00:18 (0.4639 s / it)
[15:25:08.534283] * Acc@1 23.240 Acc@5 77.150 loss 3.427
[15:25:08.534803] Accuracy of the network on the 10000 test images: 23.2%
[15:25:08.534812] Max accuracy: 23.24%
[15:25:08.536705] log_dir: ./output_dir/mae_mk0.5/linear
[15:23:40.001916] Epoch: [1]  [  0/195]  eta: 0:02:17  lr: 0.010000  loss: 5.9335 (5.9335)  time: 0.7038  data: 0.2185  max mem: 127
[15:23:49.621014] Epoch: [1]  [ 20/195]  eta: 0:01:26  lr: 0.011026  loss: 5.8095 (5.8258)  time: 0.4809  data: 0.0001  max mem: 127
[15:23:59.314000] Epoch: [1]  [ 40/195]  eta: 0:01:15  lr: 0.012051  loss: 5.5850 (5.7141)  time: 0.4846  data: 0.0001  max mem: 127
[15:24:09.047250] Epoch: [1]  [ 60/195]  eta: 0:01:05  lr: 0.013077  loss: 5.3491 (5.5986)  time: 0.4866  data: 0.0001  max mem: 127
[15:24:18.767596] Epoch: [1]  [ 80/195]  eta: 0:00:56  lr: 0.014103  loss: 5.1033 (5.4770)  time: 0.4860  data: 0.0001  max mem: 127
[15:24:28.405288] Epoch: [1]  [100/195]  eta: 0:00:46  lr: 0.015128  loss: 4.8374 (5.3521)  time: 0.4819  data: 0.0001  max mem: 127
[15:24:38.212158] Epoch: [1]  [120/195]  eta: 0:00:36  lr: 0.016154  loss: 4.5533 (5.2222)  time: 0.4903  data: 0.0001  max mem: 127
[15:24:47.796233] Epoch: [1]  [140/195]  eta: 0:00:26  lr: 0.017179  loss: 4.2881 (5.0892)  time: 0.4792  data: 0.0001  max mem: 127
[15:24:57.464947] Epoch: [1]  [160/195]  eta: 0:00:16  lr: 0.018205  loss: 3.9944 (4.9547)  time: 0.4834  data: 0.0001  max mem: 127
[15:25:07.266325] Epoch: [1]  [180/195]  eta: 0:00:07  lr: 0.019231  loss: 3.7141 (4.8202)  time: 0.4900  data: 0.0001  max mem: 127
[15:25:13.991420] Epoch: [1]  [194/195]  eta: 0:00:00  lr: 0.019949  loss: 3.5555 (4.7271)  time: 0.4830  data: 0.0001  max mem: 127
[15:25:14.039436] Epoch: [1] Total time: 0:01:34 (0.4859 s / it)
[15:25:14.039615] Averaged stats: lr: 0.019949  loss: 3.5555 (4.7271)
[15:23:46.940684] Epoch: [1]  [  0/195]  eta: 0:02:21  lr: 0.010000  loss: 5.8774 (5.8774)  time: 0.7270  data: 0.2397  max mem: 127
[15:23:56.680886] Epoch: [1]  [ 20/195]  eta: 0:01:27  lr: 0.011026  loss: 5.7513 (5.7635)  time: 0.4870  data: 0.0001  max mem: 127
[15:24:06.415687] Epoch: [1]  [ 40/195]  eta: 0:01:16  lr: 0.012051  loss: 5.5205 (5.6476)  time: 0.4867  data: 0.0001  max mem: 127
[15:24:16.162847] Epoch: [1]  [ 60/195]  eta: 0:01:06  lr: 0.013077  loss: 5.2764 (5.5278)  time: 0.4873  data: 0.0001  max mem: 127
[15:24:25.776697] Epoch: [1]  [ 80/195]  eta: 0:00:56  lr: 0.014103  loss: 5.0140 (5.4036)  time: 0.4807  data: 0.0001  max mem: 127
[15:24:35.566446] Epoch: [1]  [100/195]  eta: 0:00:46  lr: 0.015128  loss: 4.7532 (5.2744)  time: 0.4895  data: 0.0001  max mem: 127
[15:24:45.190070] Epoch: [1]  [120/195]  eta: 0:00:36  lr: 0.016154  loss: 4.4555 (5.1414)  time: 0.4811  data: 0.0001  max mem: 127
[15:24:54.834141] Epoch: [1]  [140/195]  eta: 0:00:26  lr: 0.017179  loss: 4.1884 (5.0047)  time: 0.4822  data: 0.0001  max mem: 127
[15:25:04.626857] Epoch: [1]  [160/195]  eta: 0:00:17  lr: 0.018205  loss: 3.8637 (4.8656)  time: 0.4896  data: 0.0001  max mem: 127
[15:25:14.210064] Epoch: [1]  [180/195]  eta: 0:00:07  lr: 0.019231  loss: 3.5897 (4.7273)  time: 0.4791  data: 0.0001  max mem: 127
[15:25:21.047461] Epoch: [1]  [194/195]  eta: 0:00:00  lr: 0.019949  loss: 3.4341 (4.6310)  time: 0.4843  data: 0.0001  max mem: 127
[15:25:21.117348] Epoch: [1] Total time: 0:01:34 (0.4867 s / it)
[15:25:21.117431] Averaged stats: lr: 0.019949  loss: 3.4341 (4.6310)
[15:25:14.706041] Test:  [ 0/40]  eta: 0:00:25  loss: 3.4115 (3.4115)  acc1: 26.1719 (26.1719)  acc5: 80.8594 (80.8594)  time: 0.6295  data: 0.1624  max mem: 127
[15:25:19.393607] Test:  [10/40]  eta: 0:00:14  loss: 3.4115 (3.4146)  acc1: 26.1719 (25.7812)  acc5: 79.6875 (79.2614)  time: 0.4833  data: 0.0149  max mem: 127
[15:25:23.988599] Test:  [20/40]  eta: 0:00:09  loss: 3.4095 (3.4135)  acc1: 26.5625 (25.8929)  acc5: 79.2969 (79.1853)  time: 0.4641  data: 0.0001  max mem: 127
[15:25:28.706244] Test:  [30/40]  eta: 0:00:04  loss: 3.4112 (3.4134)  acc1: 26.5625 (26.0963)  acc5: 79.6875 (79.5237)  time: 0.4656  data: 0.0001  max mem: 127
[15:25:32.554281] Test:  [39/40]  eta: 0:00:00  loss: 3.4225 (3.4166)  acc1: 25.0000 (25.5600)  acc5: 80.0781 (79.4600)  time: 0.4518  data: 0.0001  max mem: 127
[15:25:32.602307] Test: Total time: 0:00:18 (0.4632 s / it)
[15:25:32.602360] * Acc@1 25.560 Acc@5 79.460 loss 3.417
[15:25:32.602467] Accuracy of the network on the 10000 test images: 25.6%
[15:25:32.602474] Max accuracy: 25.56%
[15:25:32.604383] log_dir: ./output_dir/mae_mk0.7/linear
[15:25:21.815517] Test:  [ 0/40]  eta: 0:00:25  loss: 3.2196 (3.2196)  acc1: 45.3125 (45.3125)  acc5: 95.3125 (95.3125)  time: 0.6439  data: 0.1720  max mem: 127
[15:25:26.527949] Test:  [10/40]  eta: 0:00:14  loss: 3.2208 (3.2187)  acc1: 43.3594 (44.6023)  acc5: 93.7500 (93.3594)  time: 0.4869  data: 0.0158  max mem: 127
[15:25:31.258182] Test:  [20/40]  eta: 0:00:09  loss: 3.2166 (3.2187)  acc1: 44.1406 (44.9405)  acc5: 93.3594 (93.3408)  time: 0.4721  data: 0.0001  max mem: 127
[15:25:35.853495] Test:  [30/40]  eta: 0:00:04  loss: 3.2161 (3.2202)  acc1: 44.5312 (45.2243)  acc5: 92.9688 (93.1956)  time: 0.4662  data: 0.0001  max mem: 127
[15:25:39.660508] Test:  [39/40]  eta: 0:00:00  loss: 3.2292 (3.2230)  acc1: 43.7500 (44.9700)  acc5: 92.9688 (93.1600)  time: 0.4437  data: 0.0001  max mem: 127
[15:25:39.706016] Test: Total time: 0:00:18 (0.4634 s / it)
[15:25:39.706065] * Acc@1 44.970 Acc@5 93.160 loss 3.223
[15:25:39.706158] Accuracy of the network on the 10000 test images: 45.0%
[15:25:39.706165] Max accuracy: 44.97%
[15:25:39.707606] log_dir: ./output_dir/mae_mk0.9/linear
[15:24:40.825242] Epoch: [2]  [  0/195]  eta: 0:02:42  lr: 0.020000  loss: 3.4463 (3.4463)  time: 0.8347  data: 0.3564  max mem: 127
[15:24:50.406563] Epoch: [2]  [ 20/195]  eta: 0:01:26  lr: 0.021026  loss: 3.3125 (3.3221)  time: 0.4790  data: 0.0001  max mem: 127
[15:25:00.205837] Epoch: [2]  [ 40/195]  eta: 0:01:16  lr: 0.022051  loss: 3.0717 (3.2119)  time: 0.4899  data: 0.0001  max mem: 127
[15:25:09.909025] Epoch: [2]  [ 60/195]  eta: 0:01:06  lr: 0.023077  loss: 2.9062 (3.1162)  time: 0.4851  data: 0.0001  max mem: 127
[15:25:19.588101] Epoch: [2]  [ 80/195]  eta: 0:00:56  lr: 0.024103  loss: 2.7681 (3.0318)  time: 0.4839  data: 0.0001  max mem: 127
[15:25:29.391834] Epoch: [2]  [100/195]  eta: 0:00:46  lr: 0.025128  loss: 2.6489 (2.9568)  time: 0.4902  data: 0.0001  max mem: 127
[15:25:39.125201] Epoch: [2]  [120/195]  eta: 0:00:36  lr: 0.026154  loss: 2.5606 (2.8903)  time: 0.4866  data: 0.0001  max mem: 127
[15:25:48.740096] Epoch: [2]  [140/195]  eta: 0:00:26  lr: 0.027179  loss: 2.4633 (2.8299)  time: 0.4807  data: 0.0001  max mem: 127
[15:25:58.461264] Epoch: [2]  [160/195]  eta: 0:00:17  lr: 0.028205  loss: 2.4012 (2.7766)  time: 0.4860  data: 0.0001  max mem: 127
[15:26:08.156804] Epoch: [2]  [180/195]  eta: 0:00:07  lr: 0.029231  loss: 2.3468 (2.7288)  time: 0.4847  data: 0.0001  max mem: 127
[15:26:14.978299] Epoch: [2]  [194/195]  eta: 0:00:00  lr: 0.029949  loss: 2.3153 (2.6987)  time: 0.4860  data: 0.0001  max mem: 127
[15:26:15.054133] Epoch: [2] Total time: 0:01:35 (0.4875 s / it)
[15:26:15.054219] Averaged stats: lr: 0.029949  loss: 2.3153 (2.6987)
[15:26:15.840630] Test:  [ 0/40]  eta: 0:00:29  loss: 2.2813 (2.2813)  acc1: 22.2656 (22.2656)  acc5: 82.0312 (82.0312)  time: 0.7452  data: 0.2823  max mem: 127
[15:26:20.525151] Test:  [10/40]  eta: 0:00:14  loss: 2.2782 (2.2703)  acc1: 26.5625 (26.1364)  acc5: 81.2500 (81.6051)  time: 0.4935  data: 0.0258  max mem: 127
[15:26:25.210197] Test:  [20/40]  eta: 0:00:09  loss: 2.2564 (2.2600)  acc1: 26.9531 (27.7530)  acc5: 82.4219 (82.1987)  time: 0.4684  data: 0.0001  max mem: 127
[15:26:29.890502] Test:  [30/40]  eta: 0:00:04  loss: 2.2558 (2.2617)  acc1: 27.7344 (27.7470)  acc5: 82.8125 (82.4093)  time: 0.4682  data: 0.0001  max mem: 127
[15:26:33.707101] Test:  [39/40]  eta: 0:00:00  loss: 2.2575 (2.2630)  acc1: 27.3438 (27.3600)  acc5: 82.4219 (82.1200)  time: 0.4483  data: 0.0001  max mem: 127
[15:26:33.782497] Test: Total time: 0:00:18 (0.4672 s / it)
[15:26:33.782568] * Acc@1 27.360 Acc@5 82.120 loss 2.263
[15:26:33.784501] Accuracy of the network on the 10000 test images: 27.4%
[15:26:33.784518] Max accuracy: 27.36%
[15:26:33.786578] log_dir: ./output_dir/mae_mk0.3/linear
[15:25:09.291223] Epoch: [2]  [  0/195]  eta: 0:02:26  lr: 0.020000  loss: 3.4506 (3.4506)  time: 0.7536  data: 0.2763  max mem: 127
[15:25:18.978738] Epoch: [2]  [ 20/195]  eta: 0:01:27  lr: 0.021026  loss: 3.3363 (3.3373)  time: 0.4843  data: 0.0001  max mem: 127
[15:25:28.758489] Epoch: [2]  [ 40/195]  eta: 0:01:16  lr: 0.022051  loss: 3.0989 (3.2272)  time: 0.4889  data: 0.0001  max mem: 127
[15:25:38.510493] Epoch: [2]  [ 60/195]  eta: 0:01:06  lr: 0.023077  loss: 2.9274 (3.1319)  time: 0.4876  data: 0.0001  max mem: 127
[15:25:48.158843] Epoch: [2]  [ 80/195]  eta: 0:00:56  lr: 0.024103  loss: 2.7727 (3.0460)  time: 0.4824  data: 0.0001  max mem: 127
[15:25:57.896280] Epoch: [2]  [100/195]  eta: 0:00:46  lr: 0.025128  loss: 2.6552 (2.9691)  time: 0.4868  data: 0.0001  max mem: 127
[15:26:07.627670] Epoch: [2]  [120/195]  eta: 0:00:36  lr: 0.026154  loss: 2.5580 (2.9010)  time: 0.4865  data: 0.0001  max mem: 127
[15:26:17.235877] Epoch: [2]  [140/195]  eta: 0:00:26  lr: 0.027179  loss: 2.4560 (2.8392)  time: 0.4804  data: 0.0001  max mem: 127
[15:26:27.032833] Epoch: [2]  [160/195]  eta: 0:00:17  lr: 0.028205  loss: 2.3971 (2.7846)  time: 0.4898  data: 0.0001  max mem: 127
[15:26:36.679659] Epoch: [2]  [180/195]  eta: 0:00:07  lr: 0.029231  loss: 2.3379 (2.7353)  time: 0.4823  data: 0.0001  max mem: 127
[15:26:43.489564] Epoch: [2]  [194/195]  eta: 0:00:00  lr: 0.029949  loss: 2.3053 (2.7043)  time: 0.4818  data: 0.0001  max mem: 127
[15:26:43.540855] Epoch: [2] Total time: 0:01:35 (0.4872 s / it)
[15:26:43.540921] Averaged stats: lr: 0.029949  loss: 2.3053 (2.7043)
Not using distributed mode
[15:26:46.741913] job dir: /home/wsj/mae
[15:26:46.741977] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=2,
ema_alpha=0.99,
enable_ema=False,
enable_bootstrap=True,
ema_warmup_epochs=0,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae',
log_dir='./output_dir/bmae',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
Traceback (most recent call last):
  File "/home/wsj/mae/main_pretrain.py", line 251, in <module>
    main(args)
  File "/home/wsj/mae/main_pretrain.py", line 145, in main
    dataset_train = datasets.CIFAR10(root=args.data_path, train=True, transform=transform_train, download=False)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wsj/anaconda3/envs/pagedATTN/lib/python3.11/site-packages/torchvision/datasets/cifar.py", line 68, in __init__
    if not self._check_integrity():
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wsj/anaconda3/envs/pagedATTN/lib/python3.11/site-packages/torchvision/datasets/cifar.py", line 132, in _check_integrity
    if not check_integrity(fpath, md5):
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wsj/anaconda3/envs/pagedATTN/lib/python3.11/site-packages/torchvision/datasets/utils.py", line 58, in check_integrity
    return check_md5(fpath, md5)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/wsj/anaconda3/envs/pagedATTN/lib/python3.11/site-packages/torchvision/datasets/utils.py", line 50, in check_md5
    return md5 == calculate_md5(fpath, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wsj/anaconda3/envs/pagedATTN/lib/python3.11/site-packages/torchvision/datasets/utils.py", line 45, in calculate_md5
    md5.update(chunk)
KeyboardInterrupt
Not using distributed mode
[15:26:51.722788] job dir: /home/wsj/mae
[15:26:51.722836] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=2,
ema_alpha=0.99,
enable_ema=False,
enable_bootstrap=True,
ema_warmup_epochs=0,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae',
log_dir='./output_dir/bmae',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:26:52.331204] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:26:52.331385] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7d88fca9fc90>
[15:26:52.441158] use bootstrapped MAE!
[15:26:52.804259] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:26:52.804290] base lr: 5.00e-04
[15:26:52.804295] actual lr: 5.00e-04
[15:26:52.804301] accumulate grad iterations: 1
[15:26:52.804304] effective batch size: 256
[15:26:52.805366] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:26:52.805449] Start training for 200 epochs
[15:26:52.806453] log_dir: ./output_dir/bmae
Not using distributed mode
[15:26:54.537143] job dir: /home/wsj/mae
[15:26:54.537199] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=4,
ema_alpha=0.99,
enable_ema=False,
enable_bootstrap=True,
ema_warmup_epochs=0,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae',
log_dir='./output_dir/bmae',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:26:55.140372] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:26:55.140546] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x725a74504f10>
[15:26:55.252110] use bootstrapped MAE!
[15:26:55.684214] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:26:55.684249] base lr: 5.00e-04
[15:26:55.684256] actual lr: 5.00e-04
[15:26:55.684262] accumulate grad iterations: 1
[15:26:55.684265] effective batch size: 256
[15:26:55.685448] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:26:55.685535] Start training for 200 epochs
[15:26:55.686661] log_dir: ./output_dir/bmae
Not using distributed mode
[15:26:57.839014] job dir: /home/wsj/mae
[15:26:57.839061] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=8,
ema_alpha=0.99,
enable_ema=False,
enable_bootstrap=True,
ema_warmup_epochs=0,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae',
log_dir='./output_dir/bmae',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:26:58.350939] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:26:58.351068] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x798762718650>
[15:26:58.455004] use bootstrapped MAE!
[15:26:59.580016] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:26:59.580050] base lr: 5.00e-04
[15:26:59.580056] actual lr: 5.00e-04
[15:26:59.580061] accumulate grad iterations: 1
[15:26:59.580065] effective batch size: 256
[15:26:59.581200] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:26:59.581282] Start training for 200 epochs
[15:26:59.582307] log_dir: ./output_dir/bmae
[15:26:44.257537] Test:  [ 0/40]  eta: 0:00:27  loss: 2.2413 (2.2413)  acc1: 26.5625 (26.5625)  acc5: 84.7656 (84.7656)  time: 0.6786  data: 0.2130  max mem: 127
[15:26:48.948133] Test:  [10/40]  eta: 0:00:14  loss: 2.2427 (2.2420)  acc1: 28.1250 (28.3381)  acc5: 83.9844 (83.1321)  time: 0.4881  data: 0.0195  max mem: 127
[15:26:53.623650] Test:  [20/40]  eta: 0:00:09  loss: 2.2242 (2.2338)  acc1: 29.6875 (29.2597)  acc5: 84.3750 (83.8728)  time: 0.4683  data: 0.0001  max mem: 127
[15:26:58.302504] Test:  [30/40]  eta: 0:00:04  loss: 2.2377 (2.2364)  acc1: 29.6875 (29.2087)  acc5: 84.3750 (83.8710)  time: 0.4677  data: 0.0001  max mem: 127
[15:27:02.112678] Test:  [39/40]  eta: 0:00:00  loss: 2.2433 (2.2402)  acc1: 28.1250 (28.6600)  acc5: 83.9844 (83.5700)  time: 0.4479  data: 0.0001  max mem: 127
[15:27:02.171775] Test: Total time: 0:00:18 (0.4648 s / it)
[15:27:02.171823] * Acc@1 28.660 Acc@5 83.570 loss 2.240
[15:27:02.172033] Accuracy of the network on the 10000 test images: 28.7%
[15:27:02.172040] Max accuracy: 28.66%
[15:27:02.173833] log_dir: ./output_dir/mae_mk0.5/linear
Not using distributed mode
[15:27:02.567786] job dir: /home/wsj/mae
[15:27:02.567843] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=16,
ema_alpha=0.99,
enable_ema=False,
enable_bootstrap=True,
ema_warmup_epochs=0,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae',
log_dir='./output_dir/bmae',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:27:03.215747] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:27:03.215923] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x717a36203b50>
[15:27:03.331142] use bootstrapped MAE!
[15:27:05.004798] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:27:05.004846] base lr: 5.00e-04
[15:27:05.004853] actual lr: 5.00e-04
[15:27:05.004858] accumulate grad iterations: 1
[15:27:05.004861] effective batch size: 256
[15:27:05.006273] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:27:05.006380] Start training for 200 epochs
[15:27:05.007585] log_dir: ./output_dir/bmae
[15:25:33.342905] Epoch: [2]  [  0/195]  eta: 0:02:23  lr: 0.020000  loss: 3.4256 (3.4256)  time: 0.7379  data: 0.2527  max mem: 127
[15:25:42.999661] Epoch: [2]  [ 20/195]  eta: 0:01:26  lr: 0.021026  loss: 3.3102 (3.3116)  time: 0.4828  data: 0.0001  max mem: 127
[15:25:52.713519] Epoch: [2]  [ 40/195]  eta: 0:01:16  lr: 0.022051  loss: 3.0673 (3.2019)  time: 0.4857  data: 0.0001  max mem: 127
[15:26:02.418519] Epoch: [2]  [ 60/195]  eta: 0:01:05  lr: 0.023077  loss: 2.8927 (3.1068)  time: 0.4852  data: 0.0003  max mem: 127
[15:26:12.153200] Epoch: [2]  [ 80/195]  eta: 0:00:56  lr: 0.024103  loss: 2.7479 (3.0200)  time: 0.4867  data: 0.0001  max mem: 127
[15:26:21.797320] Epoch: [2]  [100/195]  eta: 0:00:46  lr: 0.025128  loss: 2.6304 (2.9434)  time: 0.4822  data: 0.0001  max mem: 127
[15:26:31.627978] Epoch: [2]  [120/195]  eta: 0:00:36  lr: 0.026154  loss: 2.5196 (2.8743)  time: 0.4915  data: 0.0001  max mem: 127
[15:26:41.254497] Epoch: [2]  [140/195]  eta: 0:00:26  lr: 0.027179  loss: 2.4289 (2.8116)  time: 0.4813  data: 0.0001  max mem: 127
[15:26:50.942813] Epoch: [2]  [160/195]  eta: 0:00:17  lr: 0.028205  loss: 2.3629 (2.7560)  time: 0.4844  data: 0.0001  max mem: 127
[15:27:00.789589] Epoch: [2]  [180/195]  eta: 0:00:07  lr: 0.029231  loss: 2.3045 (2.7059)  time: 0.4923  data: 0.0002  max mem: 127
[15:27:07.504591] Epoch: [2]  [194/195]  eta: 0:00:00  lr: 0.029949  loss: 2.2673 (2.6742)  time: 0.4840  data: 0.0001  max mem: 127
[15:27:07.552060] Epoch: [2] Total time: 0:01:34 (0.4869 s / it)
[15:27:07.552118] Averaged stats: lr: 0.029949  loss: 2.2673 (2.6742)
Not using distributed mode
[15:27:04.823303] job dir: /home/wsj/mae
[15:27:04.823359] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=32,
ema_alpha=0.99,
enable_ema=False,
enable_bootstrap=True,
ema_warmup_epochs=0,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae',
log_dir='./output_dir/bmae',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:27:05.488710] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:27:05.488882] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7a350547ce50>
[15:27:05.600782] use bootstrapped MAE!
[15:27:08.739736] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:27:08.739772] base lr: 5.00e-04
[15:27:08.739778] actual lr: 5.00e-04
[15:27:08.739783] accumulate grad iterations: 1
[15:27:08.739787] effective batch size: 256
[15:27:08.740991] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:27:08.741077] Start training for 200 epochs
[15:27:08.742136] log_dir: ./output_dir/bmae
[15:25:40.414612] Epoch: [2]  [  0/195]  eta: 0:02:17  lr: 0.020000  loss: 3.3056 (3.3056)  time: 0.7060  data: 0.2230  max mem: 127
[15:25:50.120905] Epoch: [2]  [ 20/195]  eta: 0:01:26  lr: 0.021026  loss: 3.1511 (3.1667)  time: 0.4853  data: 0.0001  max mem: 127
[15:25:59.832709] Epoch: [2]  [ 40/195]  eta: 0:01:16  lr: 0.022051  loss: 2.9132 (3.0511)  time: 0.4856  data: 0.0001  max mem: 127
[15:26:09.569688] Epoch: [2]  [ 60/195]  eta: 0:01:06  lr: 0.023077  loss: 2.7353 (2.9490)  time: 0.4868  data: 0.0001  max mem: 127
[15:26:19.183530] Epoch: [2]  [ 80/195]  eta: 0:00:56  lr: 0.024103  loss: 2.5693 (2.8568)  time: 0.4807  data: 0.0001  max mem: 127
[15:26:28.994822] Epoch: [2]  [100/195]  eta: 0:00:46  lr: 0.025128  loss: 2.4318 (2.7736)  time: 0.4905  data: 0.0001  max mem: 127
[15:26:38.638336] Epoch: [2]  [120/195]  eta: 0:00:36  lr: 0.026154  loss: 2.3119 (2.6991)  time: 0.4821  data: 0.0001  max mem: 127
[15:26:48.285064] Epoch: [2]  [140/195]  eta: 0:00:26  lr: 0.027179  loss: 2.2067 (2.6295)  time: 0.4823  data: 0.0001  max mem: 127
[15:26:58.068652] Epoch: [2]  [160/195]  eta: 0:00:17  lr: 0.028205  loss: 2.1254 (2.5670)  time: 0.4891  data: 0.0001  max mem: 127
[15:27:07.659644] Epoch: [2]  [180/195]  eta: 0:00:07  lr: 0.029231  loss: 2.0558 (2.5115)  time: 0.4795  data: 0.0001  max mem: 127
[15:27:14.479344] Epoch: [2]  [194/195]  eta: 0:00:00  lr: 0.029949  loss: 2.0052 (2.4753)  time: 0.4833  data: 0.0001  max mem: 127
[15:27:14.526152] Epoch: [2] Total time: 0:01:34 (0.4862 s / it)
[15:27:14.526212] Averaged stats: lr: 0.029949  loss: 2.0052 (2.4753)
Not using distributed mode
[15:27:21.988703] job dir: /home/wsj/mae
[15:27:21.988756] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=64,
ema_alpha=0.99,
enable_ema=False,
enable_bootstrap=True,
ema_warmup_epochs=0,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae',
log_dir='./output_dir/bmae',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:27:22.636465] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:27:22.636646] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7b92d44c00d0>
[15:27:22.746106] use bootstrapped MAE!
[15:27:25.280124] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:27:25.280183] base lr: 5.00e-04
[15:27:25.280189] actual lr: 5.00e-04
[15:27:25.280198] accumulate grad iterations: 1
[15:27:25.280202] effective batch size: 256
[15:27:25.281797] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:27:25.281938] Start training for 200 epochs
[15:27:25.283220] log_dir: ./output_dir/bmae
[15:27:08.209523] Test:  [ 0/40]  eta: 0:00:24  loss: 2.1874 (2.1874)  acc1: 32.4219 (32.4219)  acc5: 87.5000 (87.5000)  time: 0.6201  data: 0.1555  max mem: 127
[15:27:12.878346] Test:  [10/40]  eta: 0:00:14  loss: 2.1850 (2.1903)  acc1: 32.4219 (32.5994)  acc5: 85.1562 (85.5114)  time: 0.4808  data: 0.0143  max mem: 127
[15:27:17.499791] Test:  [20/40]  eta: 0:00:09  loss: 2.1829 (2.1876)  acc1: 33.5938 (33.4449)  acc5: 85.1562 (85.5283)  time: 0.4645  data: 0.0002  max mem: 127
[15:27:22.221973] Test:  [30/40]  eta: 0:00:04  loss: 2.1872 (2.1895)  acc1: 34.3750 (33.6694)  acc5: 85.5469 (85.4839)  time: 0.4671  data: 0.0001  max mem: 127
[15:27:26.059193] Test:  [39/40]  eta: 0:00:00  loss: 2.2007 (2.1920)  acc1: 32.4219 (33.3500)  acc5: 85.1562 (85.4000)  time: 0.4515  data: 0.0001  max mem: 127
[15:27:26.108248] Test: Total time: 0:00:18 (0.4630 s / it)
[15:27:26.108308] * Acc@1 33.350 Acc@5 85.400 loss 2.192
[15:27:26.108445] Accuracy of the network on the 10000 test images: 33.4%
[15:27:26.108451] Max accuracy: 33.35%
[15:27:26.110594] log_dir: ./output_dir/mae_mk0.7/linear
[15:26:53.947511] Epoch: [0]  [  0/195]  eta: 0:03:42  lr: 0.000000  loss: 2.5151 (2.5151)  time: 1.1402  data: 0.2877  max mem: 2070
[15:26:55.223451] Epoch: [0]  [ 20/195]  eta: 0:00:20  lr: 0.000001  loss: 2.4594 (2.4379)  time: 0.0638  data: 0.0002  max mem: 2139
[15:26:56.570594] Epoch: [0]  [ 40/195]  eta: 0:00:14  lr: 0.000003  loss: 1.9374 (2.2089)  time: 0.0673  data: 0.0009  max mem: 2139
[15:26:58.623779] Epoch: [0]  [ 60/195]  eta: 0:00:12  lr: 0.000004  loss: 1.4184 (1.9575)  time: 0.1026  data: 0.0001  max mem: 2139
[15:27:00.972179] Epoch: [0]  [ 80/195]  eta: 0:00:11  lr: 0.000005  loss: 1.1293 (1.7559)  time: 0.1174  data: 0.0001  max mem: 2139
[15:27:04.223238] Epoch: [0]  [100/195]  eta: 0:00:10  lr: 0.000006  loss: 0.9976 (1.6066)  time: 0.1625  data: 0.0001  max mem: 2139
[15:27:07.629505] Epoch: [0]  [120/195]  eta: 0:00:09  lr: 0.000008  loss: 0.9240 (1.4949)  time: 0.1703  data: 0.0002  max mem: 2139
[15:27:12.389913] Epoch: [0]  [140/195]  eta: 0:00:07  lr: 0.000009  loss: 0.8550 (1.4052)  time: 0.2380  data: 0.0002  max mem: 2139
[15:27:17.911382] Epoch: [0]  [160/195]  eta: 0:00:05  lr: 0.000010  loss: 0.8223 (1.3331)  time: 0.2760  data: 0.0002  max mem: 2139
[15:27:23.410554] Epoch: [0]  [180/195]  eta: 0:00:02  lr: 0.000012  loss: 0.7954 (1.2738)  time: 0.2749  data: 0.0002  max mem: 2139
[15:27:27.382177] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.000012  loss: 0.7869 (1.2387)  time: 0.2815  data: 0.0001  max mem: 2139
[15:27:27.430479] Epoch: [0] Total time: 0:00:34 (0.1776 s / it)
[15:27:27.430577] Averaged stats: lr: 0.000012  loss: 0.7869 (1.2387)
[15:27:27.656786] log_dir: ./output_dir/bmae
[15:27:15.216551] Test:  [ 0/40]  eta: 0:00:26  loss: 1.8634 (1.8634)  acc1: 50.0000 (50.0000)  acc5: 96.0938 (96.0938)  time: 0.6523  data: 0.1857  max mem: 127
[15:27:19.930136] Test:  [10/40]  eta: 0:00:14  loss: 1.8634 (1.8648)  acc1: 50.3906 (51.8111)  acc5: 94.9219 (94.9219)  time: 0.4878  data: 0.0170  max mem: 127
[15:27:24.658890] Test:  [20/40]  eta: 0:00:09  loss: 1.8567 (1.8660)  acc1: 51.1719 (51.4509)  acc5: 94.5312 (94.8103)  time: 0.4721  data: 0.0001  max mem: 127
[15:27:29.232440] Test:  [30/40]  eta: 0:00:04  loss: 1.8679 (1.8689)  acc1: 51.1719 (51.3987)  acc5: 94.9219 (94.8085)  time: 0.4651  data: 0.0001  max mem: 127
[15:27:33.057028] Test:  [39/40]  eta: 0:00:00  loss: 1.8738 (1.8735)  acc1: 48.8281 (50.9800)  acc5: 94.5312 (94.7600)  time: 0.4433  data: 0.0001  max mem: 127
[15:27:33.108980] Test: Total time: 0:00:18 (0.4636 s / it)
[15:27:33.109028] * Acc@1 50.980 Acc@5 94.760 loss 1.873
[15:27:33.109125] Accuracy of the network on the 10000 test images: 51.0%
[15:27:33.109132] Max accuracy: 50.98%
[15:27:33.110742] log_dir: ./output_dir/mae_mk0.9/linear
[15:26:56.876424] Epoch: [0]  [  0/195]  eta: 0:03:51  lr: 0.000000  loss: 2.5151 (2.5151)  time: 1.1887  data: 0.2855  max mem: 2070
[15:26:59.161711] Epoch: [0]  [ 20/195]  eta: 0:00:28  lr: 0.000001  loss: 2.4594 (2.4379)  time: 0.1142  data: 0.0002  max mem: 2139
[15:27:01.706117] Epoch: [0]  [ 40/195]  eta: 0:00:22  lr: 0.000003  loss: 1.9374 (2.2089)  time: 0.1272  data: 0.0002  max mem: 2139
[15:27:05.133842] Epoch: [0]  [ 60/195]  eta: 0:00:20  lr: 0.000004  loss: 1.4184 (1.9575)  time: 0.1713  data: 0.0002  max mem: 2139
[15:27:08.905324] Epoch: [0]  [ 80/195]  eta: 0:00:18  lr: 0.000005  loss: 1.1293 (1.7559)  time: 0.1885  data: 0.0002  max mem: 2139
[15:27:14.179067] Epoch: [0]  [100/195]  eta: 0:00:17  lr: 0.000006  loss: 0.9976 (1.6066)  time: 0.2636  data: 0.0001  max mem: 2139
[15:27:19.772205] Epoch: [0]  [120/195]  eta: 0:00:14  lr: 0.000008  loss: 0.9240 (1.4949)  time: 0.2796  data: 0.0002  max mem: 2139
[15:27:25.375704] Epoch: [0]  [140/195]  eta: 0:00:11  lr: 0.000009  loss: 0.8550 (1.4052)  time: 0.2801  data: 0.0002  max mem: 2139
[15:27:31.661338] Epoch: [0]  [160/195]  eta: 0:00:07  lr: 0.000010  loss: 0.8223 (1.3331)  time: 0.3142  data: 0.0002  max mem: 2139
[15:27:38.371882] Epoch: [0]  [180/195]  eta: 0:00:03  lr: 0.000012  loss: 0.7954 (1.2738)  time: 0.3355  data: 0.0002  max mem: 2139
[15:27:43.073578] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.000012  loss: 0.7869 (1.2387)  time: 0.3368  data: 0.0001  max mem: 2139
[15:27:43.122137] Epoch: [0] Total time: 0:00:47 (0.2433 s / it)
[15:27:43.122562] Averaged stats: lr: 0.000012  loss: 0.7869 (1.2387)
[15:27:43.375380] log_dir: ./output_dir/bmae
[15:27:01.139881] Epoch: [0]  [  0/195]  eta: 0:05:03  lr: 0.000000  loss: 2.5151 (2.5151)  time: 1.5565  data: 0.2672  max mem: 2070
[15:27:04.522659] Epoch: [0]  [ 20/195]  eta: 0:00:41  lr: 0.000001  loss: 2.4594 (2.4379)  time: 0.1691  data: 0.0002  max mem: 2139
[15:27:08.039829] Epoch: [0]  [ 40/195]  eta: 0:00:31  lr: 0.000003  loss: 1.9374 (2.2089)  time: 0.1758  data: 0.0002  max mem: 2139
[15:27:13.057885] Epoch: [0]  [ 60/195]  eta: 0:00:29  lr: 0.000004  loss: 1.4184 (1.9575)  time: 0.2509  data: 0.0002  max mem: 2139
[15:27:18.712419] Epoch: [0]  [ 80/195]  eta: 0:00:27  lr: 0.000005  loss: 1.1293 (1.7559)  time: 0.2827  data: 0.0002  max mem: 2139
[15:27:24.325071] Epoch: [0]  [100/195]  eta: 0:00:23  lr: 0.000006  loss: 0.9976 (1.6066)  time: 0.2806  data: 0.0002  max mem: 2139
[15:27:30.470003] Epoch: [0]  [120/195]  eta: 0:00:19  lr: 0.000008  loss: 0.9240 (1.4949)  time: 0.3072  data: 0.0002  max mem: 2139
[15:27:37.211934] Epoch: [0]  [140/195]  eta: 0:00:14  lr: 0.000009  loss: 0.8550 (1.4052)  time: 0.3371  data: 0.0002  max mem: 2139
[15:27:43.902081] Epoch: [0]  [160/195]  eta: 0:00:09  lr: 0.000010  loss: 0.8223 (1.3331)  time: 0.3345  data: 0.0001  max mem: 2139
[15:27:50.643178] Epoch: [0]  [180/195]  eta: 0:00:04  lr: 0.000012  loss: 0.7954 (1.2738)  time: 0.3370  data: 0.0002  max mem: 2139
[15:27:55.370632] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.000012  loss: 0.7869 (1.2387)  time: 0.3367  data: 0.0001  max mem: 2139
[15:27:55.404102] Epoch: [0] Total time: 0:00:55 (0.2863 s / it)
[15:27:55.404168] Averaged stats: lr: 0.000012  loss: 0.7869 (1.2387)
[15:27:55.654068] log_dir: ./output_dir/bmae
[15:26:34.614526] Epoch: [3]  [  0/195]  eta: 0:02:41  lr: 0.030000  loss: 2.2694 (2.2694)  time: 0.8272  data: 0.3470  max mem: 127
[15:26:44.255317] Epoch: [3]  [ 20/195]  eta: 0:01:27  lr: 0.031026  loss: 2.2605 (2.2618)  time: 0.4820  data: 0.0001  max mem: 127
[15:26:54.052656] Epoch: [3]  [ 40/195]  eta: 0:01:16  lr: 0.032051  loss: 2.2013 (2.2366)  time: 0.4898  data: 0.0001  max mem: 127
[15:27:03.734889] Epoch: [3]  [ 60/195]  eta: 0:01:06  lr: 0.033077  loss: 2.1803 (2.2199)  time: 0.4841  data: 0.0001  max mem: 127
[15:27:13.455278] Epoch: [3]  [ 80/195]  eta: 0:00:56  lr: 0.034103  loss: 2.1610 (2.2059)  time: 0.4860  data: 0.0001  max mem: 127
[15:27:23.258056] Epoch: [3]  [100/195]  eta: 0:00:46  lr: 0.035128  loss: 2.1385 (2.1934)  time: 0.4901  data: 0.0001  max mem: 127
[15:27:32.965668] Epoch: [3]  [120/195]  eta: 0:00:36  lr: 0.036154  loss: 2.1169 (2.1818)  time: 0.4853  data: 0.0001  max mem: 127
[15:27:42.608155] Epoch: [3]  [140/195]  eta: 0:00:26  lr: 0.037179  loss: 2.0945 (2.1702)  time: 0.4821  data: 0.0001  max mem: 127
[15:27:52.333835] Epoch: [3]  [160/195]  eta: 0:00:17  lr: 0.038205  loss: 2.0778 (2.1586)  time: 0.4863  data: 0.0001  max mem: 127
[15:28:02.050570] Epoch: [3]  [180/195]  eta: 0:00:07  lr: 0.039231  loss: 2.0575 (2.1476)  time: 0.4858  data: 0.0001  max mem: 127
[15:28:08.858512] Epoch: [3]  [194/195]  eta: 0:00:00  lr: 0.039949  loss: 2.0641 (2.1413)  time: 0.4863  data: 0.0001  max mem: 127
[15:28:08.912345] Epoch: [3] Total time: 0:01:35 (0.4878 s / it)
[15:28:08.912406] Averaged stats: lr: 0.039949  loss: 2.0641 (2.1413)
[15:27:07.897495] Epoch: [0]  [  0/195]  eta: 0:09:23  lr: 0.000000  loss: 2.5151 (2.5151)  time: 2.8888  data: 0.2813  max mem: 2070
[15:27:12.988402] Epoch: [0]  [ 20/195]  eta: 0:01:06  lr: 0.000001  loss: 2.4594 (2.4379)  time: 0.2545  data: 0.0002  max mem: 2139
[15:27:18.685855] Epoch: [0]  [ 40/195]  eta: 0:00:51  lr: 0.000003  loss: 1.9374 (2.2089)  time: 0.2848  data: 0.0002  max mem: 2139
[15:27:24.435724] Epoch: [0]  [ 60/195]  eta: 0:00:42  lr: 0.000004  loss: 1.4184 (1.9575)  time: 0.2875  data: 0.0002  max mem: 2139
[15:27:30.687229] Epoch: [0]  [ 80/195]  eta: 0:00:36  lr: 0.000005  loss: 1.1293 (1.7559)  time: 0.3125  data: 0.0002  max mem: 2139
[15:27:37.546786] Epoch: [0]  [100/195]  eta: 0:00:30  lr: 0.000006  loss: 0.9976 (1.6066)  time: 0.3429  data: 0.0002  max mem: 2139
[15:27:44.286899] Epoch: [0]  [120/195]  eta: 0:00:24  lr: 0.000008  loss: 0.9240 (1.4949)  time: 0.3370  data: 0.0002  max mem: 2139
[15:27:51.142720] Epoch: [0]  [140/195]  eta: 0:00:17  lr: 0.000009  loss: 0.8550 (1.4052)  time: 0.3427  data: 0.0002  max mem: 2139
[15:27:57.872939] Epoch: [0]  [160/195]  eta: 0:00:11  lr: 0.000010  loss: 0.8223 (1.3331)  time: 0.3365  data: 0.0002  max mem: 2139
[15:28:04.727169] Epoch: [0]  [180/195]  eta: 0:00:04  lr: 0.000012  loss: 0.7954 (1.2738)  time: 0.3427  data: 0.0002  max mem: 2139
[15:28:09.523750] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.000012  loss: 0.7869 (1.2387)  time: 0.3424  data: 0.0001  max mem: 2139
[15:28:09.558744] Epoch: [0] Total time: 0:01:04 (0.3310 s / it)
[15:28:09.558815] Averaged stats: lr: 0.000012  loss: 0.7869 (1.2387)
[15:28:09.788139] log_dir: ./output_dir/bmae
[15:27:10.790014] Epoch: [0]  [  0/195]  eta: 0:06:39  lr: 0.000000  loss: 2.5151 (2.5151)  time: 2.0467  data: 0.3152  max mem: 2070
[15:27:16.503531] Epoch: [0]  [ 20/195]  eta: 0:01:04  lr: 0.000001  loss: 2.4594 (2.4379)  time: 0.2856  data: 0.0002  max mem: 2139
[15:27:22.225906] Epoch: [0]  [ 40/195]  eta: 0:00:50  lr: 0.000003  loss: 1.9374 (2.2089)  time: 0.2861  data: 0.0002  max mem: 2139
[15:27:28.102059] Epoch: [0]  [ 60/195]  eta: 0:00:42  lr: 0.000004  loss: 1.4184 (1.9575)  time: 0.2938  data: 0.0002  max mem: 2139
[15:27:34.962551] Epoch: [0]  [ 80/195]  eta: 0:00:37  lr: 0.000005  loss: 1.1293 (1.7559)  time: 0.3430  data: 0.0002  max mem: 2139
[15:27:41.809271] Epoch: [0]  [100/195]  eta: 0:00:31  lr: 0.000006  loss: 0.9976 (1.6066)  time: 0.3423  data: 0.0002  max mem: 2139
[15:27:48.564639] Epoch: [0]  [120/195]  eta: 0:00:24  lr: 0.000008  loss: 0.9240 (1.4949)  time: 0.3377  data: 0.0002  max mem: 2139
[15:27:55.405339] Epoch: [0]  [140/195]  eta: 0:00:18  lr: 0.000009  loss: 0.8550 (1.4052)  time: 0.3420  data: 0.0002  max mem: 2139
[15:28:02.165151] Epoch: [0]  [160/195]  eta: 0:00:11  lr: 0.000010  loss: 0.8223 (1.3331)  time: 0.3380  data: 0.0002  max mem: 2139
[15:28:09.018120] Epoch: [0]  [180/195]  eta: 0:00:04  lr: 0.000012  loss: 0.7954 (1.2738)  time: 0.3426  data: 0.0001  max mem: 2139
[15:28:13.734134] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.000012  loss: 0.7869 (1.2387)  time: 0.3384  data: 0.0001  max mem: 2139
[15:28:13.778073] Epoch: [0] Total time: 0:01:05 (0.3335 s / it)
[15:28:13.778133] Averaged stats: lr: 0.000012  loss: 0.7869 (1.2387)
[15:28:13.984918] log_dir: ./output_dir/bmae
[15:28:09.660593] Test:  [ 0/40]  eta: 0:00:28  loss: 2.0280 (2.0280)  acc1: 28.9062 (28.9062)  acc5: 88.2812 (88.2812)  time: 0.7110  data: 0.2441  max mem: 127
[15:28:14.319806] Test:  [10/40]  eta: 0:00:14  loss: 2.0280 (2.0159)  acc1: 31.6406 (31.9602)  acc5: 83.5938 (84.3395)  time: 0.4881  data: 0.0223  max mem: 127
[15:28:19.004569] Test:  [20/40]  eta: 0:00:09  loss: 2.0083 (2.0038)  acc1: 34.3750 (33.4635)  acc5: 84.7656 (84.8772)  time: 0.4671  data: 0.0001  max mem: 127
[15:28:23.669560] Test:  [30/40]  eta: 0:00:04  loss: 2.0000 (2.0061)  acc1: 34.3750 (33.7072)  acc5: 84.7656 (84.7782)  time: 0.4674  data: 0.0001  max mem: 127
[15:28:27.494946] Test:  [39/40]  eta: 0:00:00  loss: 2.0013 (2.0077)  acc1: 33.5938 (33.5000)  acc5: 83.9844 (84.4700)  time: 0.4479  data: 0.0001  max mem: 127
[15:28:27.550666] Test: Total time: 0:00:18 (0.4650 s / it)
[15:28:27.550715] * Acc@1 33.500 Acc@5 84.470 loss 2.008
[15:28:27.550993] Accuracy of the network on the 10000 test images: 33.5%
[15:28:27.551004] Max accuracy: 33.50%
[15:28:27.552513] log_dir: ./output_dir/mae_mk0.3/linear
[15:27:28.196153] Epoch: [1]  [  0/195]  eta: 0:01:44  lr: 0.000013  loss: 0.8015 (0.8015)  time: 0.5384  data: 0.2118  max mem: 2139
[15:27:34.801629] Epoch: [1]  [ 20/195]  eta: 0:00:59  lr: 0.000014  loss: 0.7691 (0.7724)  time: 0.3302  data: 0.0002  max mem: 2139
[15:27:41.358690] Epoch: [1]  [ 40/195]  eta: 0:00:51  lr: 0.000015  loss: 0.7630 (0.7685)  time: 0.3278  data: 0.0002  max mem: 2139
[15:27:47.825011] Epoch: [1]  [ 60/195]  eta: 0:00:44  lr: 0.000016  loss: 0.7609 (0.7666)  time: 0.3233  data: 0.0002  max mem: 2139
[15:27:54.480347] Epoch: [1]  [ 80/195]  eta: 0:00:38  lr: 0.000018  loss: 0.7709 (0.7666)  time: 0.3327  data: 0.0002  max mem: 2139
[15:28:01.055099] Epoch: [1]  [100/195]  eta: 0:00:31  lr: 0.000019  loss: 0.7641 (0.7657)  time: 0.3287  data: 0.0002  max mem: 2139
[15:28:07.703760] Epoch: [1]  [120/195]  eta: 0:00:24  lr: 0.000020  loss: 0.7579 (0.7643)  time: 0.3324  data: 0.0002  max mem: 2139
[15:28:14.159150] Epoch: [1]  [140/195]  eta: 0:00:18  lr: 0.000021  loss: 0.7526 (0.7636)  time: 0.3227  data: 0.0002  max mem: 2139
[15:28:20.877134] Epoch: [1]  [160/195]  eta: 0:00:11  lr: 0.000023  loss: 0.7600 (0.7630)  time: 0.3359  data: 0.0002  max mem: 2139
[15:28:27.726915] Epoch: [1]  [180/195]  eta: 0:00:04  lr: 0.000024  loss: 0.7463 (0.7619)  time: 0.3424  data: 0.0002  max mem: 2139
[15:28:32.501878] Epoch: [1]  [194/195]  eta: 0:00:00  lr: 0.000025  loss: 0.7478 (0.7612)  time: 0.3414  data: 0.0001  max mem: 2139
[15:28:32.555608] Epoch: [1] Total time: 0:01:04 (0.3328 s / it)
[15:28:32.555699] Averaged stats: lr: 0.000025  loss: 0.7478 (0.7612)
[15:28:32.558643] log_dir: ./output_dir/bmae
[15:27:27.445388] Epoch: [0]  [  0/195]  eta: 0:07:01  lr: 0.000000  loss: 2.5151 (2.5151)  time: 2.1611  data: 0.2569  max mem: 2070
[15:27:34.211745] Epoch: [0]  [ 20/195]  eta: 0:01:14  lr: 0.000001  loss: 2.4594 (2.4379)  time: 0.3383  data: 0.0002  max mem: 2139
[15:27:41.063306] Epoch: [0]  [ 40/195]  eta: 0:00:59  lr: 0.000003  loss: 1.9374 (2.2089)  time: 0.3425  data: 0.0002  max mem: 2139
[15:27:47.823854] Epoch: [0]  [ 60/195]  eta: 0:00:49  lr: 0.000004  loss: 1.4184 (1.9575)  time: 0.3380  data: 0.0002  max mem: 2139
[15:27:54.671478] Epoch: [0]  [ 80/195]  eta: 0:00:41  lr: 0.000005  loss: 1.1293 (1.7559)  time: 0.3423  data: 0.0002  max mem: 2139
[15:28:01.417953] Epoch: [0]  [100/195]  eta: 0:00:33  lr: 0.000006  loss: 0.9976 (1.6066)  time: 0.3373  data: 0.0001  max mem: 2139
[15:28:08.272946] Epoch: [0]  [120/195]  eta: 0:00:26  lr: 0.000008  loss: 0.9240 (1.4949)  time: 0.3427  data: 0.0001  max mem: 2139
[15:28:14.954102] Epoch: [0]  [140/195]  eta: 0:00:19  lr: 0.000009  loss: 0.8550 (1.4052)  time: 0.3340  data: 0.0001  max mem: 2139
[15:28:21.805070] Epoch: [0]  [160/195]  eta: 0:00:12  lr: 0.000010  loss: 0.8223 (1.3331)  time: 0.3425  data: 0.0002  max mem: 2139
[15:28:28.656134] Epoch: [0]  [180/195]  eta: 0:00:05  lr: 0.000012  loss: 0.7954 (1.2738)  time: 0.3425  data: 0.0002  max mem: 2139
[15:28:33.393033] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.000012  loss: 0.7869 (1.2387)  time: 0.3396  data: 0.0001  max mem: 2139
[15:28:33.442875] Epoch: [0] Total time: 0:01:08 (0.3495 s / it)
[15:28:33.442932] Averaged stats: lr: 0.000012  loss: 0.7869 (1.2387)
[15:28:33.639569] log_dir: ./output_dir/bmae
[15:27:02.911936] Epoch: [3]  [  0/195]  eta: 0:02:23  lr: 0.030000  loss: 2.2619 (2.2619)  time: 0.7370  data: 0.2493  max mem: 127
[15:27:12.587525] Epoch: [3]  [ 20/195]  eta: 0:01:26  lr: 0.031026  loss: 2.2467 (2.2492)  time: 0.4837  data: 0.0001  max mem: 127
[15:27:22.376244] Epoch: [3]  [ 40/195]  eta: 0:01:16  lr: 0.032051  loss: 2.1934 (2.2258)  time: 0.4894  data: 0.0001  max mem: 127
[15:27:32.126075] Epoch: [3]  [ 60/195]  eta: 0:01:06  lr: 0.033077  loss: 2.1654 (2.2085)  time: 0.4874  data: 0.0001  max mem: 127
[15:27:41.762998] Epoch: [3]  [ 80/195]  eta: 0:00:56  lr: 0.034103  loss: 2.1389 (2.1924)  time: 0.4818  data: 0.0001  max mem: 127
[15:27:51.493600] Epoch: [3]  [100/195]  eta: 0:00:46  lr: 0.035128  loss: 2.1145 (2.1768)  time: 0.4865  data: 0.0001  max mem: 127
[15:28:01.212776] Epoch: [3]  [120/195]  eta: 0:00:36  lr: 0.036154  loss: 2.0884 (2.1634)  time: 0.4859  data: 0.0001  max mem: 127
[15:28:10.818608] Epoch: [3]  [140/195]  eta: 0:00:26  lr: 0.037179  loss: 2.0596 (2.1496)  time: 0.4802  data: 0.0001  max mem: 127
[15:28:20.614730] Epoch: [3]  [160/195]  eta: 0:00:17  lr: 0.038205  loss: 2.0419 (2.1365)  time: 0.4898  data: 0.0001  max mem: 127
[15:28:30.299768] Epoch: [3]  [180/195]  eta: 0:00:07  lr: 0.039231  loss: 2.0231 (2.1241)  time: 0.4842  data: 0.0001  max mem: 127
[15:28:37.117546] Epoch: [3]  [194/195]  eta: 0:00:00  lr: 0.039949  loss: 2.0183 (2.1166)  time: 0.4814  data: 0.0001  max mem: 127
[15:28:37.181676] Epoch: [3] Total time: 0:01:35 (0.4872 s / it)
[15:28:37.181754] Averaged stats: lr: 0.039949  loss: 2.0183 (2.1166)
[15:27:43.913387] Epoch: [1]  [  0/195]  eta: 0:01:44  lr: 0.000013  loss: 0.8015 (0.8015)  time: 0.5365  data: 0.2096  max mem: 2139
[15:27:50.642012] Epoch: [1]  [ 20/195]  eta: 0:01:00  lr: 0.000014  loss: 0.7691 (0.7724)  time: 0.3364  data: 0.0002  max mem: 2139
[15:27:57.310179] Epoch: [1]  [ 40/195]  eta: 0:00:52  lr: 0.000015  loss: 0.7630 (0.7685)  time: 0.3334  data: 0.0002  max mem: 2139
[15:28:04.024192] Epoch: [1]  [ 60/195]  eta: 0:00:45  lr: 0.000016  loss: 0.7609 (0.7666)  time: 0.3357  data: 0.0002  max mem: 2139
[15:28:10.757682] Epoch: [1]  [ 80/195]  eta: 0:00:38  lr: 0.000018  loss: 0.7709 (0.7666)  time: 0.3366  data: 0.0002  max mem: 2139
[15:28:17.461609] Epoch: [1]  [100/195]  eta: 0:00:32  lr: 0.000019  loss: 0.7641 (0.7657)  time: 0.3352  data: 0.0002  max mem: 2139
[15:28:24.282953] Epoch: [1]  [120/195]  eta: 0:00:25  lr: 0.000020  loss: 0.7579 (0.7643)  time: 0.3410  data: 0.0001  max mem: 2139
[15:28:31.109938] Epoch: [1]  [140/195]  eta: 0:00:18  lr: 0.000021  loss: 0.7526 (0.7636)  time: 0.3413  data: 0.0002  max mem: 2139
[15:28:37.789258] Epoch: [1]  [160/195]  eta: 0:00:11  lr: 0.000023  loss: 0.7600 (0.7630)  time: 0.3339  data: 0.0001  max mem: 2139
[15:28:44.627735] Epoch: [1]  [180/195]  eta: 0:00:05  lr: 0.000024  loss: 0.7463 (0.7619)  time: 0.3419  data: 0.0001  max mem: 2139
[15:28:49.446094] Epoch: [1]  [194/195]  eta: 0:00:00  lr: 0.000025  loss: 0.7478 (0.7612)  time: 0.3434  data: 0.0001  max mem: 2139
[15:28:49.491029] Epoch: [1] Total time: 0:01:06 (0.3391 s / it)
[15:28:49.491092] Averaged stats: lr: 0.000025  loss: 0.7478 (0.7612)
[15:28:49.493393] log_dir: ./output_dir/bmae
[15:28:37.920506] Test:  [ 0/40]  eta: 0:00:27  loss: 1.9612 (1.9612)  acc1: 32.4219 (32.4219)  acc5: 89.8438 (89.8438)  time: 0.6964  data: 0.2320  max mem: 127
[15:28:42.606583] Test:  [10/40]  eta: 0:00:14  loss: 1.9621 (1.9610)  acc1: 33.5938 (33.6293)  acc5: 86.7188 (86.2926)  time: 0.4892  data: 0.0213  max mem: 127
[15:28:47.278584] Test:  [20/40]  eta: 0:00:09  loss: 1.9564 (1.9506)  acc1: 35.1562 (34.6354)  acc5: 86.7188 (86.5699)  time: 0.4678  data: 0.0002  max mem: 127
[15:28:51.946214] Test:  [30/40]  eta: 0:00:04  loss: 1.9552 (1.9539)  acc1: 34.7656 (34.5136)  acc5: 86.3281 (86.3407)  time: 0.4669  data: 0.0002  max mem: 127
[15:28:55.772479] Test:  [39/40]  eta: 0:00:00  loss: 1.9608 (1.9573)  acc1: 33.2031 (33.8600)  acc5: 85.9375 (86.2500)  time: 0.4480  data: 0.0002  max mem: 127
[15:28:55.842530] Test: Total time: 0:00:18 (0.4655 s / it)
[15:28:55.842594] * Acc@1 33.860 Acc@5 86.250 loss 1.957
[15:28:55.842733] Accuracy of the network on the 10000 test images: 33.9%
[15:28:55.842740] Max accuracy: 33.86%
[15:28:55.844555] log_dir: ./output_dir/mae_mk0.5/linear
[15:27:26.867150] Epoch: [3]  [  0/195]  eta: 0:02:27  lr: 0.030000  loss: 2.2177 (2.2177)  time: 0.7554  data: 0.2694  max mem: 127
[15:27:36.540280] Epoch: [3]  [ 20/195]  eta: 0:01:26  lr: 0.031026  loss: 2.2063 (2.2103)  time: 0.4836  data: 0.0001  max mem: 127
[15:27:46.270387] Epoch: [3]  [ 40/195]  eta: 0:01:16  lr: 0.032051  loss: 2.1560 (2.1859)  time: 0.4865  data: 0.0001  max mem: 127
[15:27:55.989835] Epoch: [3]  [ 60/195]  eta: 0:01:06  lr: 0.033077  loss: 2.1381 (2.1696)  time: 0.4859  data: 0.0001  max mem: 127
[15:28:05.701125] Epoch: [3]  [ 80/195]  eta: 0:00:56  lr: 0.034103  loss: 2.0864 (2.1519)  time: 0.4855  data: 0.0001  max mem: 127
[15:28:15.340184] Epoch: [3]  [100/195]  eta: 0:00:46  lr: 0.035128  loss: 2.0758 (2.1364)  time: 0.4819  data: 0.0001  max mem: 127
[15:28:25.148846] Epoch: [3]  [120/195]  eta: 0:00:36  lr: 0.036154  loss: 2.0406 (2.1218)  time: 0.4904  data: 0.0001  max mem: 127
[15:28:34.787402] Epoch: [3]  [140/195]  eta: 0:00:26  lr: 0.037179  loss: 2.0129 (2.1072)  time: 0.4819  data: 0.0001  max mem: 127
[15:28:44.443551] Epoch: [3]  [160/195]  eta: 0:00:17  lr: 0.038205  loss: 1.9884 (2.0931)  time: 0.4828  data: 0.0001  max mem: 127
[15:28:54.244365] Epoch: [3]  [180/195]  eta: 0:00:07  lr: 0.039231  loss: 1.9707 (2.0796)  time: 0.4900  data: 0.0002  max mem: 127
[15:29:00.953670] Epoch: [3]  [194/195]  eta: 0:00:00  lr: 0.039949  loss: 1.9663 (2.0716)  time: 0.4831  data: 0.0001  max mem: 127
[15:29:01.011036] Epoch: [3] Total time: 0:01:34 (0.4867 s / it)
[15:29:01.011131] Averaged stats: lr: 0.039949  loss: 1.9663 (2.0716)
[15:27:56.224594] Epoch: [1]  [  0/195]  eta: 0:01:51  lr: 0.000013  loss: 0.8015 (0.8015)  time: 0.5695  data: 0.2358  max mem: 2139
[15:28:02.939980] Epoch: [1]  [ 20/195]  eta: 0:01:00  lr: 0.000014  loss: 0.7691 (0.7724)  time: 0.3357  data: 0.0001  max mem: 2139
[15:28:09.671580] Epoch: [1]  [ 40/195]  eta: 0:00:52  lr: 0.000015  loss: 0.7630 (0.7685)  time: 0.3365  data: 0.0001  max mem: 2139
[15:28:16.281335] Epoch: [1]  [ 60/195]  eta: 0:00:45  lr: 0.000016  loss: 0.7609 (0.7666)  time: 0.3304  data: 0.0002  max mem: 2139
[15:28:23.087072] Epoch: [1]  [ 80/195]  eta: 0:00:38  lr: 0.000018  loss: 0.7709 (0.7666)  time: 0.3402  data: 0.0002  max mem: 2139
[15:28:29.929449] Epoch: [1]  [100/195]  eta: 0:00:32  lr: 0.000019  loss: 0.7641 (0.7657)  time: 0.3421  data: 0.0002  max mem: 2139
[15:28:36.632812] Epoch: [1]  [120/195]  eta: 0:00:25  lr: 0.000020  loss: 0.7579 (0.7643)  time: 0.3351  data: 0.0001  max mem: 2139
[15:28:43.494065] Epoch: [1]  [140/195]  eta: 0:00:18  lr: 0.000021  loss: 0.7526 (0.7636)  time: 0.3430  data: 0.0001  max mem: 2139
[15:28:50.313995] Epoch: [1]  [160/195]  eta: 0:00:11  lr: 0.000023  loss: 0.7600 (0.7630)  time: 0.3410  data: 0.0002  max mem: 2139
[15:28:57.200087] Epoch: [1]  [180/195]  eta: 0:00:05  lr: 0.000024  loss: 0.7463 (0.7619)  time: 0.3443  data: 0.0002  max mem: 2139
[15:29:02.021167] Epoch: [1]  [194/195]  eta: 0:00:00  lr: 0.000025  loss: 0.7478 (0.7612)  time: 0.3444  data: 0.0001  max mem: 2139
[15:29:02.084234] Epoch: [1] Total time: 0:01:06 (0.3407 s / it)
[15:29:02.084293] Averaged stats: lr: 0.000025  loss: 0.7478 (0.7612)
[15:29:02.086620] log_dir: ./output_dir/bmae
[15:27:33.836560] Epoch: [3]  [  0/195]  eta: 0:02:21  lr: 0.030000  loss: 1.9669 (1.9669)  time: 0.7248  data: 0.2440  max mem: 127
[15:27:43.573385] Epoch: [3]  [ 20/195]  eta: 0:01:27  lr: 0.031026  loss: 1.9307 (1.9409)  time: 0.4868  data: 0.0001  max mem: 127
[15:27:53.296202] Epoch: [3]  [ 40/195]  eta: 0:01:16  lr: 0.032051  loss: 1.8727 (1.9112)  time: 0.4861  data: 0.0001  max mem: 127
[15:28:03.015128] Epoch: [3]  [ 60/195]  eta: 0:01:06  lr: 0.033077  loss: 1.8472 (1.8897)  time: 0.4859  data: 0.0001  max mem: 127
[15:28:12.669022] Epoch: [3]  [ 80/195]  eta: 0:00:56  lr: 0.034103  loss: 1.7984 (1.8677)  time: 0.4827  data: 0.0001  max mem: 127
[15:28:22.489517] Epoch: [3]  [100/195]  eta: 0:00:46  lr: 0.035128  loss: 1.7621 (1.8468)  time: 0.4910  data: 0.0001  max mem: 127
[15:28:32.168677] Epoch: [3]  [120/195]  eta: 0:00:36  lr: 0.036154  loss: 1.7284 (1.8274)  time: 0.4839  data: 0.0001  max mem: 127
[15:28:41.796630] Epoch: [3]  [140/195]  eta: 0:00:26  lr: 0.037179  loss: 1.6821 (1.8061)  time: 0.4814  data: 0.0001  max mem: 127
[15:28:51.624518] Epoch: [3]  [160/195]  eta: 0:00:17  lr: 0.038205  loss: 1.6352 (1.7859)  time: 0.4914  data: 0.0005  max mem: 127
[15:29:01.188828] Epoch: [3]  [180/195]  eta: 0:00:07  lr: 0.039231  loss: 1.6306 (1.7687)  time: 0.4782  data: 0.0001  max mem: 127
[15:29:08.037354] Epoch: [3]  [194/195]  eta: 0:00:00  lr: 0.039949  loss: 1.6026 (1.7573)  time: 0.4841  data: 0.0001  max mem: 127
[15:29:08.085563] Epoch: [3] Total time: 0:01:34 (0.4871 s / it)
[15:29:08.085777] Averaged stats: lr: 0.039949  loss: 1.6026 (1.7573)
Not using distributed mode
[15:29:10.936667] job dir: /home/wsj/mae
[15:29:10.936730] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=2,
ema_alpha=0.99,
enable_ema=True,
enable_bootstrap=True,
ema_warmup_epochs=2,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae',
log_dir='./output_dir/bmae',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:29:11.588952] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:29:11.589128] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x727872cafad0>
[15:29:12.112848] use bootstrapped MAE!
[15:29:12.112884] ema enabled!
[15:29:12.132001] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:29:12.132042] base lr: 5.00e-04
[15:29:12.132050] actual lr: 5.00e-04
[15:29:12.132056] accumulate grad iterations: 1
[15:29:12.132061] effective batch size: 256
[15:29:12.133600] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:29:12.133701] Start training for 200 epochs
[15:29:12.135123] log_dir: ./output_dir/bmae
Not using distributed mode
[15:29:13.349440] job dir: /home/wsj/mae
[15:29:13.349484] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=2,
ema_alpha=0.99,
enable_ema=True,
enable_bootstrap=True,
ema_warmup_epochs=4,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae',
log_dir='./output_dir/bmae',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:29:14.000829] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:29:14.001083] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x73e1256ded50>
[15:29:14.575252] use bootstrapped MAE!
[15:29:14.575294] ema enabled!
[15:29:14.703551] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:29:14.703588] base lr: 5.00e-04
[15:29:14.703594] actual lr: 5.00e-04
[15:29:14.703599] accumulate grad iterations: 1
[15:29:14.703604] effective batch size: 256
[15:29:14.704820] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:29:14.704908] Start training for 200 epochs
[15:29:14.706018] log_dir: ./output_dir/bmae
[15:28:10.346845] Epoch: [1]  [  0/195]  eta: 0:01:48  lr: 0.000013  loss: 0.8015 (0.8015)  time: 0.5574  data: 0.2135  max mem: 2139
[15:28:17.122522] Epoch: [1]  [ 20/195]  eta: 0:01:01  lr: 0.000014  loss: 0.7691 (0.7724)  time: 0.3387  data: 0.0002  max mem: 2139
[15:28:23.954035] Epoch: [1]  [ 40/195]  eta: 0:00:53  lr: 0.000015  loss: 0.7630 (0.7685)  time: 0.3415  data: 0.0002  max mem: 2139
[15:28:30.807873] Epoch: [1]  [ 60/195]  eta: 0:00:46  lr: 0.000016  loss: 0.7609 (0.7666)  time: 0.3426  data: 0.0002  max mem: 2139
[15:28:37.502102] Epoch: [1]  [ 80/195]  eta: 0:00:39  lr: 0.000018  loss: 0.7709 (0.7666)  time: 0.3347  data: 0.0002  max mem: 2139
[15:28:44.362553] Epoch: [1]  [100/195]  eta: 0:00:32  lr: 0.000019  loss: 0.7641 (0.7657)  time: 0.3430  data: 0.0002  max mem: 2139
[15:28:51.183233] Epoch: [1]  [120/195]  eta: 0:00:25  lr: 0.000020  loss: 0.7579 (0.7643)  time: 0.3410  data: 0.0002  max mem: 2139
[15:28:58.070235] Epoch: [1]  [140/195]  eta: 0:00:18  lr: 0.000021  loss: 0.7526 (0.7636)  time: 0.3443  data: 0.0002  max mem: 2139
[15:29:04.897400] Epoch: [1]  [160/195]  eta: 0:00:11  lr: 0.000023  loss: 0.7600 (0.7630)  time: 0.3413  data: 0.0002  max mem: 2139
[15:29:11.795059] Epoch: [1]  [180/195]  eta: 0:00:05  lr: 0.000024  loss: 0.7463 (0.7619)  time: 0.3448  data: 0.0002  max mem: 2139
[15:29:16.611716] Epoch: [1]  [194/195]  eta: 0:00:00  lr: 0.000025  loss: 0.7478 (0.7612)  time: 0.3443  data: 0.0001  max mem: 2139
[15:29:16.657369] Epoch: [1] Total time: 0:01:06 (0.3429 s / it)
[15:29:16.657563] Averaged stats: lr: 0.000025  loss: 0.7478 (0.7612)
[15:29:16.660544] log_dir: ./output_dir/bmae
[15:29:01.696309] Test:  [ 0/40]  eta: 0:00:25  loss: 1.8810 (1.8810)  acc1: 37.1094 (37.1094)  acc5: 88.6719 (88.6719)  time: 0.6435  data: 0.1761  max mem: 127
[15:29:06.379711] Test:  [10/40]  eta: 0:00:14  loss: 1.8846 (1.8857)  acc1: 37.5000 (38.0327)  acc5: 88.6719 (88.4943)  time: 0.4842  data: 0.0162  max mem: 127
[15:29:10.985207] Test:  [20/40]  eta: 0:00:09  loss: 1.8842 (1.8818)  acc1: 39.8438 (39.1741)  acc5: 87.8906 (88.0766)  time: 0.4644  data: 0.0002  max mem: 127
[15:29:15.700912] Test:  [30/40]  eta: 0:00:04  loss: 1.8852 (1.8850)  acc1: 40.6250 (39.0499)  acc5: 87.5000 (87.8402)  time: 0.4660  data: 0.0001  max mem: 127
[15:29:19.545784] Test:  [39/40]  eta: 0:00:00  loss: 1.8920 (1.8869)  acc1: 38.2812 (38.6200)  acc5: 87.1094 (87.7300)  time: 0.4514  data: 0.0001  max mem: 127
[15:29:19.591991] Test: Total time: 0:00:18 (0.4635 s / it)
[15:29:19.592042] * Acc@1 38.620 Acc@5 87.730 loss 1.887
[15:29:19.592160] Accuracy of the network on the 10000 test images: 38.6%
[15:29:19.592167] Max accuracy: 38.62%
[15:29:19.593740] log_dir: ./output_dir/mae_mk0.7/linear
Not using distributed mode
[15:29:17.608578] job dir: /home/wsj/mae
[15:29:17.608633] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=2,
ema_alpha=0.99,
enable_ema=True,
enable_bootstrap=True,
ema_warmup_epochs=8,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae',
log_dir='./output_dir/bmae',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:29:18.263208] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:29:18.263379] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x74bc00700910>
[15:29:19.117358] use bootstrapped MAE!
[15:29:19.117388] ema enabled!
[15:29:19.631635] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:29:19.631691] base lr: 5.00e-04
[15:29:19.631697] actual lr: 5.00e-04
[15:29:19.631702] accumulate grad iterations: 1
[15:29:19.631706] effective batch size: 256
[15:29:19.633218] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:29:19.633328] Start training for 200 epochs
[15:29:19.634688] log_dir: ./output_dir/bmae
[15:28:14.520000] Epoch: [1]  [  0/195]  eta: 0:01:44  lr: 0.000013  loss: 0.8015 (0.8015)  time: 0.5343  data: 0.1926  max mem: 2139
[15:28:21.367426] Epoch: [1]  [ 20/195]  eta: 0:01:01  lr: 0.000014  loss: 0.7691 (0.7724)  time: 0.3423  data: 0.0001  max mem: 2139
[15:28:28.217076] Epoch: [1]  [ 40/195]  eta: 0:00:53  lr: 0.000015  loss: 0.7630 (0.7685)  time: 0.3424  data: 0.0002  max mem: 2139
[15:28:34.919267] Epoch: [1]  [ 60/195]  eta: 0:00:46  lr: 0.000016  loss: 0.7609 (0.7666)  time: 0.3351  data: 0.0002  max mem: 2139
[15:28:41.782114] Epoch: [1]  [ 80/195]  eta: 0:00:39  lr: 0.000018  loss: 0.7709 (0.7666)  time: 0.3431  data: 0.0002  max mem: 2139
[15:28:48.648941] Epoch: [1]  [100/195]  eta: 0:00:32  lr: 0.000019  loss: 0.7641 (0.7657)  time: 0.3433  data: 0.0002  max mem: 2139
[15:28:55.481122] Epoch: [1]  [120/195]  eta: 0:00:25  lr: 0.000020  loss: 0.7579 (0.7643)  time: 0.3416  data: 0.0002  max mem: 2139
[15:29:02.308998] Epoch: [1]  [140/195]  eta: 0:00:18  lr: 0.000021  loss: 0.7526 (0.7636)  time: 0.3414  data: 0.0002  max mem: 2139
[15:29:09.203386] Epoch: [1]  [160/195]  eta: 0:00:12  lr: 0.000023  loss: 0.7600 (0.7630)  time: 0.3447  data: 0.0002  max mem: 2139
[15:29:16.141864] Epoch: [1]  [180/195]  eta: 0:00:05  lr: 0.000024  loss: 0.7463 (0.7619)  time: 0.3469  data: 0.0002  max mem: 2139
[15:29:20.906849] Epoch: [1]  [194/195]  eta: 0:00:00  lr: 0.000025  loss: 0.7478 (0.7612)  time: 0.3414  data: 0.0001  max mem: 2139
[15:29:20.953487] Epoch: [1] Total time: 0:01:06 (0.3434 s / it)
[15:29:20.953719] Averaged stats: lr: 0.000025  loss: 0.7478 (0.7612)
[15:29:20.956968] log_dir: ./output_dir/bmae
Not using distributed mode
[15:29:20.319055] job dir: /home/wsj/mae
[15:29:20.319110] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=2,
ema_alpha=0.99,
enable_ema=True,
enable_bootstrap=True,
ema_warmup_epochs=16,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae',
log_dir='./output_dir/bmae',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:29:20.973658] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:29:20.973838] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7456b032cd50>
[15:29:22.306439] use bootstrapped MAE!
[15:29:22.306487] ema enabled!
[15:29:23.440465] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:29:23.440512] base lr: 5.00e-04
[15:29:23.440519] actual lr: 5.00e-04
[15:29:23.440524] accumulate grad iterations: 1
[15:29:23.440528] effective batch size: 256
[15:29:23.442032] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:29:23.442150] Start training for 200 epochs
[15:29:23.443475] log_dir: ./output_dir/bmae
Not using distributed mode
[15:29:22.803055] job dir: /home/wsj/mae
[15:29:22.803114] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=2,
ema_alpha=0.99,
enable_ema=True,
enable_bootstrap=True,
ema_warmup_epochs=32,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae',
log_dir='./output_dir/bmae',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:29:23.451810] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:29:23.452069] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x72025107d6d0>
[15:29:25.052261] use bootstrapped MAE!
[15:29:25.052290] ema enabled!
[15:29:26.506947] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:29:26.506979] base lr: 5.00e-04
[15:29:26.506986] actual lr: 5.00e-04
[15:29:26.506991] accumulate grad iterations: 1
[15:29:26.506995] effective batch size: 256
[15:29:26.508234] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:29:26.508334] Start training for 200 epochs
[15:29:26.509469] log_dir: ./output_dir/bmae
[15:29:08.789762] Test:  [ 0/40]  eta: 0:00:26  loss: 1.4560 (1.4560)  acc1: 50.0000 (50.0000)  acc5: 96.4844 (96.4844)  time: 0.6570  data: 0.1917  max mem: 127
[15:29:13.524802] Test:  [10/40]  eta: 0:00:14  loss: 1.4625 (1.4617)  acc1: 54.6875 (55.1847)  acc5: 95.7031 (95.2415)  time: 0.4901  data: 0.0176  max mem: 127
[15:29:18.242663] Test:  [20/40]  eta: 0:00:09  loss: 1.4464 (1.4602)  acc1: 55.8594 (55.5804)  acc5: 95.7031 (95.4055)  time: 0.4726  data: 0.0003  max mem: 127
[15:29:22.818210] Test:  [30/40]  eta: 0:00:04  loss: 1.4562 (1.4632)  acc1: 56.6406 (55.6956)  acc5: 95.7031 (95.3125)  time: 0.4646  data: 0.0003  max mem: 127
[15:29:26.641865] Test:  [39/40]  eta: 0:00:00  loss: 1.4744 (1.4703)  acc1: 55.0781 (55.2800)  acc5: 95.3125 (95.2700)  time: 0.4435  data: 0.0002  max mem: 127
[15:29:26.704463] Test: Total time: 0:00:18 (0.4643 s / it)
[15:29:26.704519] * Acc@1 55.280 Acc@5 95.270 loss 1.470
[15:29:26.704643] Accuracy of the network on the 10000 test images: 55.3%
[15:29:26.704651] Max accuracy: 55.28%
[15:29:26.706467] log_dir: ./output_dir/mae_mk0.9/linear
Not using distributed mode
[15:29:24.905249] job dir: /home/wsj/mae
[15:29:24.905303] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=2,
ema_alpha=0.99,
enable_ema=True,
enable_bootstrap=True,
ema_warmup_epochs=32,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae',
log_dir='./output_dir/bmae',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:29:25.556430] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:29:25.556605] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7dac5a329a10>
[15:29:26.783465] use bootstrapped MAE!
[15:29:26.783520] ema enabled!
[15:29:28.290526] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:29:28.290564] base lr: 5.00e-04
[15:29:28.290571] actual lr: 5.00e-04
[15:29:28.290576] accumulate grad iterations: 1
[15:29:28.290580] effective batch size: 256
[15:29:28.291980] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:29:28.292092] Start training for 200 epochs
[15:29:28.293354] log_dir: ./output_dir/bmae
[15:28:33.150872] Epoch: [2]  [  0/195]  eta: 0:01:55  lr: 0.000025  loss: 0.7753 (0.7753)  time: 0.5911  data: 0.2510  max mem: 2139
[15:28:39.908732] Epoch: [2]  [ 20/195]  eta: 0:01:01  lr: 0.000026  loss: 0.7487 (0.7510)  time: 0.3379  data: 0.0002  max mem: 2139
[15:28:46.771358] Epoch: [2]  [ 40/195]  eta: 0:00:53  lr: 0.000028  loss: 0.7403 (0.7472)  time: 0.3431  data: 0.0002  max mem: 2139
[15:28:53.597128] Epoch: [2]  [ 60/195]  eta: 0:00:46  lr: 0.000029  loss: 0.7464 (0.7477)  time: 0.3412  data: 0.0002  max mem: 2139
[15:29:00.485899] Epoch: [2]  [ 80/195]  eta: 0:00:39  lr: 0.000030  loss: 0.7519 (0.7483)  time: 0.3444  data: 0.0002  max mem: 2139
[15:29:07.313957] Epoch: [2]  [100/195]  eta: 0:00:32  lr: 0.000031  loss: 0.7501 (0.7479)  time: 0.3414  data: 0.0002  max mem: 2139
[15:29:14.206871] Epoch: [2]  [120/195]  eta: 0:00:25  lr: 0.000033  loss: 0.7447 (0.7472)  time: 0.3446  data: 0.0002  max mem: 2139
[15:29:21.022679] Epoch: [2]  [140/195]  eta: 0:00:18  lr: 0.000034  loss: 0.7401 (0.7463)  time: 0.3407  data: 0.0002  max mem: 2139
[15:29:27.851490] Epoch: [2]  [160/195]  eta: 0:00:12  lr: 0.000035  loss: 0.7350 (0.7451)  time: 0.3414  data: 0.0004  max mem: 2139
[15:29:34.781425] Epoch: [2]  [180/195]  eta: 0:00:05  lr: 0.000037  loss: 0.7238 (0.7431)  time: 0.3464  data: 0.0009  max mem: 2139
[15:29:39.640765] Epoch: [2]  [194/195]  eta: 0:00:00  lr: 0.000037  loss: 0.7280 (0.7421)  time: 0.3470  data: 0.0001  max mem: 2139
[15:29:39.687426] Epoch: [2] Total time: 0:01:07 (0.3442 s / it)
[15:29:39.687532] Averaged stats: lr: 0.000037  loss: 0.7280 (0.7421)
[15:29:39.692043] log_dir: ./output_dir/bmae
Not using distributed mode
[15:29:34.705313] job dir: /home/wsj/mae
[15:29:34.705373] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=2,
ema_alpha=0.99,
enable_ema=True,
enable_bootstrap=True,
ema_warmup_epochs=64,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae',
log_dir='./output_dir/bmae',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:29:35.342051] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:29:35.342230] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7c4ba1f08910>
[15:29:37.606930] use bootstrapped MAE!
[15:29:37.606980] ema enabled!
[15:29:39.871897] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:29:39.871938] base lr: 5.00e-04
[15:29:39.871946] actual lr: 5.00e-04
[15:29:39.871951] accumulate grad iterations: 1
[15:29:39.871957] effective batch size: 256
[15:29:39.873398] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:29:39.873548] Start training for 200 epochs
[15:29:39.874844] log_dir: ./output_dir/bmae
[15:28:34.190627] Epoch: [1]  [  0/195]  eta: 0:01:47  lr: 0.000013  loss: 0.8015 (0.8015)  time: 0.5502  data: 0.2114  max mem: 2139
[15:28:41.052588] Epoch: [1]  [ 20/195]  eta: 0:01:01  lr: 0.000014  loss: 0.7691 (0.7724)  time: 0.3431  data: 0.0001  max mem: 2139
[15:28:47.916199] Epoch: [1]  [ 40/195]  eta: 0:00:53  lr: 0.000015  loss: 0.7630 (0.7685)  time: 0.3431  data: 0.0002  max mem: 2139
[15:28:54.746413] Epoch: [1]  [ 60/195]  eta: 0:00:46  lr: 0.000016  loss: 0.7609 (0.7666)  time: 0.3415  data: 0.0002  max mem: 2139
[15:29:01.634582] Epoch: [1]  [ 80/195]  eta: 0:00:39  lr: 0.000018  loss: 0.7709 (0.7666)  time: 0.3444  data: 0.0002  max mem: 2139
[15:29:08.464701] Epoch: [1]  [100/195]  eta: 0:00:32  lr: 0.000019  loss: 0.7641 (0.7657)  time: 0.3415  data: 0.0001  max mem: 2139
[15:29:15.402664] Epoch: [1]  [120/195]  eta: 0:00:25  lr: 0.000020  loss: 0.7579 (0.7643)  time: 0.3469  data: 0.0002  max mem: 2139
[15:29:22.249165] Epoch: [1]  [140/195]  eta: 0:00:18  lr: 0.000021  loss: 0.7526 (0.7636)  time: 0.3423  data: 0.0007  max mem: 2139
[15:29:29.193493] Epoch: [1]  [160/195]  eta: 0:00:12  lr: 0.000023  loss: 0.7600 (0.7630)  time: 0.3472  data: 0.0005  max mem: 2139
[15:29:36.128796] Epoch: [1]  [180/195]  eta: 0:00:05  lr: 0.000024  loss: 0.7463 (0.7619)  time: 0.3467  data: 0.0002  max mem: 2139
[15:29:40.934173] Epoch: [1]  [194/195]  eta: 0:00:00  lr: 0.000025  loss: 0.7478 (0.7612)  time: 0.3443  data: 0.0008  max mem: 2139
[15:29:40.985574] Epoch: [1] Total time: 0:01:07 (0.3454 s / it)
[15:29:40.985655] Averaged stats: lr: 0.000025  loss: 0.7478 (0.7612)
[15:29:40.987476] shadow updated!
[15:29:41.003700] log_dir: ./output_dir/bmae
[15:28:50.044643] Epoch: [2]  [  0/195]  eta: 0:01:47  lr: 0.000025  loss: 0.7753 (0.7753)  time: 0.5502  data: 0.2120  max mem: 2139
[15:28:56.929726] Epoch: [2]  [ 20/195]  eta: 0:01:01  lr: 0.000026  loss: 0.7487 (0.7510)  time: 0.3442  data: 0.0002  max mem: 2139
[15:29:03.755091] Epoch: [2]  [ 40/195]  eta: 0:00:53  lr: 0.000028  loss: 0.7403 (0.7472)  time: 0.3412  data: 0.0001  max mem: 2139
[15:29:10.652188] Epoch: [2]  [ 60/195]  eta: 0:00:46  lr: 0.000029  loss: 0.7464 (0.7477)  time: 0.3448  data: 0.0002  max mem: 2139
[15:29:17.480689] Epoch: [2]  [ 80/195]  eta: 0:00:39  lr: 0.000030  loss: 0.7519 (0.7483)  time: 0.3414  data: 0.0002  max mem: 2139
[15:29:24.356344] Epoch: [2]  [100/195]  eta: 0:00:32  lr: 0.000031  loss: 0.7501 (0.7479)  time: 0.3437  data: 0.0002  max mem: 2139
[15:29:31.340899] Epoch: [2]  [120/195]  eta: 0:00:25  lr: 0.000033  loss: 0.7447 (0.7472)  time: 0.3492  data: 0.0002  max mem: 2139
[15:29:38.280264] Epoch: [2]  [140/195]  eta: 0:00:19  lr: 0.000034  loss: 0.7401 (0.7463)  time: 0.3469  data: 0.0001  max mem: 2139
[15:29:45.108756] Epoch: [2]  [160/195]  eta: 0:00:12  lr: 0.000035  loss: 0.7350 (0.7451)  time: 0.3414  data: 0.0002  max mem: 2139
[15:29:52.087907] Epoch: [2]  [180/195]  eta: 0:00:05  lr: 0.000037  loss: 0.7238 (0.7431)  time: 0.3489  data: 0.0001  max mem: 2139
[15:29:56.981932] Epoch: [2]  [194/195]  eta: 0:00:00  lr: 0.000037  loss: 0.7280 (0.7421)  time: 0.3496  data: 0.0001  max mem: 2139
[15:29:57.024231] Epoch: [2] Total time: 0:01:07 (0.3463 s / it)
[15:29:57.024294] Averaged stats: lr: 0.000037  loss: 0.7280 (0.7421)
[15:29:57.026552] log_dir: ./output_dir/bmae
[15:29:13.336046] Epoch: [0]  [  0/195]  eta: 0:03:53  lr: 0.000000  loss: 2.5151 (2.5151)  time: 1.1997  data: 0.4210  max mem: 2091
[15:29:14.992691] Epoch: [0]  [ 20/195]  eta: 0:00:23  lr: 0.000001  loss: 2.4594 (2.4379)  time: 0.0827  data: 0.0003  max mem: 2159
[15:29:16.956603] Epoch: [0]  [ 40/195]  eta: 0:00:18  lr: 0.000003  loss: 1.9374 (2.2089)  time: 0.0981  data: 0.0002  max mem: 2159
[15:29:19.374925] Epoch: [0]  [ 60/195]  eta: 0:00:16  lr: 0.000004  loss: 1.4184 (1.9575)  time: 0.1209  data: 0.0001  max mem: 2159
[15:29:22.164025] Epoch: [0]  [ 80/195]  eta: 0:00:14  lr: 0.000005  loss: 1.1293 (1.7559)  time: 0.1394  data: 0.0002  max mem: 2159
[15:29:26.266737] Epoch: [0]  [100/195]  eta: 0:00:13  lr: 0.000006  loss: 0.9976 (1.6066)  time: 0.2051  data: 0.0002  max mem: 2159
[15:29:32.127269] Epoch: [0]  [120/195]  eta: 0:00:12  lr: 0.000008  loss: 0.9240 (1.4949)  time: 0.2930  data: 0.0002  max mem: 2159
[15:29:39.352705] Epoch: [0]  [140/195]  eta: 0:00:10  lr: 0.000009  loss: 0.8550 (1.4052)  time: 0.3612  data: 0.0003  max mem: 2159
[15:29:47.219852] Epoch: [0]  [160/195]  eta: 0:00:07  lr: 0.000010  loss: 0.8223 (1.3331)  time: 0.3933  data: 0.0002  max mem: 2159
[15:29:55.595891] Epoch: [0]  [180/195]  eta: 0:00:03  lr: 0.000012  loss: 0.7954 (1.2738)  time: 0.4188  data: 0.0002  max mem: 2159
[15:30:01.473924] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.000012  loss: 0.7869 (1.2387)  time: 0.4203  data: 0.0001  max mem: 2159
[15:30:01.521065] Epoch: [0] Total time: 0:00:49 (0.2533 s / it)
[15:30:01.521155] Averaged stats: lr: 0.000012  loss: 0.7869 (1.2387)
[15:30:01.758185] log_dir: ./output_dir/bmae
[15:28:28.326442] Epoch: [4]  [  0/195]  eta: 0:02:30  lr: 0.040000  loss: 1.9790 (1.9790)  time: 0.7731  data: 0.2887  max mem: 127
[15:28:37.927676] Epoch: [4]  [ 20/195]  eta: 0:01:26  lr: 0.041026  loss: 2.0502 (2.0436)  time: 0.4800  data: 0.0001  max mem: 127
[15:28:47.733424] Epoch: [4]  [ 40/195]  eta: 0:01:16  lr: 0.042051  loss: 2.0117 (2.0325)  time: 0.4902  data: 0.0001  max mem: 127
[15:28:57.403665] Epoch: [4]  [ 60/195]  eta: 0:01:06  lr: 0.043077  loss: 2.0050 (2.0247)  time: 0.4835  data: 0.0001  max mem: 127
[15:29:07.079673] Epoch: [4]  [ 80/195]  eta: 0:00:56  lr: 0.044103  loss: 1.9878 (2.0179)  time: 0.4838  data: 0.0001  max mem: 127
[15:29:16.825549] Epoch: [4]  [100/195]  eta: 0:00:46  lr: 0.045128  loss: 1.9996 (2.0140)  time: 0.4873  data: 0.0001  max mem: 127
[15:29:26.550038] Epoch: [4]  [120/195]  eta: 0:00:36  lr: 0.046154  loss: 1.9800 (2.0092)  time: 0.4862  data: 0.0001  max mem: 127
[15:29:36.094898] Epoch: [4]  [140/195]  eta: 0:00:26  lr: 0.047179  loss: 1.9657 (2.0043)  time: 0.4772  data: 0.0002  max mem: 127
[15:29:45.789346] Epoch: [4]  [160/195]  eta: 0:00:17  lr: 0.048205  loss: 1.9580 (1.9986)  time: 0.4847  data: 0.0001  max mem: 127
[15:29:55.541161] Epoch: [4]  [180/195]  eta: 0:00:07  lr: 0.049231  loss: 1.9407 (1.9938)  time: 0.4876  data: 0.0001  max mem: 127
[15:30:02.349895] Epoch: [4]  [194/195]  eta: 0:00:00  lr: 0.049949  loss: 1.9574 (1.9912)  time: 0.4867  data: 0.0001  max mem: 127
[15:30:02.399759] Epoch: [4] Total time: 0:01:34 (0.4864 s / it)
[15:30:02.399841] Averaged stats: lr: 0.049949  loss: 1.9574 (1.9912)
[15:29:02.661140] Epoch: [2]  [  0/195]  eta: 0:01:51  lr: 0.000025  loss: 0.7753 (0.7753)  time: 0.5734  data: 0.2327  max mem: 2139
[15:29:09.556392] Epoch: [2]  [ 20/195]  eta: 0:01:02  lr: 0.000026  loss: 0.7487 (0.7510)  time: 0.3447  data: 0.0001  max mem: 2139
[15:29:16.484757] Epoch: [2]  [ 40/195]  eta: 0:00:54  lr: 0.000028  loss: 0.7403 (0.7472)  time: 0.3464  data: 0.0002  max mem: 2139
[15:29:23.294187] Epoch: [2]  [ 60/195]  eta: 0:00:46  lr: 0.000029  loss: 0.7464 (0.7477)  time: 0.3404  data: 0.0002  max mem: 2139
[15:29:30.314738] Epoch: [2]  [ 80/195]  eta: 0:00:40  lr: 0.000030  loss: 0.7519 (0.7483)  time: 0.3510  data: 0.0003  max mem: 2139
[15:29:37.253272] Epoch: [2]  [100/195]  eta: 0:00:33  lr: 0.000031  loss: 0.7501 (0.7479)  time: 0.3469  data: 0.0002  max mem: 2139
[15:29:44.075092] Epoch: [2]  [120/195]  eta: 0:00:26  lr: 0.000033  loss: 0.7447 (0.7472)  time: 0.3411  data: 0.0002  max mem: 2139
[15:29:51.051848] Epoch: [2]  [140/195]  eta: 0:00:19  lr: 0.000034  loss: 0.7401 (0.7463)  time: 0.3488  data: 0.0002  max mem: 2139
[15:29:57.995259] Epoch: [2]  [160/195]  eta: 0:00:12  lr: 0.000035  loss: 0.7350 (0.7451)  time: 0.3471  data: 0.0002  max mem: 2139
[15:30:04.992572] Epoch: [2]  [180/195]  eta: 0:00:05  lr: 0.000037  loss: 0.7238 (0.7431)  time: 0.3498  data: 0.0002  max mem: 2139
[15:30:09.894841] Epoch: [2]  [194/195]  eta: 0:00:00  lr: 0.000037  loss: 0.7280 (0.7421)  time: 0.3500  data: 0.0001  max mem: 2139
[15:30:09.936638] Epoch: [2] Total time: 0:01:07 (0.3479 s / it)
[15:30:09.936702] Averaged stats: lr: 0.000037  loss: 0.7280 (0.7421)
[15:30:09.938940] log_dir: ./output_dir/bmae
[15:29:16.022017] Epoch: [0]  [  0/195]  eta: 0:04:16  lr: 0.000000  loss: 2.5151 (2.5151)  time: 1.3150  data: 0.3501  max mem: 2091
[15:29:18.479164] Epoch: [0]  [ 20/195]  eta: 0:00:31  lr: 0.000001  loss: 2.4594 (2.4379)  time: 0.1228  data: 0.0002  max mem: 2159
[15:29:21.132722] Epoch: [0]  [ 40/195]  eta: 0:00:24  lr: 0.000003  loss: 1.9374 (2.2089)  time: 0.1326  data: 0.0005  max mem: 2159
[15:29:24.663819] Epoch: [0]  [ 60/195]  eta: 0:00:22  lr: 0.000004  loss: 1.4184 (1.9575)  time: 0.1765  data: 0.0002  max mem: 2159
[15:29:29.743932] Epoch: [0]  [ 80/195]  eta: 0:00:21  lr: 0.000005  loss: 1.1293 (1.7559)  time: 0.2540  data: 0.0010  max mem: 2159
[15:29:36.855019] Epoch: [0]  [100/195]  eta: 0:00:20  lr: 0.000006  loss: 0.9976 (1.6066)  time: 0.3555  data: 0.0002  max mem: 2159
[15:29:44.579746] Epoch: [0]  [120/195]  eta: 0:00:18  lr: 0.000008  loss: 0.9240 (1.4949)  time: 0.3862  data: 0.0008  max mem: 2159
[15:29:52.991125] Epoch: [0]  [140/195]  eta: 0:00:14  lr: 0.000009  loss: 0.8550 (1.4052)  time: 0.4205  data: 0.0002  max mem: 2159
[15:30:01.399761] Epoch: [0]  [160/195]  eta: 0:00:10  lr: 0.000010  loss: 0.8223 (1.3331)  time: 0.4204  data: 0.0002  max mem: 2159
[15:30:09.730004] Epoch: [0]  [180/195]  eta: 0:00:04  lr: 0.000012  loss: 0.7954 (1.2738)  time: 0.4165  data: 0.0002  max mem: 2159
[15:30:15.604310] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.000012  loss: 0.7869 (1.2387)  time: 0.4201  data: 0.0001  max mem: 2159
[15:30:15.628034] Epoch: [0] Total time: 0:01:00 (0.3124 s / it)
[15:30:15.628098] Averaged stats: lr: 0.000012  loss: 0.7869 (1.2387)
[15:30:15.853485] log_dir: ./output_dir/bmae
[15:30:03.217416] Test:  [ 0/40]  eta: 0:00:30  loss: 1.9093 (1.9093)  acc1: 34.3750 (34.3750)  acc5: 89.0625 (89.0625)  time: 0.7745  data: 0.3158  max mem: 127
[15:30:07.890680] Test:  [10/40]  eta: 0:00:14  loss: 1.9093 (1.8992)  acc1: 34.7656 (35.2273)  acc5: 86.7188 (86.4347)  time: 0.4952  data: 0.0290  max mem: 127
[15:30:12.567677] Test:  [20/40]  eta: 0:00:09  loss: 1.8956 (1.8858)  acc1: 36.7188 (36.5885)  acc5: 86.7188 (86.6815)  time: 0.4674  data: 0.0003  max mem: 127
[15:30:17.247270] Test:  [30/40]  eta: 0:00:04  loss: 1.8843 (1.8882)  acc1: 36.7188 (36.6557)  acc5: 86.3281 (86.4289)  time: 0.4677  data: 0.0002  max mem: 127
[15:30:21.063759] Test:  [39/40]  eta: 0:00:00  loss: 1.8829 (1.8902)  acc1: 35.9375 (36.2500)  acc5: 85.5469 (86.3400)  time: 0.4481  data: 0.0001  max mem: 127
[15:30:21.116897] Test: Total time: 0:00:18 (0.4669 s / it)
[15:30:21.116959] * Acc@1 36.250 Acc@5 86.340 loss 1.890
[15:30:21.117107] Accuracy of the network on the 10000 test images: 36.2%
[15:30:21.117114] Max accuracy: 36.25%
[15:30:21.118861] log_dir: ./output_dir/mae_mk0.3/linear
[15:29:17.265934] Epoch: [2]  [  0/195]  eta: 0:01:57  lr: 0.000025  loss: 0.7753 (0.7753)  time: 0.6042  data: 0.2570  max mem: 2139
[15:29:24.113788] Epoch: [2]  [ 20/195]  eta: 0:01:02  lr: 0.000026  loss: 0.7487 (0.7510)  time: 0.3423  data: 0.0003  max mem: 2139
[15:29:31.074586] Epoch: [2]  [ 40/195]  eta: 0:00:54  lr: 0.000028  loss: 0.7403 (0.7472)  time: 0.3480  data: 0.0012  max mem: 2139
[15:29:38.014453] Epoch: [2]  [ 60/195]  eta: 0:00:47  lr: 0.000029  loss: 0.7464 (0.7477)  time: 0.3469  data: 0.0002  max mem: 2139
[15:29:44.868535] Epoch: [2]  [ 80/195]  eta: 0:00:40  lr: 0.000030  loss: 0.7519 (0.7483)  time: 0.3427  data: 0.0002  max mem: 2139
[15:29:51.844653] Epoch: [2]  [100/195]  eta: 0:00:33  lr: 0.000031  loss: 0.7501 (0.7479)  time: 0.3488  data: 0.0002  max mem: 2139
[15:29:58.786421] Epoch: [2]  [120/195]  eta: 0:00:26  lr: 0.000033  loss: 0.7447 (0.7472)  time: 0.3470  data: 0.0002  max mem: 2139
[15:30:05.786948] Epoch: [2]  [140/195]  eta: 0:00:19  lr: 0.000034  loss: 0.7401 (0.7463)  time: 0.3500  data: 0.0003  max mem: 2139
[15:30:12.742746] Epoch: [2]  [160/195]  eta: 0:00:12  lr: 0.000035  loss: 0.7350 (0.7451)  time: 0.3477  data: 0.0002  max mem: 2139
[15:30:19.750635] Epoch: [2]  [180/195]  eta: 0:00:05  lr: 0.000037  loss: 0.7238 (0.7431)  time: 0.3503  data: 0.0002  max mem: 2139
[15:30:24.654122] Epoch: [2]  [194/195]  eta: 0:00:00  lr: 0.000037  loss: 0.7280 (0.7421)  time: 0.3502  data: 0.0001  max mem: 2139
[15:30:24.703850] Epoch: [2] Total time: 0:01:08 (0.3489 s / it)
[15:30:24.703934] Averaged stats: lr: 0.000037  loss: 0.7280 (0.7421)
[15:30:24.706930] log_dir: ./output_dir/bmae
[15:29:21.531077] Epoch: [2]  [  0/195]  eta: 0:01:51  lr: 0.000025  loss: 0.7753 (0.7753)  time: 0.5728  data: 0.2312  max mem: 2139
[15:29:28.385643] Epoch: [2]  [ 20/195]  eta: 0:01:01  lr: 0.000026  loss: 0.7487 (0.7510)  time: 0.3427  data: 0.0002  max mem: 2139
[15:29:35.299483] Epoch: [2]  [ 40/195]  eta: 0:00:54  lr: 0.000028  loss: 0.7403 (0.7472)  time: 0.3456  data: 0.0002  max mem: 2139
[15:29:42.153451] Epoch: [2]  [ 60/195]  eta: 0:00:46  lr: 0.000029  loss: 0.7464 (0.7477)  time: 0.3426  data: 0.0002  max mem: 2139
[15:29:49.129862] Epoch: [2]  [ 80/195]  eta: 0:00:39  lr: 0.000030  loss: 0.7519 (0.7483)  time: 0.3488  data: 0.0002  max mem: 2139
[15:29:56.122027] Epoch: [2]  [100/195]  eta: 0:00:33  lr: 0.000031  loss: 0.7501 (0.7479)  time: 0.3496  data: 0.0003  max mem: 2139
[15:30:03.067037] Epoch: [2]  [120/195]  eta: 0:00:26  lr: 0.000033  loss: 0.7447 (0.7472)  time: 0.3472  data: 0.0002  max mem: 2139
[15:30:10.039772] Epoch: [2]  [140/195]  eta: 0:00:19  lr: 0.000034  loss: 0.7401 (0.7463)  time: 0.3486  data: 0.0002  max mem: 2139
[15:30:17.031720] Epoch: [2]  [160/195]  eta: 0:00:12  lr: 0.000035  loss: 0.7350 (0.7451)  time: 0.3496  data: 0.0003  max mem: 2139
[15:30:24.035890] Epoch: [2]  [180/195]  eta: 0:00:05  lr: 0.000037  loss: 0.7238 (0.7431)  time: 0.3502  data: 0.0002  max mem: 2139
[15:30:28.881814] Epoch: [2]  [194/195]  eta: 0:00:00  lr: 0.000037  loss: 0.7280 (0.7421)  time: 0.3474  data: 0.0001  max mem: 2139
[15:30:28.929537] Epoch: [2] Total time: 0:01:07 (0.3486 s / it)
[15:30:28.929629] Averaged stats: lr: 0.000037  loss: 0.7280 (0.7421)
[15:30:28.932528] log_dir: ./output_dir/bmae
[15:28:56.627767] Epoch: [4]  [  0/195]  eta: 0:02:32  lr: 0.040000  loss: 1.9481 (1.9481)  time: 0.7821  data: 0.2933  max mem: 127
[15:29:06.293107] Epoch: [4]  [ 20/195]  eta: 0:01:27  lr: 0.041026  loss: 2.0061 (2.0016)  time: 0.4832  data: 0.0001  max mem: 127
[15:29:16.088763] Epoch: [4]  [ 40/195]  eta: 0:01:16  lr: 0.042051  loss: 1.9733 (1.9902)  time: 0.4897  data: 0.0002  max mem: 127
[15:29:25.834584] Epoch: [4]  [ 60/195]  eta: 0:01:06  lr: 0.043077  loss: 1.9660 (1.9814)  time: 0.4873  data: 0.0001  max mem: 127
[15:29:35.456282] Epoch: [4]  [ 80/195]  eta: 0:00:56  lr: 0.044103  loss: 1.9465 (1.9732)  time: 0.4811  data: 0.0009  max mem: 127
[15:29:45.165156] Epoch: [4]  [100/195]  eta: 0:00:46  lr: 0.045128  loss: 1.9456 (1.9680)  time: 0.4854  data: 0.0001  max mem: 127
[15:29:54.912659] Epoch: [4]  [120/195]  eta: 0:00:36  lr: 0.046154  loss: 1.9254 (1.9617)  time: 0.4873  data: 0.0002  max mem: 127
[15:30:04.536832] Epoch: [4]  [140/195]  eta: 0:00:26  lr: 0.047179  loss: 1.9034 (1.9552)  time: 0.4812  data: 0.0001  max mem: 127
[15:30:14.357915] Epoch: [4]  [160/195]  eta: 0:00:17  lr: 0.048205  loss: 1.9018 (1.9481)  time: 0.4910  data: 0.0001  max mem: 127
[15:30:23.985785] Epoch: [4]  [180/195]  eta: 0:00:07  lr: 0.049231  loss: 1.8831 (1.9419)  time: 0.4814  data: 0.0001  max mem: 127
[15:30:30.806925] Epoch: [4]  [194/195]  eta: 0:00:00  lr: 0.049949  loss: 1.8887 (1.9384)  time: 0.4808  data: 0.0001  max mem: 127
[15:30:30.848904] Epoch: [4] Total time: 0:01:35 (0.4872 s / it)
[15:30:30.848964] Averaged stats: lr: 0.049949  loss: 1.8887 (1.9384)
[15:29:21.554118] Epoch: [0]  [  0/195]  eta: 0:06:14  lr: 0.000000  loss: 2.5151 (2.5151)  time: 1.9184  data: 0.4561  max mem: 2091
[15:29:25.318672] Epoch: [0]  [ 20/195]  eta: 0:00:47  lr: 0.000001  loss: 2.4594 (2.4379)  time: 0.1882  data: 0.0003  max mem: 2159
[15:29:30.703024] Epoch: [0]  [ 40/195]  eta: 0:00:41  lr: 0.000003  loss: 1.9374 (2.2089)  time: 0.2692  data: 0.0008  max mem: 2159
[15:29:37.927168] Epoch: [0]  [ 60/195]  eta: 0:00:40  lr: 0.000004  loss: 1.4184 (1.9575)  time: 0.3612  data: 0.0002  max mem: 2159
[15:29:45.613468] Epoch: [0]  [ 80/195]  eta: 0:00:36  lr: 0.000005  loss: 1.1293 (1.7559)  time: 0.3843  data: 0.0003  max mem: 2159
[15:29:54.032242] Epoch: [0]  [100/195]  eta: 0:00:32  lr: 0.000006  loss: 0.9976 (1.6066)  time: 0.4209  data: 0.0002  max mem: 2159
[15:30:02.330965] Epoch: [0]  [120/195]  eta: 0:00:26  lr: 0.000008  loss: 0.9240 (1.4949)  time: 0.4149  data: 0.0002  max mem: 2159
[15:30:10.744938] Epoch: [0]  [140/195]  eta: 0:00:19  lr: 0.000009  loss: 0.8550 (1.4052)  time: 0.4207  data: 0.0002  max mem: 2159
[15:30:19.045398] Epoch: [0]  [160/195]  eta: 0:00:12  lr: 0.000010  loss: 0.8223 (1.3331)  time: 0.4150  data: 0.0002  max mem: 2159
[15:30:27.459905] Epoch: [0]  [180/195]  eta: 0:00:05  lr: 0.000012  loss: 0.7954 (1.2738)  time: 0.4207  data: 0.0002  max mem: 2159
[15:30:33.353382] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.000012  loss: 0.7869 (1.2387)  time: 0.4207  data: 0.0001  max mem: 2159
[15:30:33.382898] Epoch: [0] Total time: 0:01:13 (0.3782 s / it)
[15:30:33.382962] Averaged stats: lr: 0.000012  loss: 0.7869 (1.2387)
[15:30:33.625308] log_dir: ./output_dir/bmae
[15:29:25.477367] Epoch: [0]  [  0/195]  eta: 0:06:36  lr: 0.000000  loss: 2.5151 (2.5151)  time: 2.0325  data: 0.4510  max mem: 2091
[15:29:31.071958] Epoch: [0]  [ 20/195]  eta: 0:01:03  lr: 0.000001  loss: 2.4594 (2.4379)  time: 0.2797  data: 0.0006  max mem: 2159
[15:29:38.484687] Epoch: [0]  [ 40/195]  eta: 0:00:56  lr: 0.000003  loss: 1.9374 (2.2089)  time: 0.3706  data: 0.0002  max mem: 2159
[15:29:46.368871] Epoch: [0]  [ 60/195]  eta: 0:00:50  lr: 0.000004  loss: 1.4184 (1.9575)  time: 0.3942  data: 0.0003  max mem: 2159
[15:29:54.878880] Epoch: [0]  [ 80/195]  eta: 0:00:44  lr: 0.000005  loss: 1.1293 (1.7559)  time: 0.4255  data: 0.0002  max mem: 2159
[15:30:03.371204] Epoch: [0]  [100/195]  eta: 0:00:37  lr: 0.000006  loss: 0.9976 (1.6066)  time: 0.4246  data: 0.0002  max mem: 2159
[15:30:11.929231] Epoch: [0]  [120/195]  eta: 0:00:30  lr: 0.000008  loss: 0.9240 (1.4949)  time: 0.4279  data: 0.0002  max mem: 2159
[15:30:20.399397] Epoch: [0]  [140/195]  eta: 0:00:22  lr: 0.000009  loss: 0.8550 (1.4052)  time: 0.4235  data: 0.0002  max mem: 2159
[15:30:28.950078] Epoch: [0]  [160/195]  eta: 0:00:14  lr: 0.000010  loss: 0.8223 (1.3331)  time: 0.4275  data: 0.0002  max mem: 2159
[15:30:37.394574] Epoch: [0]  [180/195]  eta: 0:00:06  lr: 0.000012  loss: 0.7954 (1.2738)  time: 0.4222  data: 0.0002  max mem: 2159
[15:30:43.387108] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.000012  loss: 0.7869 (1.2387)  time: 0.4287  data: 0.0001  max mem: 2159
[15:30:43.426955] Epoch: [0] Total time: 0:01:19 (0.4102 s / it)
[15:30:43.427040] Averaged stats: lr: 0.000012  loss: 0.7869 (1.2387)
[15:30:43.673957] log_dir: ./output_dir/bmae
[15:29:40.265473] Epoch: [3]  [  0/195]  eta: 0:01:51  lr: 0.000038  loss: 0.7568 (0.7568)  time: 0.5723  data: 0.2446  max mem: 2139
[15:29:47.171719] Epoch: [3]  [ 20/195]  eta: 0:01:02  lr: 0.000039  loss: 0.7184 (0.7239)  time: 0.3453  data: 0.0002  max mem: 2139
[15:29:54.160659] Epoch: [3]  [ 40/195]  eta: 0:00:54  lr: 0.000040  loss: 0.7138 (0.7214)  time: 0.3494  data: 0.0002  max mem: 2139
[15:30:01.099937] Epoch: [3]  [ 60/195]  eta: 0:00:47  lr: 0.000041  loss: 0.7225 (0.7220)  time: 0.3469  data: 0.0002  max mem: 2139
[15:30:08.101565] Epoch: [3]  [ 80/195]  eta: 0:00:40  lr: 0.000043  loss: 0.7221 (0.7222)  time: 0.3500  data: 0.0002  max mem: 2139
[15:30:15.060663] Epoch: [3]  [100/195]  eta: 0:00:33  lr: 0.000044  loss: 0.7228 (0.7218)  time: 0.3479  data: 0.0002  max mem: 2139
[15:30:22.066151] Epoch: [3]  [120/195]  eta: 0:00:26  lr: 0.000045  loss: 0.7192 (0.7215)  time: 0.3502  data: 0.0002  max mem: 2139
[15:30:28.989638] Epoch: [3]  [140/195]  eta: 0:00:19  lr: 0.000046  loss: 0.7177 (0.7210)  time: 0.3461  data: 0.0002  max mem: 2139
[15:30:35.964701] Epoch: [3]  [160/195]  eta: 0:00:12  lr: 0.000048  loss: 0.7207 (0.7206)  time: 0.3487  data: 0.0002  max mem: 2139
[15:30:42.971352] Epoch: [3]  [180/195]  eta: 0:00:05  lr: 0.000049  loss: 0.7015 (0.7189)  time: 0.3503  data: 0.0002  max mem: 2139
[15:30:47.864985] Epoch: [3]  [194/195]  eta: 0:00:00  lr: 0.000050  loss: 0.6999 (0.7178)  time: 0.3497  data: 0.0001  max mem: 2139
[15:30:47.911638] Epoch: [3] Total time: 0:01:08 (0.3498 s / it)
[15:30:47.911734] Averaged stats: lr: 0.000050  loss: 0.6999 (0.7178)
[15:30:47.914817] log_dir: ./output_dir/bmae
[15:30:31.586213] Test:  [ 0/40]  eta: 0:00:27  loss: 1.8191 (1.8191)  acc1: 37.8906 (37.8906)  acc5: 91.0156 (91.0156)  time: 0.6943  data: 0.2311  max mem: 127
[15:30:36.271871] Test:  [10/40]  eta: 0:00:14  loss: 1.8249 (1.8214)  acc1: 37.8906 (38.0327)  acc5: 88.6719 (88.3523)  time: 0.4890  data: 0.0212  max mem: 127
[15:30:40.942019] Test:  [20/40]  eta: 0:00:09  loss: 1.8196 (1.8089)  acc1: 39.8438 (39.1927)  acc5: 88.6719 (88.8207)  time: 0.4677  data: 0.0002  max mem: 127
[15:30:45.629446] Test:  [30/40]  eta: 0:00:04  loss: 1.8158 (1.8124)  acc1: 39.8438 (39.0121)  acc5: 88.2812 (88.5585)  time: 0.4678  data: 0.0001  max mem: 127
[15:30:49.448340] Test:  [39/40]  eta: 0:00:00  loss: 1.8222 (1.8157)  acc1: 37.8906 (38.4700)  acc5: 88.2812 (88.5800)  time: 0.4487  data: 0.0001  max mem: 127
[15:30:49.497127] Test: Total time: 0:00:18 (0.4651 s / it)
[15:30:49.497173] * Acc@1 38.470 Acc@5 88.580 loss 1.816
[15:30:49.497267] Accuracy of the network on the 10000 test images: 38.5%
[15:30:49.497274] Max accuracy: 38.47%
[15:30:49.498791] log_dir: ./output_dir/mae_mk0.5/linear
[15:29:28.855304] Epoch: [0]  [  0/195]  eta: 0:07:37  lr: 0.000000  loss: 2.5151 (2.5151)  time: 2.3447  data: 0.3765  max mem: 2091
[15:29:35.975286] Epoch: [0]  [ 20/195]  eta: 0:01:18  lr: 0.000001  loss: 2.4594 (2.4379)  time: 0.3560  data: 0.0002  max mem: 2159
[15:29:43.631836] Epoch: [0]  [ 40/195]  eta: 0:01:04  lr: 0.000003  loss: 1.9374 (2.2089)  time: 0.3828  data: 0.0002  max mem: 2159
[15:29:52.218049] Epoch: [0]  [ 60/195]  eta: 0:00:56  lr: 0.000004  loss: 1.4184 (1.9575)  time: 0.4293  data: 0.0002  max mem: 2159
[15:30:00.808111] Epoch: [0]  [ 80/195]  eta: 0:00:48  lr: 0.000005  loss: 1.1293 (1.7559)  time: 0.4295  data: 0.0002  max mem: 2159
[15:30:09.285987] Epoch: [0]  [100/195]  eta: 0:00:40  lr: 0.000006  loss: 0.9976 (1.6066)  time: 0.4239  data: 0.0002  max mem: 2159
[15:30:17.791650] Epoch: [0]  [120/195]  eta: 0:00:31  lr: 0.000008  loss: 0.9240 (1.4949)  time: 0.4252  data: 0.0002  max mem: 2159
[15:30:26.310312] Epoch: [0]  [140/195]  eta: 0:00:23  lr: 0.000009  loss: 0.8550 (1.4052)  time: 0.4259  data: 0.0002  max mem: 2159
[15:30:34.843509] Epoch: [0]  [160/195]  eta: 0:00:14  lr: 0.000010  loss: 0.8223 (1.3331)  time: 0.4266  data: 0.0002  max mem: 2159
[15:30:43.454118] Epoch: [0]  [180/195]  eta: 0:00:06  lr: 0.000012  loss: 0.7954 (1.2738)  time: 0.4305  data: 0.0002  max mem: 2159
[15:30:49.395812] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.000012  loss: 0.7869 (1.2387)  time: 0.4265  data: 0.0001  max mem: 2159
[15:30:49.441832] Epoch: [0] Total time: 0:01:22 (0.4253 s / it)
[15:30:49.441942] Averaged stats: lr: 0.000012  loss: 0.7869 (1.2387)
[15:30:49.681234] log_dir: ./output_dir/bmae
[15:29:30.727312] Epoch: [0]  [  0/195]  eta: 0:07:54  lr: 0.000000  loss: 2.5151 (2.5151)  time: 2.4329  data: 0.4105  max mem: 2091
[15:29:38.012131] Epoch: [0]  [ 20/195]  eta: 0:01:20  lr: 0.000001  loss: 2.4594 (2.4379)  time: 0.3642  data: 0.0002  max mem: 2159
[15:29:45.902841] Epoch: [0]  [ 40/195]  eta: 0:01:06  lr: 0.000003  loss: 1.9374 (2.2089)  time: 0.3945  data: 0.0009  max mem: 2159
[15:29:54.399896] Epoch: [0]  [ 60/195]  eta: 0:00:57  lr: 0.000004  loss: 1.4184 (1.9575)  time: 0.4248  data: 0.0002  max mem: 2159
[15:30:02.806013] Epoch: [0]  [ 80/195]  eta: 0:00:48  lr: 0.000005  loss: 1.1293 (1.7559)  time: 0.4203  data: 0.0002  max mem: 2159
[15:30:11.307302] Epoch: [0]  [100/195]  eta: 0:00:40  lr: 0.000006  loss: 0.9976 (1.6066)  time: 0.4250  data: 0.0002  max mem: 2159
[15:30:19.772060] Epoch: [0]  [120/195]  eta: 0:00:31  lr: 0.000008  loss: 0.9240 (1.4949)  time: 0.4232  data: 0.0002  max mem: 2159
[15:30:28.293793] Epoch: [0]  [140/195]  eta: 0:00:23  lr: 0.000009  loss: 0.8550 (1.4052)  time: 0.4260  data: 0.0002  max mem: 2159
[15:30:36.771082] Epoch: [0]  [160/195]  eta: 0:00:14  lr: 0.000010  loss: 0.8223 (1.3331)  time: 0.4238  data: 0.0002  max mem: 2159
[15:30:45.194630] Epoch: [0]  [180/195]  eta: 0:00:06  lr: 0.000012  loss: 0.7954 (1.2738)  time: 0.4211  data: 0.0002  max mem: 2159
[15:30:51.058017] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.000012  loss: 0.7869 (1.2387)  time: 0.4170  data: 0.0001  max mem: 2159
[15:30:51.086162] Epoch: [0] Total time: 0:01:22 (0.4246 s / it)
[15:30:51.086227] Averaged stats: lr: 0.000012  loss: 0.7869 (1.2387)
[15:30:51.308712] log_dir: ./output_dir/bmae
[15:29:20.370675] Epoch: [4]  [  0/195]  eta: 0:02:31  lr: 0.040000  loss: 1.9007 (1.9007)  time: 0.7761  data: 0.2905  max mem: 127
[15:29:29.971105] Epoch: [4]  [ 20/195]  eta: 0:01:26  lr: 0.041026  loss: 1.9443 (1.9434)  time: 0.4800  data: 0.0001  max mem: 127
[15:29:39.670298] Epoch: [4]  [ 40/195]  eta: 0:01:15  lr: 0.042051  loss: 1.9229 (1.9331)  time: 0.4849  data: 0.0001  max mem: 127
[15:29:49.407092] Epoch: [4]  [ 60/195]  eta: 0:01:05  lr: 0.043077  loss: 1.9209 (1.9269)  time: 0.4868  data: 0.0001  max mem: 127
[15:29:59.146770] Epoch: [4]  [ 80/195]  eta: 0:00:56  lr: 0.044103  loss: 1.8964 (1.9184)  time: 0.4869  data: 0.0001  max mem: 127
[15:30:08.800419] Epoch: [4]  [100/195]  eta: 0:00:46  lr: 0.045128  loss: 1.8871 (1.9121)  time: 0.4826  data: 0.0002  max mem: 127
[15:30:18.625657] Epoch: [4]  [120/195]  eta: 0:00:36  lr: 0.046154  loss: 1.8618 (1.9050)  time: 0.4912  data: 0.0001  max mem: 127
[15:30:28.208896] Epoch: [4]  [140/195]  eta: 0:00:26  lr: 0.047179  loss: 1.8499 (1.8978)  time: 0.4791  data: 0.0001  max mem: 127
[15:30:37.877244] Epoch: [4]  [160/195]  eta: 0:00:17  lr: 0.048205  loss: 1.8411 (1.8903)  time: 0.4834  data: 0.0001  max mem: 127
[15:30:47.687271] Epoch: [4]  [180/195]  eta: 0:00:07  lr: 0.049231  loss: 1.8307 (1.8837)  time: 0.4905  data: 0.0002  max mem: 127
[15:30:54.427684] Epoch: [4]  [194/195]  eta: 0:00:00  lr: 0.049949  loss: 1.8228 (1.8794)  time: 0.4841  data: 0.0002  max mem: 127
[15:30:54.489703] Epoch: [4] Total time: 0:01:34 (0.4866 s / it)
[15:30:54.489784] Averaged stats: lr: 0.049949  loss: 1.8228 (1.8794)
[15:29:27.601219] Epoch: [4]  [  0/195]  eta: 0:02:54  lr: 0.040000  loss: 1.5770 (1.5770)  time: 0.8938  data: 0.4151  max mem: 127
[15:29:37.308199] Epoch: [4]  [ 20/195]  eta: 0:01:28  lr: 0.041026  loss: 1.5588 (1.5729)  time: 0.4853  data: 0.0001  max mem: 127
[15:29:47.040532] Epoch: [4]  [ 40/195]  eta: 0:01:16  lr: 0.042051  loss: 1.5520 (1.5592)  time: 0.4866  data: 0.0013  max mem: 127
[15:29:56.791002] Epoch: [4]  [ 60/195]  eta: 0:01:06  lr: 0.043077  loss: 1.5496 (1.5552)  time: 0.4875  data: 0.0001  max mem: 127
[15:30:06.404565] Epoch: [4]  [ 80/195]  eta: 0:00:56  lr: 0.044103  loss: 1.5184 (1.5451)  time: 0.4806  data: 0.0001  max mem: 127
[15:30:16.214284] Epoch: [4]  [100/195]  eta: 0:00:46  lr: 0.045128  loss: 1.4841 (1.5342)  time: 0.4904  data: 0.0001  max mem: 127
[15:30:25.828809] Epoch: [4]  [120/195]  eta: 0:00:36  lr: 0.046154  loss: 1.4717 (1.5240)  time: 0.4807  data: 0.0001  max mem: 127
[15:30:35.492062] Epoch: [4]  [140/195]  eta: 0:00:26  lr: 0.047179  loss: 1.4511 (1.5139)  time: 0.4831  data: 0.0001  max mem: 127
[15:30:45.316048] Epoch: [4]  [160/195]  eta: 0:00:17  lr: 0.048205  loss: 1.4153 (1.5037)  time: 0.4912  data: 0.0001  max mem: 127
[15:30:54.904608] Epoch: [4]  [180/195]  eta: 0:00:07  lr: 0.049231  loss: 1.4291 (1.4969)  time: 0.4794  data: 0.0001  max mem: 127
[15:31:01.775487] Epoch: [4]  [194/195]  eta: 0:00:00  lr: 0.049949  loss: 1.4233 (1.4920)  time: 0.4848  data: 0.0001  max mem: 127
[15:31:01.813641] Epoch: [4] Total time: 0:01:35 (0.4877 s / it)
[15:31:01.813701] Averaged stats: lr: 0.049949  loss: 1.4233 (1.4920)
[15:29:42.632088] Epoch: [0]  [  0/195]  eta: 0:08:57  lr: 0.000000  loss: 2.5151 (2.5151)  time: 2.7561  data: 0.4723  max mem: 2091
[15:29:51.134466] Epoch: [0]  [ 20/195]  eta: 0:01:33  lr: 0.000001  loss: 2.4594 (2.4379)  time: 0.4251  data: 0.0002  max mem: 2159
[15:29:59.597472] Epoch: [0]  [ 40/195]  eta: 0:01:14  lr: 0.000003  loss: 1.9374 (2.2089)  time: 0.4231  data: 0.0002  max mem: 2159
[15:30:07.948940] Epoch: [0]  [ 60/195]  eta: 0:01:02  lr: 0.000004  loss: 1.4184 (1.9575)  time: 0.4175  data: 0.0002  max mem: 2159
[15:30:16.314275] Epoch: [0]  [ 80/195]  eta: 0:00:51  lr: 0.000005  loss: 1.1293 (1.7559)  time: 0.4182  data: 0.0002  max mem: 2159
[15:30:24.795194] Epoch: [0]  [100/195]  eta: 0:00:42  lr: 0.000006  loss: 0.9976 (1.6066)  time: 0.4240  data: 0.0002  max mem: 2159
[15:30:33.238556] Epoch: [0]  [120/195]  eta: 0:00:33  lr: 0.000008  loss: 0.9240 (1.4949)  time: 0.4221  data: 0.0002  max mem: 2159
[15:30:41.624131] Epoch: [0]  [140/195]  eta: 0:00:24  lr: 0.000009  loss: 0.8550 (1.4052)  time: 0.4192  data: 0.0002  max mem: 2159
[15:30:49.937177] Epoch: [0]  [160/195]  eta: 0:00:15  lr: 0.000010  loss: 0.8223 (1.3331)  time: 0.4156  data: 0.0002  max mem: 2159
[15:30:58.376077] Epoch: [0]  [180/195]  eta: 0:00:06  lr: 0.000012  loss: 0.7954 (1.2738)  time: 0.4219  data: 0.0002  max mem: 2159
[15:31:04.313927] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.000012  loss: 0.7869 (1.2387)  time: 0.4239  data: 0.0001  max mem: 2159
[15:31:04.355473] Epoch: [0] Total time: 0:01:24 (0.4332 s / it)
[15:31:04.355538] Averaged stats: lr: 0.000012  loss: 0.7869 (1.2387)
[15:31:04.591756] log_dir: ./output_dir/bmae
[15:29:57.581845] Epoch: [3]  [  0/195]  eta: 0:01:48  lr: 0.000038  loss: 0.7568 (0.7568)  time: 0.5541  data: 0.2119  max mem: 2139
[15:30:04.579174] Epoch: [3]  [ 20/195]  eta: 0:01:02  lr: 0.000039  loss: 0.7184 (0.7239)  time: 0.3498  data: 0.0002  max mem: 2139
[15:30:11.534843] Epoch: [3]  [ 40/195]  eta: 0:00:54  lr: 0.000040  loss: 0.7138 (0.7214)  time: 0.3477  data: 0.0002  max mem: 2139
[15:30:18.544263] Epoch: [3]  [ 60/195]  eta: 0:00:47  lr: 0.000041  loss: 0.7225 (0.7220)  time: 0.3504  data: 0.0002  max mem: 2139
[15:30:25.485620] Epoch: [3]  [ 80/195]  eta: 0:00:40  lr: 0.000043  loss: 0.7221 (0.7222)  time: 0.3470  data: 0.0002  max mem: 2139
[15:30:32.440988] Epoch: [3]  [100/195]  eta: 0:00:33  lr: 0.000044  loss: 0.7228 (0.7218)  time: 0.3477  data: 0.0002  max mem: 2139
[15:30:39.447035] Epoch: [3]  [120/195]  eta: 0:00:26  lr: 0.000045  loss: 0.7192 (0.7215)  time: 0.3503  data: 0.0002  max mem: 2139
[15:30:46.446728] Epoch: [3]  [140/195]  eta: 0:00:19  lr: 0.000046  loss: 0.7177 (0.7210)  time: 0.3499  data: 0.0002  max mem: 2139
[15:30:53.387999] Epoch: [3]  [160/195]  eta: 0:00:12  lr: 0.000048  loss: 0.7207 (0.7206)  time: 0.3470  data: 0.0002  max mem: 2139
[15:31:00.379704] Epoch: [3]  [180/195]  eta: 0:00:05  lr: 0.000049  loss: 0.7015 (0.7189)  time: 0.3495  data: 0.0002  max mem: 2139
[15:31:05.269336] Epoch: [3]  [194/195]  eta: 0:00:00  lr: 0.000050  loss: 0.6999 (0.7178)  time: 0.3493  data: 0.0001  max mem: 2139
[15:31:05.306940] Epoch: [3] Total time: 0:01:08 (0.3502 s / it)
[15:31:05.307004] Averaged stats: lr: 0.000050  loss: 0.6999 (0.7178)
[15:31:05.309273] log_dir: ./output_dir/bmae
[15:30:55.198998] Test:  [ 0/40]  eta: 0:00:26  loss: 1.7185 (1.7185)  acc1: 42.1875 (42.1875)  acc5: 90.6250 (90.6250)  time: 0.6665  data: 0.2032  max mem: 127
[15:30:59.893883] Test:  [10/40]  eta: 0:00:14  loss: 1.7334 (1.7282)  acc1: 41.7969 (42.7557)  acc5: 89.8438 (89.7372)  time: 0.4873  data: 0.0187  max mem: 127
[15:31:04.502107] Test:  [20/40]  eta: 0:00:09  loss: 1.7281 (1.7226)  acc1: 44.5312 (43.5082)  acc5: 89.8438 (89.7879)  time: 0.4651  data: 0.0002  max mem: 127
[15:31:09.206463] Test:  [30/40]  eta: 0:00:04  loss: 1.7281 (1.7264)  acc1: 44.1406 (43.0444)  acc5: 89.8438 (89.6799)  time: 0.4655  data: 0.0002  max mem: 127
[15:31:13.048201] Test:  [39/40]  eta: 0:00:00  loss: 1.7353 (1.7284)  acc1: 41.0156 (42.5200)  acc5: 89.4531 (89.6500)  time: 0.4506  data: 0.0001  max mem: 127
[15:31:13.090363] Test: Total time: 0:00:18 (0.4640 s / it)
[15:31:13.090421] * Acc@1 42.520 Acc@5 89.650 loss 1.728
[15:31:13.090562] Accuracy of the network on the 10000 test images: 42.5%
[15:31:13.090568] Max accuracy: 42.52%
[15:31:13.092323] log_dir: ./output_dir/mae_mk0.7/linear
[15:30:10.483263] Epoch: [3]  [  0/195]  eta: 0:01:46  lr: 0.000038  loss: 0.7568 (0.7568)  time: 0.5436  data: 0.1987  max mem: 2139
[15:30:17.492739] Epoch: [3]  [ 20/195]  eta: 0:01:02  lr: 0.000039  loss: 0.7184 (0.7239)  time: 0.3504  data: 0.0002  max mem: 2139
[15:30:24.510842] Epoch: [3]  [ 40/195]  eta: 0:00:55  lr: 0.000040  loss: 0.7138 (0.7214)  time: 0.3509  data: 0.0002  max mem: 2139
[15:30:31.401978] Epoch: [3]  [ 60/195]  eta: 0:00:47  lr: 0.000041  loss: 0.7225 (0.7220)  time: 0.3445  data: 0.0002  max mem: 2139
[15:30:38.409539] Epoch: [3]  [ 80/195]  eta: 0:00:40  lr: 0.000043  loss: 0.7221 (0.7222)  time: 0.3503  data: 0.0002  max mem: 2139
[15:30:45.411507] Epoch: [3]  [100/195]  eta: 0:00:33  lr: 0.000044  loss: 0.7228 (0.7218)  time: 0.3501  data: 0.0002  max mem: 2139
[15:30:52.354824] Epoch: [3]  [120/195]  eta: 0:00:26  lr: 0.000045  loss: 0.7192 (0.7215)  time: 0.3471  data: 0.0002  max mem: 2139
[15:30:59.345060] Epoch: [3]  [140/195]  eta: 0:00:19  lr: 0.000046  loss: 0.7177 (0.7210)  time: 0.3495  data: 0.0002  max mem: 2139
[15:31:06.278904] Epoch: [3]  [160/195]  eta: 0:00:12  lr: 0.000048  loss: 0.7207 (0.7206)  time: 0.3466  data: 0.0002  max mem: 2139
[15:31:13.263114] Epoch: [3]  [180/195]  eta: 0:00:05  lr: 0.000049  loss: 0.7015 (0.7189)  time: 0.3492  data: 0.0002  max mem: 2139
[15:31:18.151983] Epoch: [3]  [194/195]  eta: 0:00:00  lr: 0.000050  loss: 0.6999 (0.7178)  time: 0.3491  data: 0.0001  max mem: 2139
[15:31:18.180298] Epoch: [3] Total time: 0:01:08 (0.3500 s / it)
[15:31:18.180367] Averaged stats: lr: 0.000050  loss: 0.6999 (0.7178)
[15:31:18.182627] log_dir: ./output_dir/bmae
[15:31:02.501574] Test:  [ 0/40]  eta: 0:00:25  loss: 1.2626 (1.2626)  acc1: 52.7344 (52.7344)  acc5: 96.8750 (96.8750)  time: 0.6497  data: 0.1814  max mem: 127
[15:31:07.213121] Test:  [10/40]  eta: 0:00:14  loss: 1.2810 (1.2789)  acc1: 57.4219 (57.2088)  acc5: 95.7031 (95.4190)  time: 0.4873  data: 0.0167  max mem: 127
[15:31:11.923006] Test:  [20/40]  eta: 0:00:09  loss: 1.2625 (1.2742)  acc1: 57.8125 (57.4219)  acc5: 95.7031 (95.6287)  time: 0.4710  data: 0.0002  max mem: 127
[15:31:16.529187] Test:  [30/40]  eta: 0:00:04  loss: 1.2596 (1.2765)  acc1: 58.2031 (57.8251)  acc5: 95.7031 (95.7031)  time: 0.4657  data: 0.0002  max mem: 127
[15:31:20.340639] Test:  [39/40]  eta: 0:00:00  loss: 1.2927 (1.2856)  acc1: 56.6406 (57.3800)  acc5: 95.3125 (95.6300)  time: 0.4443  data: 0.0002  max mem: 127
[15:31:20.385559] Test: Total time: 0:00:18 (0.4634 s / it)
[15:31:20.385628] * Acc@1 57.380 Acc@5 95.630 loss 1.286
[15:31:20.385771] Accuracy of the network on the 10000 test images: 57.4%
[15:31:20.385778] Max accuracy: 57.38%
[15:31:20.387622] log_dir: ./output_dir/mae_mk0.9/linear
[15:30:02.447867] Epoch: [1]  [  0/195]  eta: 0:02:14  lr: 0.000013  loss: 0.8015 (0.8015)  time: 0.6887  data: 0.2716  max mem: 2159
[15:30:10.848468] Epoch: [1]  [ 20/195]  eta: 0:01:15  lr: 0.000014  loss: 0.7691 (0.7724)  time: 0.4200  data: 0.0002  max mem: 2159
[15:30:19.163141] Epoch: [1]  [ 40/195]  eta: 0:01:05  lr: 0.000015  loss: 0.7630 (0.7685)  time: 0.4157  data: 0.0002  max mem: 2159
[15:30:27.547867] Epoch: [1]  [ 60/195]  eta: 0:00:57  lr: 0.000016  loss: 0.7609 (0.7666)  time: 0.4192  data: 0.0002  max mem: 2159
[15:30:35.841709] Epoch: [1]  [ 80/195]  eta: 0:00:48  lr: 0.000018  loss: 0.7709 (0.7666)  time: 0.4147  data: 0.0003  max mem: 2159
[15:30:44.096558] Epoch: [1]  [100/195]  eta: 0:00:39  lr: 0.000019  loss: 0.7641 (0.7657)  time: 0.4127  data: 0.0002  max mem: 2159
[15:30:52.265871] Epoch: [1]  [120/195]  eta: 0:00:31  lr: 0.000020  loss: 0.7579 (0.7643)  time: 0.4084  data: 0.0003  max mem: 2159
[15:31:00.641533] Epoch: [1]  [140/195]  eta: 0:00:22  lr: 0.000021  loss: 0.7526 (0.7636)  time: 0.4187  data: 0.0003  max mem: 2159
[15:31:08.918710] Epoch: [1]  [160/195]  eta: 0:00:14  lr: 0.000023  loss: 0.7600 (0.7630)  time: 0.4138  data: 0.0002  max mem: 2159
[15:31:17.328147] Epoch: [1]  [180/195]  eta: 0:00:06  lr: 0.000024  loss: 0.7463 (0.7619)  time: 0.4204  data: 0.0003  max mem: 2159
[15:31:23.180146] Epoch: [1]  [194/195]  eta: 0:00:00  lr: 0.000025  loss: 0.7478 (0.7612)  time: 0.4187  data: 0.0001  max mem: 2159
[15:31:23.231215] Epoch: [1] Total time: 0:01:21 (0.4178 s / it)
[15:31:23.231452] Averaged stats: lr: 0.000025  loss: 0.7478 (0.7612)
[15:31:23.234542] log_dir: ./output_dir/bmae
[15:30:25.319732] Epoch: [3]  [  0/195]  eta: 0:01:59  lr: 0.000038  loss: 0.7568 (0.7568)  time: 0.6118  data: 0.2649  max mem: 2139
[15:30:32.274731] Epoch: [3]  [ 20/195]  eta: 0:01:03  lr: 0.000039  loss: 0.7184 (0.7239)  time: 0.3477  data: 0.0002  max mem: 2139
[15:30:39.281359] Epoch: [3]  [ 40/195]  eta: 0:00:55  lr: 0.000040  loss: 0.7138 (0.7214)  time: 0.3503  data: 0.0002  max mem: 2139
[15:30:46.283432] Epoch: [3]  [ 60/195]  eta: 0:00:47  lr: 0.000041  loss: 0.7225 (0.7220)  time: 0.3501  data: 0.0002  max mem: 2139
[15:30:53.225364] Epoch: [3]  [ 80/195]  eta: 0:00:40  lr: 0.000043  loss: 0.7221 (0.7222)  time: 0.3470  data: 0.0003  max mem: 2139
[15:31:00.215972] Epoch: [3]  [100/195]  eta: 0:00:33  lr: 0.000044  loss: 0.7228 (0.7218)  time: 0.3495  data: 0.0003  max mem: 2139
[15:31:07.151950] Epoch: [3]  [120/195]  eta: 0:00:26  lr: 0.000045  loss: 0.7192 (0.7215)  time: 0.3467  data: 0.0002  max mem: 2139
[15:31:14.134321] Epoch: [3]  [140/195]  eta: 0:00:19  lr: 0.000046  loss: 0.7177 (0.7210)  time: 0.3491  data: 0.0003  max mem: 2139
[15:31:21.061843] Epoch: [3]  [160/195]  eta: 0:00:12  lr: 0.000048  loss: 0.7207 (0.7206)  time: 0.3463  data: 0.0002  max mem: 2139
[15:31:28.040849] Epoch: [3]  [180/195]  eta: 0:00:05  lr: 0.000049  loss: 0.7015 (0.7189)  time: 0.3489  data: 0.0002  max mem: 2139
[15:31:32.926699] Epoch: [3]  [194/195]  eta: 0:00:00  lr: 0.000050  loss: 0.6999 (0.7178)  time: 0.3489  data: 0.0001  max mem: 2139
[15:31:32.960309] Epoch: [3] Total time: 0:01:08 (0.3500 s / it)
[15:31:32.960403] Averaged stats: lr: 0.000050  loss: 0.6999 (0.7178)
[15:31:32.963457] log_dir: ./output_dir/bmae
[15:30:29.500884] Epoch: [3]  [  0/195]  eta: 0:01:50  lr: 0.000038  loss: 0.7568 (0.7568)  time: 0.5675  data: 0.2167  max mem: 2139
[15:30:36.505101] Epoch: [3]  [ 20/195]  eta: 0:01:03  lr: 0.000039  loss: 0.7184 (0.7239)  time: 0.3502  data: 0.0002  max mem: 2139
[15:30:43.511536] Epoch: [3]  [ 40/195]  eta: 0:00:55  lr: 0.000040  loss: 0.7138 (0.7214)  time: 0.3503  data: 0.0002  max mem: 2139
[15:30:50.466590] Epoch: [3]  [ 60/195]  eta: 0:00:47  lr: 0.000041  loss: 0.7225 (0.7220)  time: 0.3477  data: 0.0003  max mem: 2139
[15:30:57.462503] Epoch: [3]  [ 80/195]  eta: 0:00:40  lr: 0.000043  loss: 0.7221 (0.7222)  time: 0.3497  data: 0.0002  max mem: 2139
[15:31:04.447657] Epoch: [3]  [100/195]  eta: 0:00:33  lr: 0.000044  loss: 0.7228 (0.7218)  time: 0.3492  data: 0.0002  max mem: 2139
[15:31:11.383207] Epoch: [3]  [120/195]  eta: 0:00:26  lr: 0.000045  loss: 0.7192 (0.7215)  time: 0.3467  data: 0.0003  max mem: 2139
[15:31:18.332067] Epoch: [3]  [140/195]  eta: 0:00:19  lr: 0.000046  loss: 0.7177 (0.7210)  time: 0.3474  data: 0.0002  max mem: 2139
[15:31:25.292819] Epoch: [3]  [160/195]  eta: 0:00:12  lr: 0.000048  loss: 0.7207 (0.7206)  time: 0.3480  data: 0.0002  max mem: 2139
[15:31:32.269246] Epoch: [3]  [180/195]  eta: 0:00:05  lr: 0.000049  loss: 0.7015 (0.7189)  time: 0.3488  data: 0.0003  max mem: 2139
[15:31:37.096063] Epoch: [3]  [194/195]  eta: 0:00:00  lr: 0.000050  loss: 0.6999 (0.7178)  time: 0.3460  data: 0.0001  max mem: 2139
[15:31:37.148928] Epoch: [3] Total time: 0:01:08 (0.3498 s / it)
[15:31:37.149014] Averaged stats: lr: 0.000050  loss: 0.6999 (0.7178)
[15:31:37.151982] log_dir: ./output_dir/bmae
[15:30:16.512199] Epoch: [1]  [  0/195]  eta: 0:02:08  lr: 0.000013  loss: 0.8015 (0.8015)  time: 0.6579  data: 0.2389  max mem: 2159
[15:30:24.922061] Epoch: [1]  [ 20/195]  eta: 0:01:15  lr: 0.000014  loss: 0.7691 (0.7724)  time: 0.4204  data: 0.0002  max mem: 2159
[15:30:33.336440] Epoch: [1]  [ 40/195]  eta: 0:01:06  lr: 0.000015  loss: 0.7630 (0.7685)  time: 0.4207  data: 0.0002  max mem: 2159
[15:30:41.659000] Epoch: [1]  [ 60/195]  eta: 0:00:57  lr: 0.000016  loss: 0.7609 (0.7666)  time: 0.4161  data: 0.0002  max mem: 2159
[15:30:49.902926] Epoch: [1]  [ 80/195]  eta: 0:00:48  lr: 0.000018  loss: 0.7709 (0.7666)  time: 0.4121  data: 0.0002  max mem: 2159
[15:30:58.216468] Epoch: [1]  [100/195]  eta: 0:00:39  lr: 0.000019  loss: 0.7641 (0.7657)  time: 0.4156  data: 0.0002  max mem: 2159
[15:31:06.519665] Epoch: [1]  [120/195]  eta: 0:00:31  lr: 0.000020  loss: 0.7579 (0.7643)  time: 0.4151  data: 0.0002  max mem: 2159
[15:31:14.926949] Epoch: [1]  [140/195]  eta: 0:00:23  lr: 0.000021  loss: 0.7526 (0.7636)  time: 0.4203  data: 0.0002  max mem: 2159
[15:31:23.307890] Epoch: [1]  [160/195]  eta: 0:00:14  lr: 0.000023  loss: 0.7600 (0.7630)  time: 0.4190  data: 0.0002  max mem: 2159
[15:31:31.668975] Epoch: [1]  [180/195]  eta: 0:00:06  lr: 0.000024  loss: 0.7463 (0.7619)  time: 0.4180  data: 0.0003  max mem: 2159
[15:31:37.550557] Epoch: [1]  [194/195]  eta: 0:00:00  lr: 0.000025  loss: 0.7478 (0.7612)  time: 0.4200  data: 0.0001  max mem: 2159
[15:31:37.598260] Epoch: [1] Total time: 0:01:21 (0.4192 s / it)
[15:31:37.598350] Averaged stats: lr: 0.000025  loss: 0.7478 (0.7612)
[15:31:37.601579] log_dir: ./output_dir/bmae
[15:29:41.293699] use bmae loss
[15:29:41.873595] Epoch: [2]  [  0/195]  eta: 0:02:49  lr: 0.000025  loss: 1.6708 (1.6708)  time: 0.8687  data: 0.2300  max mem: 3763
[15:29:41.898388] use bmae loss
[15:29:42.515955] use bmae loss
[15:29:43.136137] use bmae loss
[15:29:43.757528] use bmae loss
[15:29:44.381369] use bmae loss
[15:29:44.985936] use bmae loss
[15:29:45.607763] use bmae loss
[15:29:46.213358] use bmae loss
[15:29:46.833561] use bmae loss
[15:29:47.466887] use bmae loss
[15:29:48.069485] use bmae loss
[15:29:48.692272] use bmae loss
[15:29:49.313010] use bmae loss
[15:29:49.932737] use bmae loss
[15:29:50.551793] use bmae loss
[15:29:51.173520] use bmae loss
[15:29:51.795301] use bmae loss
[15:29:52.420832] use bmae loss
[15:29:53.038451] use bmae loss
[15:29:53.661433] use bmae loss
[15:29:54.260422] Epoch: [2]  [ 20/195]  eta: 0:01:50  lr: 0.000026  loss: 0.8625 (0.9929)  time: 0.6193  data: 0.0002  max mem: 3773
[15:29:54.294560] use bmae loss
[15:29:54.906777] use bmae loss
[15:29:55.527333] use bmae loss
[15:29:56.147272] use bmae loss
[15:29:56.769188] use bmae loss
[15:29:57.343025] use bmae loss
[15:29:57.963432] use bmae loss
[15:29:58.584178] use bmae loss
[15:29:59.203824] use bmae loss
[15:29:59.827268] use bmae loss
[15:30:00.449574] use bmae loss
[15:30:01.071713] use bmae loss
[15:30:01.693523] use bmae loss
[15:30:02.313319] use bmae loss
[15:30:02.937236] use bmae loss
[15:30:03.559595] use bmae loss
[15:30:04.182243] use bmae loss
[15:30:04.819959] use bmae loss
[15:30:05.427657] use bmae loss
[15:30:06.050237] use bmae loss
[15:30:06.650634] Epoch: [2]  [ 40/195]  eta: 0:01:36  lr: 0.000028  loss: 0.4602 (0.7405)  time: 0.6195  data: 0.0002  max mem: 3773
[15:30:06.675583] use bmae loss
[15:30:07.291877] use bmae loss
[15:30:07.916759] use bmae loss
[15:30:08.540636] use bmae loss
[15:30:09.163306] use bmae loss
[15:30:09.784196] use bmae loss
[15:30:10.357835] use bmae loss
[15:30:10.981243] use bmae loss
[15:30:11.601118] use bmae loss
[15:30:12.229172] use bmae loss
[15:30:12.850636] use bmae loss
[15:30:13.486965] use bmae loss
[15:30:14.096705] use bmae loss
[15:30:14.720388] use bmae loss
[15:30:15.345015] use bmae loss
[15:30:15.971809] use bmae loss
[15:30:16.589136] use bmae loss
[15:30:17.228687] use bmae loss
[15:30:17.854412] use bmae loss
[15:30:18.459845] use bmae loss
[15:30:19.055200] Epoch: [2]  [ 60/195]  eta: 0:01:24  lr: 0.000029  loss: 0.3389 (0.6105)  time: 0.6202  data: 0.0002  max mem: 3773
[15:30:19.079450] use bmae loss
[15:30:19.704968] use bmae loss
[15:30:20.323726] use bmae loss
[15:30:20.949466] use bmae loss
[15:30:21.577999] use bmae loss
[15:30:22.181985] use bmae loss
[15:30:22.800889] use bmae loss
[15:30:23.427462] use bmae loss
[15:30:24.052794] use bmae loss
[15:30:24.688792] use bmae loss
[15:30:25.232385] use bmae loss
[15:30:25.858167] use bmae loss
[15:30:26.477969] use bmae loss
[15:30:27.100078] use bmae loss
[15:30:27.711688] use bmae loss
[15:30:28.333053] use bmae loss
[15:30:28.952932] use bmae loss
[15:30:29.525944] use bmae loss
[15:30:30.150240] use bmae loss
[15:30:30.773556] use bmae loss
[15:30:31.373795] Epoch: [2]  [ 80/195]  eta: 0:01:11  lr: 0.000030  loss: 0.2732 (0.5277)  time: 0.6159  data: 0.0003  max mem: 3773
[15:30:31.398221] use bmae loss
[15:30:32.036768] use bmae loss
[15:30:32.642660] use bmae loss
[15:30:33.267416] use bmae loss
[15:30:33.893414] use bmae loss
[15:30:34.513961] use bmae loss
[15:30:35.131477] use bmae loss
[15:30:35.756363] use bmae loss
[15:30:36.375513] use bmae loss
[15:30:37.003260] use bmae loss
[15:30:37.624153] use bmae loss
[15:30:38.233852] use bmae loss
[15:30:38.856788] use bmae loss
[15:30:39.479018] use bmae loss
[15:30:40.104397] use bmae loss
[15:30:40.724411] use bmae loss
[15:30:41.364181] use bmae loss
[15:30:41.968568] use bmae loss
[15:30:42.595277] use bmae loss
[15:30:43.216623] use bmae loss
[15:30:43.812751] Epoch: [2]  [100/195]  eta: 0:00:59  lr: 0.000031  loss: 0.2206 (0.4672)  time: 0.6219  data: 0.0003  max mem: 3773
[15:30:43.845913] use bmae loss
[15:30:44.459263] use bmae loss
[15:30:45.081512] use bmae loss
[15:30:45.709127] use bmae loss
[15:30:46.323225] use bmae loss
[15:30:46.946120] use bmae loss
[15:30:47.564226] use bmae loss
[15:30:48.142419] use bmae loss
[15:30:48.756572] use bmae loss
[15:30:49.364970] use bmae loss
[15:30:49.988917] use bmae loss
[15:30:50.609898] use bmae loss
[15:30:51.233477] use bmae loss
[15:30:51.840921] use bmae loss
[15:30:52.465211] use bmae loss
[15:30:53.082981] use bmae loss
[15:30:53.711697] use bmae loss
[15:30:54.328493] use bmae loss
[15:30:54.937278] use bmae loss
[15:30:55.559655] use bmae loss
[15:30:56.159517] Epoch: [2]  [120/195]  eta: 0:00:46  lr: 0.000033  loss: 0.1836 (0.4204)  time: 0.6173  data: 0.0003  max mem: 3773
[15:30:56.183760] use bmae loss
[15:30:56.802949] use bmae loss
[15:30:57.423646] use bmae loss
[15:30:58.047514] use bmae loss
[15:30:58.664875] use bmae loss
[15:30:59.290950] use bmae loss
[15:30:59.916305] use bmae loss
[15:31:00.528690] use bmae loss
[15:31:01.149679] use bmae loss
[15:31:01.783077] use bmae loss
[15:31:02.410527] use bmae loss
[15:31:03.014348] use bmae loss
[15:31:03.635404] use bmae loss
[15:31:04.253935] use bmae loss
[15:31:04.890805] use bmae loss
[15:31:05.463488] use bmae loss
[15:31:06.052979] use bmae loss
[15:31:06.672202] use bmae loss
[15:31:07.295003] use bmae loss
[15:31:07.914484] use bmae loss
[15:31:08.500999] Epoch: [2]  [140/195]  eta: 0:00:34  lr: 0.000034  loss: 0.1595 (0.3834)  time: 0.6170  data: 0.0003  max mem: 3773
[15:31:08.523865] use bmae loss
[15:31:09.145449] use bmae loss
[15:31:09.766840] use bmae loss
[15:31:10.386340] use bmae loss
[15:31:10.993936] use bmae loss
[15:31:11.612681] use bmae loss
[15:31:12.231878] use bmae loss
[15:31:12.856575] use bmae loss
[15:31:13.475719] use bmae loss
[15:31:14.084471] use bmae loss
[15:31:14.702124] use bmae loss
[15:31:15.326363] use bmae loss
[15:31:15.944999] use bmae loss
[15:31:16.564945] use bmae loss
[15:31:17.173937] use bmae loss
[15:31:17.795401] use bmae loss
[15:31:18.377187] use bmae loss
[15:31:18.982133] use bmae loss
[15:31:19.590613] use bmae loss
[15:31:20.224440] use bmae loss
[15:31:20.811545] Epoch: [2]  [160/195]  eta: 0:00:21  lr: 0.000035  loss: 0.1430 (0.3537)  time: 0.6155  data: 0.0002  max mem: 3773
[15:31:20.841879] use bmae loss
[15:31:21.450396] use bmae loss
[15:31:22.058043] use bmae loss
[15:31:22.678371] use bmae loss
[15:31:23.301835] use bmae loss
[15:31:23.917816] use bmae loss
[15:31:24.524955] use bmae loss
[15:31:25.151898] use bmae loss
[15:31:25.771501] use bmae loss
[15:31:26.374569] use bmae loss
[15:31:26.987494] use bmae loss
[15:31:27.589386] use bmae loss
[15:31:28.206301] use bmae loss
[15:31:28.828168] use bmae loss
[15:31:29.435622] use bmae loss
[15:31:30.043021] use bmae loss
[15:31:30.659334] use bmae loss
[15:31:31.283541] use bmae loss
[15:31:31.891051] use bmae loss
[15:31:32.512335] use bmae loss
[15:31:33.055485] Epoch: [2]  [180/195]  eta: 0:00:09  lr: 0.000037  loss: 0.1342 (0.3293)  time: 0.6122  data: 0.0002  max mem: 3773
[15:31:33.080752] use bmae loss
[15:31:33.674534] use bmae loss
[15:31:34.289952] use bmae loss
[15:31:34.898057] use bmae loss
[15:31:35.517229] use bmae loss
[15:31:36.140142] use bmae loss
[15:31:36.758151] use bmae loss
[15:31:37.321385] use bmae loss
[15:31:37.936727] use bmae loss
[15:31:38.534833] use bmae loss
[15:31:39.151829] use bmae loss
[15:31:39.755861] use bmae loss
[15:31:40.363740] use bmae loss
[15:31:40.981990] use bmae loss
[15:31:41.580499] Epoch: [2]  [194/195]  eta: 0:00:00  lr: 0.000037  loss: 0.1272 (0.3147)  time: 0.6083  data: 0.0001  max mem: 3773
[15:31:41.627319] Epoch: [2] Total time: 0:02:00 (0.6186 s / it)
[15:31:41.627380] Averaged stats: lr: 0.000037  loss: 0.1272 (0.3147)
[15:31:41.630105] log_dir: ./output_dir/bmae
Killed
Killed
Killed
Killed
Killed
Killed
[15:30:34.286141] Epoch: [1]  [  0/195]  eta: 0:02:08  lr: 0.000013  loss: 0.8015 (0.8015)  time: 0.6601  data: 0.2367  max mem: 2159
[15:30:42.651063] Epoch: [1]  [ 20/195]  eta: 0:01:15  lr: 0.000014  loss: 0.7691 (0.7724)  time: 0.4182  data: 0.0002  max mem: 2159
[15:30:50.868315] Epoch: [1]  [ 40/195]  eta: 0:01:05  lr: 0.000015  loss: 0.7630 (0.7685)  time: 0.4108  data: 0.0007  max mem: 2159
[15:30:59.177035] Epoch: [1]  [ 60/195]  eta: 0:00:56  lr: 0.000016  loss: 0.7609 (0.7666)  time: 0.4154  data: 0.0003  max mem: 2159
[15:31:07.464405] Epoch: [1]  [ 80/195]  eta: 0:00:48  lr: 0.000018  loss: 0.7709 (0.7666)  time: 0.4143  data: 0.0003  max mem: 2159
[15:31:15.877240] Epoch: [1]  [100/195]  eta: 0:00:39  lr: 0.000019  loss: 0.7641 (0.7657)  time: 0.4206  data: 0.0003  max mem: 2159
[15:31:24.230721] Epoch: [1]  [120/195]  eta: 0:00:31  lr: 0.000020  loss: 0.7579 (0.7643)  time: 0.4176  data: 0.0003  max mem: 2159
[15:31:32.629045] Epoch: [1]  [140/195]  eta: 0:00:23  lr: 0.000021  loss: 0.7526 (0.7636)  time: 0.4199  data: 0.0002  max mem: 2159
[15:31:40.966717] Epoch: [1]  [160/195]  eta: 0:00:14  lr: 0.000023  loss: 0.7600 (0.7630)  time: 0.4168  data: 0.0002  max mem: 2159
[15:31:49.374187] Epoch: [1]  [180/195]  eta: 0:00:06  lr: 0.000024  loss: 0.7463 (0.7619)  time: 0.4203  data: 0.0002  max mem: 2159
[15:31:55.265540] Epoch: [1]  [194/195]  eta: 0:00:00  lr: 0.000025  loss: 0.7478 (0.7612)  time: 0.4205  data: 0.0001  max mem: 2159
[15:31:55.317633] Epoch: [1] Total time: 0:01:21 (0.4189 s / it)
[15:31:55.317719] Averaged stats: lr: 0.000025  loss: 0.7478 (0.7612)
[15:31:55.320788] log_dir: ./output_dir/bmae
[15:30:21.988433] Epoch: [5]  [  0/195]  eta: 0:02:49  lr: 0.050000  loss: 1.9046 (1.9046)  time: 0.8688  data: 0.3874  max mem: 127
[15:30:31.584974] Epoch: [5]  [ 20/195]  eta: 0:01:27  lr: 0.051026  loss: 1.9411 (1.9443)  time: 0.4798  data: 0.0002  max mem: 127
[15:30:41.396021] Epoch: [5]  [ 40/195]  eta: 0:01:16  lr: 0.052051  loss: 1.9160 (1.9342)  time: 0.4905  data: 0.0001  max mem: 127
[15:30:51.091689] Epoch: [5]  [ 60/195]  eta: 0:01:06  lr: 0.053077  loss: 1.9263 (1.9301)  time: 0.4847  data: 0.0001  max mem: 127
[15:31:00.779622] Epoch: [5]  [ 80/195]  eta: 0:00:56  lr: 0.054103  loss: 1.8974 (1.9244)  time: 0.4844  data: 0.0001  max mem: 127
[15:31:10.568294] Epoch: [5]  [100/195]  eta: 0:00:46  lr: 0.055128  loss: 1.9191 (1.9239)  time: 0.4894  data: 0.0001  max mem: 127
[15:31:20.307727] Epoch: [5]  [120/195]  eta: 0:00:36  lr: 0.056154  loss: 1.9000 (1.9217)  time: 0.4869  data: 0.0001  max mem: 127
[15:31:29.958299] Epoch: [5]  [140/195]  eta: 0:00:26  lr: 0.057179  loss: 1.8960 (1.9180)  time: 0.4825  data: 0.0001  max mem: 127
[15:31:39.705532] Epoch: [5]  [160/195]  eta: 0:00:17  lr: 0.058205  loss: 1.8896 (1.9147)  time: 0.4873  data: 0.0001  max mem: 127
[15:31:49.439444] Epoch: [5]  [180/195]  eta: 0:00:07  lr: 0.059231  loss: 1.8977 (1.9122)  time: 0.4867  data: 0.0001  max mem: 127
[15:31:56.262035] Epoch: [5]  [194/195]  eta: 0:00:00  lr: 0.059949  loss: 1.8883 (1.9111)  time: 0.4864  data: 0.0001  max mem: 127
[15:31:56.327618] Epoch: [5] Total time: 0:01:35 (0.4882 s / it)
[15:31:56.327675] Averaged stats: lr: 0.059949  loss: 1.8883 (1.9111)
Killed
[15:30:44.376966] Epoch: [1]  [  0/195]  eta: 0:02:16  lr: 0.000013  loss: 0.8015 (0.8015)  time: 0.7018  data: 0.2811  max mem: 2159
[15:30:52.678667] Epoch: [1]  [ 20/195]  eta: 0:01:15  lr: 0.000014  loss: 0.7691 (0.7724)  time: 0.4150  data: 0.0002  max mem: 2159
[15:31:01.235280] Epoch: [1]  [ 40/195]  eta: 0:01:06  lr: 0.000015  loss: 0.7630 (0.7685)  time: 0.4278  data: 0.0002  max mem: 2159
[15:31:09.678669] Epoch: [1]  [ 60/195]  eta: 0:00:57  lr: 0.000016  loss: 0.7609 (0.7666)  time: 0.4221  data: 0.0002  max mem: 2159
[15:31:18.145999] Epoch: [1]  [ 80/195]  eta: 0:00:48  lr: 0.000018  loss: 0.7709 (0.7666)  time: 0.4233  data: 0.0003  max mem: 2159
[15:31:26.617643] Epoch: [1]  [100/195]  eta: 0:00:40  lr: 0.000019  loss: 0.7641 (0.7657)  time: 0.4235  data: 0.0002  max mem: 2159
[15:31:35.215724] Epoch: [1]  [120/195]  eta: 0:00:31  lr: 0.000020  loss: 0.7579 (0.7643)  time: 0.4299  data: 0.0003  max mem: 2159
[15:31:43.658388] Epoch: [1]  [140/195]  eta: 0:00:23  lr: 0.000021  loss: 0.7526 (0.7636)  time: 0.4221  data: 0.0002  max mem: 2159
[15:31:52.172095] Epoch: [1]  [160/195]  eta: 0:00:14  lr: 0.000023  loss: 0.7600 (0.7630)  time: 0.4256  data: 0.0002  max mem: 2159
[15:32:00.644839] Epoch: [1]  [180/195]  eta: 0:00:06  lr: 0.000024  loss: 0.7463 (0.7619)  time: 0.4236  data: 0.0002  max mem: 2159
[15:32:05.961072] Epoch: [1]  [194/195]  eta: 0:00:00  lr: 0.000025  loss: 0.7478 (0.7612)  time: 0.3938  data: 0.0001  max mem: 2159
[15:32:06.011862] Epoch: [1] Total time: 0:01:22 (0.4222 s / it)
[15:32:06.011947] Averaged stats: lr: 0.000025  loss: 0.7478 (0.7612)
[15:32:06.014959] log_dir: ./output_dir/bmae
[15:30:50.419929] Epoch: [1]  [  0/195]  eta: 0:02:23  lr: 0.000013  loss: 0.8015 (0.8015)  time: 0.7376  data: 0.3167  max mem: 2159
[15:30:58.909937] Epoch: [1]  [ 20/195]  eta: 0:01:16  lr: 0.000014  loss: 0.7691 (0.7724)  time: 0.4245  data: 0.0002  max mem: 2159
[15:31:07.407959] Epoch: [1]  [ 40/195]  eta: 0:01:07  lr: 0.000015  loss: 0.7630 (0.7685)  time: 0.4248  data: 0.0002  max mem: 2159
[15:31:15.955941] Epoch: [1]  [ 60/195]  eta: 0:00:58  lr: 0.000016  loss: 0.7609 (0.7666)  time: 0.4274  data: 0.0002  max mem: 2159
[15:31:24.505568] Epoch: [1]  [ 80/195]  eta: 0:00:49  lr: 0.000018  loss: 0.7709 (0.7666)  time: 0.4274  data: 0.0002  max mem: 2159
[15:31:33.110012] Epoch: [1]  [100/195]  eta: 0:00:40  lr: 0.000019  loss: 0.7641 (0.7657)  time: 0.4302  data: 0.0002  max mem: 2159
[15:31:41.685528] Epoch: [1]  [120/195]  eta: 0:00:32  lr: 0.000020  loss: 0.7579 (0.7643)  time: 0.4287  data: 0.0002  max mem: 2159
[15:31:50.308301] Epoch: [1]  [140/195]  eta: 0:00:23  lr: 0.000021  loss: 0.7526 (0.7636)  time: 0.4311  data: 0.0002  max mem: 2159
[15:31:58.839920] Epoch: [1]  [160/195]  eta: 0:00:15  lr: 0.000023  loss: 0.7600 (0.7630)  time: 0.4265  data: 0.0002  max mem: 2159
[15:32:06.610391] Epoch: [1]  [180/195]  eta: 0:00:06  lr: 0.000024  loss: 0.7463 (0.7619)  time: 0.3885  data: 0.0002  max mem: 2159
[15:32:11.791243] Epoch: [1]  [194/195]  eta: 0:00:00  lr: 0.000025  loss: 0.7478 (0.7612)  time: 0.3661  data: 0.0001  max mem: 2159
[15:32:11.843613] Epoch: [1] Total time: 0:01:22 (0.4213 s / it)
[15:32:11.843692] Averaged stats: lr: 0.000025  loss: 0.7478 (0.7612)
[15:32:11.845975] log_dir: ./output_dir/bmae
[15:30:51.980395] Epoch: [1]  [  0/195]  eta: 0:02:10  lr: 0.000013  loss: 0.8015 (0.8015)  time: 0.6709  data: 0.2448  max mem: 2159
[15:31:00.460668] Epoch: [1]  [ 20/195]  eta: 0:01:16  lr: 0.000014  loss: 0.7691 (0.7724)  time: 0.4240  data: 0.0002  max mem: 2159
[15:31:08.914893] Epoch: [1]  [ 40/195]  eta: 0:01:06  lr: 0.000015  loss: 0.7630 (0.7685)  time: 0.4227  data: 0.0002  max mem: 2159
[15:31:17.458470] Epoch: [1]  [ 60/195]  eta: 0:00:57  lr: 0.000016  loss: 0.7609 (0.7666)  time: 0.4271  data: 0.0002  max mem: 2159
[15:31:25.930456] Epoch: [1]  [ 80/195]  eta: 0:00:49  lr: 0.000018  loss: 0.7709 (0.7666)  time: 0.4236  data: 0.0002  max mem: 2159
[15:31:34.452198] Epoch: [1]  [100/195]  eta: 0:00:40  lr: 0.000019  loss: 0.7641 (0.7657)  time: 0.4260  data: 0.0002  max mem: 2159
[15:31:42.954070] Epoch: [1]  [120/195]  eta: 0:00:32  lr: 0.000020  loss: 0.7579 (0.7643)  time: 0.4251  data: 0.0002  max mem: 2159
[15:31:51.485885] Epoch: [1]  [140/195]  eta: 0:00:23  lr: 0.000021  loss: 0.7526 (0.7636)  time: 0.4266  data: 0.0002  max mem: 2159
[15:31:59.929605] Epoch: [1]  [160/195]  eta: 0:00:14  lr: 0.000023  loss: 0.7600 (0.7630)  time: 0.4221  data: 0.0002  max mem: 2159
[15:32:07.450620] Epoch: [1]  [180/195]  eta: 0:00:06  lr: 0.000024  loss: 0.7463 (0.7619)  time: 0.3760  data: 0.0002  max mem: 2159
[15:32:12.474711] Epoch: [1]  [194/195]  eta: 0:00:00  lr: 0.000025  loss: 0.7478 (0.7612)  time: 0.3575  data: 0.0001  max mem: 2159
[15:32:12.519399] Epoch: [1] Total time: 0:01:21 (0.4165 s / it)
[15:32:12.519460] Averaged stats: lr: 0.000025  loss: 0.7478 (0.7612)
[15:32:12.521720] log_dir: ./output_dir/bmae
[15:31:57.071287] Test:  [ 0/40]  eta: 0:00:28  loss: 1.8305 (1.8305)  acc1: 35.1562 (35.1562)  acc5: 89.8438 (89.8438)  time: 0.7062  data: 0.2376  max mem: 127
[15:32:01.742990] Test:  [10/40]  eta: 0:00:14  loss: 1.8305 (1.8210)  acc1: 38.2812 (38.0327)  acc5: 87.8906 (87.7486)  time: 0.4888  data: 0.0218  max mem: 127
[15:32:06.420452] Test:  [20/40]  eta: 0:00:09  loss: 1.8219 (1.8087)  acc1: 38.6719 (38.6533)  acc5: 87.8906 (88.1510)  time: 0.4674  data: 0.0002  max mem: 127
[15:32:11.105426] Test:  [30/40]  eta: 0:00:04  loss: 1.8100 (1.8103)  acc1: 38.2812 (38.5207)  acc5: 88.6719 (88.0292)  time: 0.4681  data: 0.0001  max mem: 127
[15:32:14.924731] Test:  [39/40]  eta: 0:00:00  loss: 1.8062 (1.8128)  acc1: 38.2812 (38.2900)  acc5: 88.2812 (87.9700)  time: 0.4485  data: 0.0001  max mem: 127
[15:32:14.976124] Test: Total time: 0:00:18 (0.4653 s / it)
[15:32:14.976194] * Acc@1 38.290 Acc@5 87.970 loss 1.813
[15:32:14.976326] Accuracy of the network on the 10000 test images: 38.3%
[15:32:14.976335] Max accuracy: 38.29%
[15:32:14.978215] log_dir: ./output_dir/mae_mk0.3/linear
[15:31:05.328565] Epoch: [1]  [  0/195]  eta: 0:02:23  lr: 0.000013  loss: 0.8015 (0.8015)  time: 0.7355  data: 0.3083  max mem: 2159
[15:31:13.838417] Epoch: [1]  [ 20/195]  eta: 0:01:17  lr: 0.000014  loss: 0.7691 (0.7724)  time: 0.4254  data: 0.0002  max mem: 2159
[15:31:22.339814] Epoch: [1]  [ 40/195]  eta: 0:01:07  lr: 0.000015  loss: 0.7630 (0.7685)  time: 0.4250  data: 0.0002  max mem: 2159
[15:31:30.819934] Epoch: [1]  [ 60/195]  eta: 0:00:58  lr: 0.000016  loss: 0.7609 (0.7666)  time: 0.4240  data: 0.0003  max mem: 2159
[15:31:39.284781] Epoch: [1]  [ 80/195]  eta: 0:00:49  lr: 0.000018  loss: 0.7709 (0.7666)  time: 0.4232  data: 0.0002  max mem: 2159
[15:31:47.778994] Epoch: [1]  [100/195]  eta: 0:00:40  lr: 0.000019  loss: 0.7641 (0.7657)  time: 0.4247  data: 0.0002  max mem: 2159
[15:31:56.215173] Epoch: [1]  [120/195]  eta: 0:00:31  lr: 0.000020  loss: 0.7579 (0.7643)  time: 0.4218  data: 0.0002  max mem: 2159
[15:32:04.333771] Epoch: [1]  [140/195]  eta: 0:00:23  lr: 0.000021  loss: 0.7526 (0.7636)  time: 0.4059  data: 0.0002  max mem: 2159
[15:32:11.601825] Epoch: [1]  [160/195]  eta: 0:00:14  lr: 0.000023  loss: 0.7600 (0.7630)  time: 0.3634  data: 0.0002  max mem: 2159
[15:32:18.775275] Epoch: [1]  [180/195]  eta: 0:00:06  lr: 0.000024  loss: 0.7463 (0.7619)  time: 0.3586  data: 0.0002  max mem: 2159
[15:32:23.864762] Epoch: [1]  [194/195]  eta: 0:00:00  lr: 0.000025  loss: 0.7478 (0.7612)  time: 0.3643  data: 0.0001  max mem: 2159
[15:32:23.916954] Epoch: [1] Total time: 0:01:19 (0.4068 s / it)
[15:32:23.917042] Averaged stats: lr: 0.000025  loss: 0.7478 (0.7612)
[15:32:23.920049] log_dir: ./output_dir/bmae
[15:30:50.248793] Epoch: [5]  [  0/195]  eta: 0:02:26  lr: 0.050000  loss: 1.8638 (1.8638)  time: 0.7492  data: 0.2648  max mem: 127
[15:30:59.934568] Epoch: [5]  [ 20/195]  eta: 0:01:26  lr: 0.051026  loss: 1.8719 (1.8798)  time: 0.4842  data: 0.0001  max mem: 127
[15:31:09.707303] Epoch: [5]  [ 40/195]  eta: 0:01:16  lr: 0.052051  loss: 1.8460 (1.8688)  time: 0.4886  data: 0.0001  max mem: 127
[15:31:19.434116] Epoch: [5]  [ 60/195]  eta: 0:01:06  lr: 0.053077  loss: 1.8559 (1.8632)  time: 0.4863  data: 0.0001  max mem: 127
[15:31:29.076799] Epoch: [5]  [ 80/195]  eta: 0:00:56  lr: 0.054103  loss: 1.8391 (1.8570)  time: 0.4821  data: 0.0001  max mem: 127
[15:31:38.813198] Epoch: [5]  [100/195]  eta: 0:00:46  lr: 0.055128  loss: 1.8315 (1.8543)  time: 0.4868  data: 0.0001  max mem: 127
[15:31:48.562391] Epoch: [5]  [120/195]  eta: 0:00:36  lr: 0.056154  loss: 1.8178 (1.8502)  time: 0.4874  data: 0.0001  max mem: 127
[15:31:58.183936] Epoch: [5]  [140/195]  eta: 0:00:26  lr: 0.057179  loss: 1.8059 (1.8448)  time: 0.4810  data: 0.0001  max mem: 127
[15:32:07.991968] Epoch: [5]  [160/195]  eta: 0:00:17  lr: 0.058205  loss: 1.8046 (1.8405)  time: 0.4904  data: 0.0001  max mem: 127
[15:32:17.672513] Epoch: [5]  [180/195]  eta: 0:00:07  lr: 0.059231  loss: 1.8132 (1.8372)  time: 0.4840  data: 0.0001  max mem: 127
[15:32:24.482931] Epoch: [5]  [194/195]  eta: 0:00:00  lr: 0.059949  loss: 1.8004 (1.8351)  time: 0.4812  data: 0.0001  max mem: 127
[15:32:24.554594] Epoch: [5] Total time: 0:01:35 (0.4875 s / it)
[15:32:24.554652] Averaged stats: lr: 0.059949  loss: 1.8004 (1.8351)
Killed
Killed
Killed
Killed
Killed
Killed
[15:32:25.263465] Test:  [ 0/40]  eta: 0:00:26  loss: 1.7217 (1.7217)  acc1: 40.2344 (40.2344)  acc5: 91.7969 (91.7969)  time: 0.6709  data: 0.2073  max mem: 127
[15:32:29.945825] Test:  [10/40]  eta: 0:00:14  loss: 1.7267 (1.7243)  acc1: 40.2344 (40.9446)  acc5: 91.0156 (89.9858)  time: 0.4866  data: 0.0190  max mem: 127
[15:32:34.618272] Test:  [20/40]  eta: 0:00:09  loss: 1.7143 (1.7117)  acc1: 41.7969 (41.7597)  acc5: 90.2344 (90.1228)  time: 0.4677  data: 0.0002  max mem: 127
[15:32:39.285079] Test:  [30/40]  eta: 0:00:04  loss: 1.7129 (1.7146)  acc1: 42.1875 (41.7087)  acc5: 89.8438 (90.0328)  time: 0.4669  data: 0.0001  max mem: 127
[15:32:43.112118] Test:  [39/40]  eta: 0:00:00  loss: 1.7205 (1.7180)  acc1: 39.8438 (41.1400)  acc5: 89.8438 (90.0100)  time: 0.4479  data: 0.0001  max mem: 127
[15:32:43.175301] Test: Total time: 0:00:18 (0.4646 s / it)
[15:32:43.175349] * Acc@1 41.140 Acc@5 90.010 loss 1.718
[15:32:43.175458] Accuracy of the network on the 10000 test images: 41.1%
[15:32:43.175464] Max accuracy: 41.14%
[15:32:43.177306] log_dir: ./output_dir/mae_mk0.5/linear
[15:31:13.834223] Epoch: [5]  [  0/195]  eta: 0:02:24  lr: 0.050000  loss: 1.7854 (1.7854)  time: 0.7410  data: 0.2499  max mem: 127
[15:31:23.522697] Epoch: [5]  [ 20/195]  eta: 0:01:26  lr: 0.051026  loss: 1.8012 (1.8090)  time: 0.4844  data: 0.0001  max mem: 127
[15:31:33.228736] Epoch: [5]  [ 40/195]  eta: 0:01:16  lr: 0.052051  loss: 1.7787 (1.7980)  time: 0.4853  data: 0.0001  max mem: 127
[15:31:42.968108] Epoch: [5]  [ 60/195]  eta: 0:01:06  lr: 0.053077  loss: 1.7930 (1.7951)  time: 0.4869  data: 0.0001  max mem: 127
[15:31:52.709247] Epoch: [5]  [ 80/195]  eta: 0:00:56  lr: 0.054103  loss: 1.7567 (1.7883)  time: 0.4870  data: 0.0001  max mem: 127
[15:32:02.361488] Epoch: [5]  [100/195]  eta: 0:00:46  lr: 0.055128  loss: 1.7608 (1.7857)  time: 0.4826  data: 0.0001  max mem: 127
[15:32:12.171493] Epoch: [5]  [120/195]  eta: 0:00:36  lr: 0.056154  loss: 1.7608 (1.7818)  time: 0.4905  data: 0.0001  max mem: 127
[15:32:21.814921] Epoch: [5]  [140/195]  eta: 0:00:26  lr: 0.057179  loss: 1.7329 (1.7758)  time: 0.4821  data: 0.0001  max mem: 127
[15:32:31.466549] Epoch: [5]  [160/195]  eta: 0:00:17  lr: 0.058205  loss: 1.7350 (1.7710)  time: 0.4825  data: 0.0001  max mem: 127
[15:32:41.247598] Epoch: [5]  [180/195]  eta: 0:00:07  lr: 0.059231  loss: 1.7365 (1.7671)  time: 0.4890  data: 0.0001  max mem: 127
[15:32:47.956646] Epoch: [5]  [194/195]  eta: 0:00:00  lr: 0.059949  loss: 1.7159 (1.7640)  time: 0.4823  data: 0.0001  max mem: 127
[15:32:48.005323] Epoch: [5] Total time: 0:01:34 (0.4867 s / it)
[15:32:48.005389] Averaged stats: lr: 0.059949  loss: 1.7159 (1.7640)
[15:31:21.129930] Epoch: [5]  [  0/195]  eta: 0:02:24  lr: 0.050000  loss: 1.3800 (1.3800)  time: 0.7411  data: 0.2530  max mem: 127
[15:31:30.851870] Epoch: [5]  [ 20/195]  eta: 0:01:27  lr: 0.051026  loss: 1.4091 (1.4067)  time: 0.4861  data: 0.0001  max mem: 127
[15:31:40.572124] Epoch: [5]  [ 40/195]  eta: 0:01:16  lr: 0.052051  loss: 1.3790 (1.3916)  time: 0.4860  data: 0.0001  max mem: 127
[15:31:50.286027] Epoch: [5]  [ 60/195]  eta: 0:01:06  lr: 0.053077  loss: 1.3817 (1.3916)  time: 0.4857  data: 0.0001  max mem: 127
[15:31:59.914400] Epoch: [5]  [ 80/195]  eta: 0:00:56  lr: 0.054103  loss: 1.3736 (1.3878)  time: 0.4814  data: 0.0001  max mem: 127
[15:32:09.726811] Epoch: [5]  [100/195]  eta: 0:00:46  lr: 0.055128  loss: 1.3489 (1.3838)  time: 0.4906  data: 0.0001  max mem: 127
[15:32:19.389467] Epoch: [5]  [120/195]  eta: 0:00:36  lr: 0.056154  loss: 1.3644 (1.3801)  time: 0.4831  data: 0.0001  max mem: 127
[15:32:29.030769] Epoch: [5]  [140/195]  eta: 0:00:26  lr: 0.057179  loss: 1.3188 (1.3726)  time: 0.4820  data: 0.0001  max mem: 127
[15:32:38.819514] Epoch: [5]  [160/195]  eta: 0:00:17  lr: 0.058205  loss: 1.3189 (1.3678)  time: 0.4894  data: 0.0001  max mem: 127
[15:32:48.399075] Epoch: [5]  [180/195]  eta: 0:00:07  lr: 0.059231  loss: 1.3416 (1.3659)  time: 0.4789  data: 0.0001  max mem: 127
[15:32:55.265789] Epoch: [5]  [194/195]  eta: 0:00:00  lr: 0.059949  loss: 1.3230 (1.3633)  time: 0.4851  data: 0.0001  max mem: 127
[15:32:55.317782] Epoch: [5] Total time: 0:01:34 (0.4868 s / it)
[15:32:55.317860] Averaged stats: lr: 0.059949  loss: 1.3230 (1.3633)
[15:32:48.697270] Test:  [ 0/40]  eta: 0:00:26  loss: 1.6075 (1.6075)  acc1: 45.7031 (45.7031)  acc5: 92.1875 (92.1875)  time: 0.6526  data: 0.1873  max mem: 127
[15:32:53.368880] Test:  [10/40]  eta: 0:00:14  loss: 1.6312 (1.6202)  acc1: 45.7031 (44.8864)  acc5: 91.4062 (91.1932)  time: 0.4840  data: 0.0171  max mem: 127
[15:32:57.967574] Test:  [20/40]  eta: 0:00:09  loss: 1.6132 (1.6139)  acc1: 46.0938 (45.8147)  acc5: 91.4062 (91.5365)  time: 0.4635  data: 0.0001  max mem: 127
[15:33:02.694649] Test:  [30/40]  eta: 0:00:04  loss: 1.6132 (1.6172)  acc1: 45.3125 (45.3503)  acc5: 91.4062 (91.3054)  time: 0.4662  data: 0.0001  max mem: 127
[15:33:06.535097] Test:  [39/40]  eta: 0:00:00  loss: 1.6234 (1.6196)  acc1: 43.7500 (44.7800)  acc5: 91.0156 (91.3300)  time: 0.4519  data: 0.0001  max mem: 127
[15:33:06.587313] Test: Total time: 0:00:18 (0.4636 s / it)
[15:33:06.587360] * Acc@1 44.780 Acc@5 91.330 loss 1.620
[15:33:06.587467] Accuracy of the network on the 10000 test images: 44.8%
[15:33:06.587473] Max accuracy: 44.78%
[15:33:06.589240] log_dir: ./output_dir/mae_mk0.7/linear
[15:32:56.016033] Test:  [ 0/40]  eta: 0:00:26  loss: 1.1651 (1.1651)  acc1: 53.5156 (53.5156)  acc5: 97.2656 (97.2656)  time: 0.6574  data: 0.1933  max mem: 127
[15:33:00.740529] Test:  [10/40]  eta: 0:00:14  loss: 1.1967 (1.1935)  acc1: 57.8125 (57.7770)  acc5: 95.7031 (95.6321)  time: 0.4892  data: 0.0177  max mem: 127
[15:33:05.452351] Test:  [20/40]  eta: 0:00:09  loss: 1.1735 (1.1870)  acc1: 58.5938 (58.1845)  acc5: 95.7031 (95.7775)  time: 0.4718  data: 0.0001  max mem: 127
[15:33:10.048140] Test:  [30/40]  eta: 0:00:04  loss: 1.1731 (1.1886)  acc1: 58.9844 (58.5181)  acc5: 96.0938 (95.9173)  time: 0.4653  data: 0.0001  max mem: 127
[15:33:13.858147] Test:  [39/40]  eta: 0:00:00  loss: 1.2042 (1.1990)  acc1: 57.4219 (58.0300)  acc5: 95.7031 (95.8900)  time: 0.4439  data: 0.0001  max mem: 127
[15:33:13.919146] Test: Total time: 0:00:18 (0.4640 s / it)
[15:33:13.919205] * Acc@1 58.030 Acc@5 95.890 loss 1.199
[15:33:13.920563] Accuracy of the network on the 10000 test images: 58.0%
[15:33:13.920579] Max accuracy: 58.03%
[15:33:13.923074] log_dir: ./output_dir/mae_mk0.9/linear
Not using distributed mode
[15:33:16.195298] job dir: /home/wsj/mae
[15:33:16.195363] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=2,
ema_alpha=0.99,
enable_ema=True,
enable_bootstrap=True,
ema_warmup_epochs=2,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae_warmup2',
log_dir='./output_dir/bmae_warmup2',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:33:16.852504] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:33:16.852679] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x77c4ea248250>
[15:33:17.371039] use bootstrapped MAE!
[15:33:17.371081] ema enabled!
[15:33:17.396853] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:33:17.396897] base lr: 5.00e-04
[15:33:17.396908] actual lr: 5.00e-04
[15:33:17.396916] accumulate grad iterations: 1
[15:33:17.396924] effective batch size: 256
[15:33:17.399064] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:33:17.399205] Start training for 200 epochs
[15:33:17.401381] log_dir: ./output_dir/bmae_warmup2
Not using distributed mode
[15:33:19.514819] job dir: /home/wsj/mae
[15:33:19.514866] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=2,
ema_alpha=0.99,
enable_ema=True,
enable_bootstrap=True,
ema_warmup_epochs=4,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae_warmup4',
log_dir='./output_dir/bmae_warmup4',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:33:20.018080] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:33:20.018210] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7043e5304790>
[15:33:20.479166] use bootstrapped MAE!
[15:33:20.479193] ema enabled!
[15:33:20.621863] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:33:20.621905] base lr: 5.00e-04
[15:33:20.621913] actual lr: 5.00e-04
[15:33:20.621919] accumulate grad iterations: 1
[15:33:20.621925] effective batch size: 256
[15:33:20.623380] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:33:20.623466] Start training for 200 epochs
[15:33:20.624789] log_dir: ./output_dir/bmae_warmup4
Not using distributed mode
[15:33:24.710356] job dir: /home/wsj/mae
[15:33:24.710410] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=2,
ema_alpha=0.99,
enable_ema=True,
enable_bootstrap=True,
ema_warmup_epochs=8,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae_warmup8',
log_dir='./output_dir/bmae_warmup8',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:33:25.330266] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:33:25.330442] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7b36ae240910>
[15:33:26.192607] use bootstrapped MAE!
[15:33:26.192638] ema enabled!
[15:33:26.699020] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:33:26.699055] base lr: 5.00e-04
[15:33:26.699061] actual lr: 5.00e-04
[15:33:26.699065] accumulate grad iterations: 1
[15:33:26.699069] effective batch size: 256
[15:33:26.700179] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:33:26.700261] Start training for 200 epochs
[15:33:26.701263] log_dir: ./output_dir/bmae_warmup8
Not using distributed mode
[15:33:27.404035] job dir: /home/wsj/mae
[15:33:27.404095] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=2,
ema_alpha=0.99,
enable_ema=True,
enable_bootstrap=True,
ema_warmup_epochs=16,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae_warmup16',
log_dir='./output_dir/bmae_warmup16',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:33:28.146254] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:33:28.146444] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x741e21f41090>
[15:33:29.466890] use bootstrapped MAE!
[15:33:29.466937] ema enabled!
[15:33:30.570233] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:33:30.570272] base lr: 5.00e-04
[15:33:30.570278] actual lr: 5.00e-04
[15:33:30.570282] accumulate grad iterations: 1
[15:33:30.570286] effective batch size: 256
[15:33:30.571564] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:33:30.571670] Start training for 200 epochs
[15:33:30.572876] log_dir: ./output_dir/bmae_warmup16
Not using distributed mode
[15:33:30.930551] job dir: /home/wsj/mae
[15:33:30.930632] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=2,
ema_alpha=0.99,
enable_ema=True,
enable_bootstrap=True,
ema_warmup_epochs=32,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae_warmup32',
log_dir='./output_dir/bmae_warmup32',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:33:31.572261] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:33:31.572440] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x76144c1b9f50>
[15:33:32.979546] use bootstrapped MAE!
[15:33:32.979578] ema enabled!
[15:33:34.472344] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:33:34.472396] base lr: 5.00e-04
[15:33:34.472402] actual lr: 5.00e-04
[15:33:34.472407] accumulate grad iterations: 1
[15:33:34.472411] effective batch size: 256
[15:33:34.473855] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:33:34.473987] Start training for 200 epochs
[15:33:34.475314] log_dir: ./output_dir/bmae_warmup32
Not using distributed mode
[15:33:33.769591] job dir: /home/wsj/mae
[15:33:33.769651] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=2,
ema_alpha=0.99,
enable_ema=True,
enable_bootstrap=True,
ema_warmup_epochs=64,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae_warmup64',
log_dir='./output_dir/bmae_warmup64',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:33:34.390802] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:33:34.391012] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x70ded51d56d0>
[15:33:36.133923] use bootstrapped MAE!
[15:33:36.133967] ema enabled!
[15:33:37.940150] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:33:37.940196] base lr: 5.00e-04
[15:33:37.940205] actual lr: 5.00e-04
[15:33:37.940209] accumulate grad iterations: 1
[15:33:37.940216] effective batch size: 256
[15:33:37.941730] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:33:37.941864] Start training for 200 epochs
[15:33:37.943135] log_dir: ./output_dir/bmae_warmup64
[15:32:15.758762] Epoch: [6]  [  0/195]  eta: 0:02:32  lr: 0.060000  loss: 1.8351 (1.8351)  time: 0.7798  data: 0.2985  max mem: 127
[15:32:25.399477] Epoch: [6]  [ 20/195]  eta: 0:01:26  lr: 0.061026  loss: 1.8720 (1.8764)  time: 0.4820  data: 0.0001  max mem: 127
[15:32:35.221960] Epoch: [6]  [ 40/195]  eta: 0:01:16  lr: 0.062051  loss: 1.8629 (1.8712)  time: 0.4911  data: 0.0001  max mem: 127
[15:32:44.889065] Epoch: [6]  [ 60/195]  eta: 0:01:06  lr: 0.063077  loss: 1.8518 (1.8664)  time: 0.4833  data: 0.0001  max mem: 127
[15:32:54.533523] Epoch: [6]  [ 80/195]  eta: 0:00:56  lr: 0.064103  loss: 1.8482 (1.8615)  time: 0.4822  data: 0.0001  max mem: 127
[15:33:04.322919] Epoch: [6]  [100/195]  eta: 0:00:46  lr: 0.065128  loss: 1.8465 (1.8591)  time: 0.4894  data: 0.0001  max mem: 127
[15:33:14.010916] Epoch: [6]  [120/195]  eta: 0:00:36  lr: 0.066154  loss: 1.8384 (1.8578)  time: 0.4844  data: 0.0001  max mem: 127
[15:33:23.665915] Epoch: [6]  [140/195]  eta: 0:00:26  lr: 0.067179  loss: 1.8339 (1.8548)  time: 0.4827  data: 0.0001  max mem: 127
[15:33:33.358168] Epoch: [6]  [160/195]  eta: 0:00:17  lr: 0.068205  loss: 1.8239 (1.8531)  time: 0.4846  data: 0.0001  max mem: 127
[15:33:43.122006] Epoch: [6]  [180/195]  eta: 0:00:07  lr: 0.069231  loss: 1.8389 (1.8512)  time: 0.4882  data: 0.0001  max mem: 127
[15:33:49.938509] Epoch: [6]  [194/195]  eta: 0:00:00  lr: 0.069949  loss: 1.8449 (1.8507)  time: 0.4870  data: 0.0001  max mem: 127
[15:33:49.984090] Epoch: [6] Total time: 0:01:35 (0.4872 s / it)
[15:33:49.984151] Averaged stats: lr: 0.069949  loss: 1.8449 (1.8507)
[15:33:18.570173] Epoch: [0]  [  0/195]  eta: 0:03:47  lr: 0.000000  loss: 2.5151 (2.5151)  time: 1.1677  data: 0.4760  max mem: 2091
[15:33:19.919451] Epoch: [0]  [ 20/195]  eta: 0:00:20  lr: 0.000001  loss: 2.4594 (2.4379)  time: 0.0674  data: 0.0001  max mem: 2159
[15:33:21.412406] Epoch: [0]  [ 40/195]  eta: 0:00:15  lr: 0.000003  loss: 1.9374 (2.2089)  time: 0.0746  data: 0.0002  max mem: 2159
[15:33:23.449590] Epoch: [0]  [ 60/195]  eta: 0:00:13  lr: 0.000004  loss: 1.4184 (1.9575)  time: 0.1018  data: 0.0002  max mem: 2159
[15:33:25.824639] Epoch: [0]  [ 80/195]  eta: 0:00:11  lr: 0.000005  loss: 1.1293 (1.7559)  time: 0.1187  data: 0.0002  max mem: 2159
[15:33:28.492540] Epoch: [0]  [100/195]  eta: 0:00:10  lr: 0.000006  loss: 0.9976 (1.6066)  time: 0.1334  data: 0.0001  max mem: 2159
[15:33:32.125689] Epoch: [0]  [120/195]  eta: 0:00:09  lr: 0.000008  loss: 0.9240 (1.4949)  time: 0.1816  data: 0.0002  max mem: 2159
[15:33:36.928470] Epoch: [0]  [140/195]  eta: 0:00:07  lr: 0.000009  loss: 0.8550 (1.4052)  time: 0.2401  data: 0.0002  max mem: 2159
[15:33:43.373572] Epoch: [0]  [160/195]  eta: 0:00:05  lr: 0.000010  loss: 0.8223 (1.3331)  time: 0.3222  data: 0.0008  max mem: 2159
[15:33:50.340044] Epoch: [0]  [180/195]  eta: 0:00:02  lr: 0.000012  loss: 0.7954 (1.2738)  time: 0.3483  data: 0.0002  max mem: 2159
[15:33:55.232332] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.000012  loss: 0.7869 (1.2387)  time: 0.3491  data: 0.0001  max mem: 2159
[15:33:55.301642] Epoch: [0] Total time: 0:00:37 (0.1944 s / it)
[15:33:55.301708] Averaged stats: lr: 0.000012  loss: 0.7869 (1.2387)
[15:33:55.461536] log_dir: ./output_dir/bmae_warmup2
Not using distributed mode
[15:33:59.584721] job dir: /home/wsj/mae
[15:33:59.584771] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=2,
ema_alpha=0.99,
enable_ema=True,
enable_bootstrap=True,
ema_warmup_epochs=2,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae_K2',
log_dir='./output_dir/bmae_K2',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:34:00.212101] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:34:00.212278] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x755e2b120bd0>
[15:34:00.734056] use bootstrapped MAE!
[15:34:00.734094] ema enabled!
[15:34:00.754542] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:34:00.754583] base lr: 5.00e-04
[15:34:00.754592] actual lr: 5.00e-04
[15:34:00.754599] accumulate grad iterations: 1
[15:34:00.754605] effective batch size: 256
[15:34:00.756293] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:34:00.756397] Start training for 200 epochs
[15:34:00.757934] log_dir: ./output_dir/bmae_K2
Not using distributed mode
[15:34:01.822973] job dir: /home/wsj/mae
[15:34:01.823031] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=4,
ema_alpha=0.99,
enable_ema=True,
enable_bootstrap=True,
ema_warmup_epochs=4,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae_K4',
log_dir='./output_dir/bmae_K4',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:34:02.462214] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:34:02.462472] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x75cd287c1450>
[15:34:03.051367] use bootstrapped MAE!
[15:34:03.051412] ema enabled!
[15:34:03.197837] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:34:03.197871] base lr: 5.00e-04
[15:34:03.197877] actual lr: 5.00e-04
[15:34:03.197882] accumulate grad iterations: 1
[15:34:03.197886] effective batch size: 256
[15:34:03.199142] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:34:03.199245] Start training for 200 epochs
[15:34:03.200367] log_dir: ./output_dir/bmae_K4
Not using distributed mode
[15:34:04.191565] job dir: /home/wsj/mae
[15:34:04.191622] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=8,
ema_alpha=0.99,
enable_ema=True,
enable_bootstrap=True,
ema_warmup_epochs=8,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae_K8',
log_dir='./output_dir/bmae_K8',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:34:04.839729] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:34:04.839910] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x72828cd1d310>
[15:34:05.745483] use bootstrapped MAE!
[15:34:05.745517] ema enabled!
[15:34:06.382360] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:34:06.382411] base lr: 5.00e-04
[15:34:06.382418] actual lr: 5.00e-04
[15:34:06.382422] accumulate grad iterations: 1
[15:34:06.382427] effective batch size: 256
[15:34:06.383955] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:34:06.384070] Start training for 200 epochs
[15:34:06.385335] log_dir: ./output_dir/bmae_K8
[15:33:50.722379] Test:  [ 0/40]  eta: 0:00:28  loss: 1.7681 (1.7681)  acc1: 37.5000 (37.5000)  acc5: 88.6719 (88.6719)  time: 0.7001  data: 0.2391  max mem: 127
[15:33:55.417672] Test:  [10/40]  eta: 0:00:14  loss: 1.7689 (1.7607)  acc1: 38.2812 (40.2699)  acc5: 89.0625 (88.8849)  time: 0.4904  data: 0.0219  max mem: 127
[15:34:00.094163] Test:  [20/40]  eta: 0:00:09  loss: 1.7647 (1.7494)  acc1: 40.2344 (40.7180)  acc5: 89.0625 (89.2857)  time: 0.4685  data: 0.0002  max mem: 127
[15:34:04.789461] Test:  [30/40]  eta: 0:00:04  loss: 1.7474 (1.7506)  acc1: 40.6250 (40.4108)  acc5: 88.6719 (89.0247)  time: 0.4685  data: 0.0001  max mem: 127
[15:34:08.588584] Test:  [39/40]  eta: 0:00:00  loss: 1.7446 (1.7529)  acc1: 39.8438 (40.2700)  acc5: 88.6719 (88.9600)  time: 0.4481  data: 0.0001  max mem: 127
[15:34:08.671193] Test: Total time: 0:00:18 (0.4662 s / it)
[15:34:08.671255] * Acc@1 40.270 Acc@5 88.960 loss 1.753
[15:34:08.671389] Accuracy of the network on the 10000 test images: 40.3%
[15:34:08.671399] Max accuracy: 40.27%
[15:34:08.672927] log_dir: ./output_dir/mae_mk0.3/linear
Not using distributed mode
[15:34:06.051246] job dir: /home/wsj/mae
[15:34:06.051303] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=8,
ema_alpha=0.99,
enable_ema=True,
enable_bootstrap=True,
ema_warmup_epochs=8,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae_K8',
log_dir='./output_dir/bmae_K8',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:34:06.774953] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:34:06.775161] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7d6d791b7290>
[15:34:07.809993] use bootstrapped MAE!
[15:34:07.810042] ema enabled!
[15:34:08.703469] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:34:08.703513] base lr: 5.00e-04
[15:34:08.703520] actual lr: 5.00e-04
[15:34:08.703525] accumulate grad iterations: 1
[15:34:08.703529] effective batch size: 256
[15:34:08.705014] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:34:08.705152] Start training for 200 epochs
[15:34:08.706416] log_dir: ./output_dir/bmae_K8
[15:33:21.993914] Epoch: [0]  [  0/195]  eta: 0:04:26  lr: 0.000000  loss: 2.5151 (2.5151)  time: 1.3679  data: 0.3415  max mem: 2091
[15:33:24.411735] Epoch: [0]  [ 20/195]  eta: 0:00:31  lr: 0.000001  loss: 2.4594 (2.4379)  time: 0.1208  data: 0.0002  max mem: 2159
[15:33:26.806852] Epoch: [0]  [ 40/195]  eta: 0:00:23  lr: 0.000003  loss: 1.9374 (2.2089)  time: 0.1197  data: 0.0002  max mem: 2159
[15:33:29.914717] Epoch: [0]  [ 60/195]  eta: 0:00:20  lr: 0.000004  loss: 1.4184 (1.9575)  time: 0.1553  data: 0.0002  max mem: 2159
[15:33:33.907131] Epoch: [0]  [ 80/195]  eta: 0:00:18  lr: 0.000005  loss: 1.1293 (1.7559)  time: 0.1996  data: 0.0002  max mem: 2159
[15:33:39.273673] Epoch: [0]  [100/195]  eta: 0:00:17  lr: 0.000006  loss: 0.9976 (1.6066)  time: 0.2683  data: 0.0002  max mem: 2159
[15:33:46.155358] Epoch: [0]  [120/195]  eta: 0:00:15  lr: 0.000008  loss: 0.9240 (1.4949)  time: 0.3440  data: 0.0002  max mem: 2159
[15:33:53.132148] Epoch: [0]  [140/195]  eta: 0:00:12  lr: 0.000009  loss: 0.8550 (1.4052)  time: 0.3488  data: 0.0002  max mem: 2159
[15:34:00.042223] Epoch: [0]  [160/195]  eta: 0:00:08  lr: 0.000010  loss: 0.8223 (1.3331)  time: 0.3455  data: 0.0002  max mem: 2159
[15:34:07.082429] Epoch: [0]  [180/195]  eta: 0:00:03  lr: 0.000012  loss: 0.7954 (1.2738)  time: 0.3520  data: 0.0002  max mem: 2159
[15:34:11.922686] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.000012  loss: 0.7869 (1.2387)  time: 0.3470  data: 0.0001  max mem: 2159
[15:34:11.961506] Epoch: [0] Total time: 0:00:51 (0.2633 s / it)
[15:34:11.961572] Averaged stats: lr: 0.000012  loss: 0.7869 (1.2387)
[15:34:12.176235] log_dir: ./output_dir/bmae_warmup4
Not using distributed mode
[15:34:08.627270] job dir: /home/wsj/mae
[15:34:08.627331] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=16,
ema_alpha=0.99,
enable_ema=True,
enable_bootstrap=True,
ema_warmup_epochs=16,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae_K16',
log_dir='./output_dir/bmae_K16',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:34:09.430096] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:34:09.430363] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x74a2ad8bab50>
[15:34:10.964277] use bootstrapped MAE!
[15:34:10.964309] ema enabled!
[15:34:12.515363] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:34:12.515425] base lr: 5.00e-04
[15:34:12.515434] actual lr: 5.00e-04
[15:34:12.515441] accumulate grad iterations: 1
[15:34:12.515447] effective batch size: 256
[15:34:12.517033] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:34:12.517159] Start training for 200 epochs
[15:34:12.518537] log_dir: ./output_dir/bmae_K16
Not using distributed mode
[15:34:11.085255] job dir: /home/wsj/mae
[15:34:11.085309] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=32,
ema_alpha=0.99,
enable_ema=True,
enable_bootstrap=True,
ema_warmup_epochs=32,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae_K32',
log_dir='./output_dir/bmae_K32',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:34:11.729808] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:34:11.730008] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x71d25e3016d0>
[15:34:12.987226] use bootstrapped MAE!
[15:34:12.987286] ema enabled!
[15:34:14.590775] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:34:14.590815] base lr: 5.00e-04
[15:34:14.590822] actual lr: 5.00e-04
[15:34:14.590826] accumulate grad iterations: 1
[15:34:14.590830] effective batch size: 256
[15:34:14.592257] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:34:14.592368] Start training for 200 epochs
[15:34:14.593648] log_dir: ./output_dir/bmae_K32
[15:32:43.919140] Epoch: [6]  [  0/195]  eta: 0:02:24  lr: 0.060000  loss: 1.7609 (1.7609)  time: 0.7409  data: 0.2599  max mem: 127
[15:32:53.599038] Epoch: [6]  [ 20/195]  eta: 0:01:26  lr: 0.061026  loss: 1.7863 (1.7939)  time: 0.4840  data: 0.0001  max mem: 127
[15:33:03.393520] Epoch: [6]  [ 40/195]  eta: 0:01:16  lr: 0.062051  loss: 1.7767 (1.7884)  time: 0.4897  data: 0.0001  max mem: 127
[15:33:13.112173] Epoch: [6]  [ 60/195]  eta: 0:01:06  lr: 0.063077  loss: 1.7776 (1.7837)  time: 0.4859  data: 0.0001  max mem: 127
[15:33:22.729104] Epoch: [6]  [ 80/195]  eta: 0:00:56  lr: 0.064103  loss: 1.7639 (1.7779)  time: 0.4808  data: 0.0001  max mem: 127
[15:33:32.430984] Epoch: [6]  [100/195]  eta: 0:00:46  lr: 0.065128  loss: 1.7657 (1.7751)  time: 0.4850  data: 0.0001  max mem: 127
[15:33:42.136495] Epoch: [6]  [120/195]  eta: 0:00:36  lr: 0.066154  loss: 1.7522 (1.7724)  time: 0.4852  data: 0.0001  max mem: 127
[15:33:51.779211] Epoch: [6]  [140/195]  eta: 0:00:26  lr: 0.067179  loss: 1.7335 (1.7685)  time: 0.4821  data: 0.0001  max mem: 127
[15:34:01.591759] Epoch: [6]  [160/195]  eta: 0:00:17  lr: 0.068205  loss: 1.7415 (1.7661)  time: 0.4906  data: 0.0007  max mem: 127
[15:34:11.010025] Epoch: [6]  [180/195]  eta: 0:00:07  lr: 0.069231  loss: 1.7504 (1.7640)  time: 0.4709  data: 0.0002  max mem: 127
[15:34:17.836025] Epoch: [6]  [194/195]  eta: 0:00:00  lr: 0.069949  loss: 1.7456 (1.7631)  time: 0.4705  data: 0.0001  max mem: 127
[15:34:17.886367] Epoch: [6] Total time: 0:01:34 (0.4857 s / it)
[15:34:17.886449] Averaged stats: lr: 0.069949  loss: 1.7456 (1.7631)
Not using distributed mode
[15:34:14.993838] job dir: /home/wsj/mae
[15:34:14.993920] Namespace(batch_size=256,
epochs=200,
accum_iter=1,
model='deit_tiny',
input_size=32,
mask_ratio=0.6,
norm_pix_loss=True,
bmae_k=64,
ema_alpha=0.99,
enable_ema=True,
enable_bootstrap=True,
ema_warmup_epochs=64,
weight_decay=0.05,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=40,
data_path='./data',
output_dir='./output_dir/bmae_K64',
log_dir='./output_dir/bmae_K64',
device='cuda',
seed=0,
resume='',
start_epoch=0,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
[15:34:15.650939] Dataset CIFAR10
    Number of datapoints: 50000
    Root location: ./data
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(32, 32), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:34:15.651120] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7eeb83e10dd0>
[15:34:18.683788] use bootstrapped MAE!
[15:34:18.683832] ema enabled!
[15:34:20.950490] Model = BootstrappedMaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
  )
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
  (decoder_blocks): ModuleList(
    (0-7): 8 x Block(
      (norm1): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=192, out_features=576, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=768, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)
  (decoder_pred): Linear(in_features=192, out_features=48, bias=True)
  (decoder_last_proj): Linear(in_features=192, out_features=192, bias=True)
)
[15:34:20.950544] base lr: 5.00e-04
[15:34:20.950551] actual lr: 5.00e-04
[15:34:20.950556] accumulate grad iterations: 1
[15:34:20.950560] effective batch size: 256
[15:34:20.952324] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
[15:34:20.952502] Start training for 200 epochs
[15:34:20.953967] log_dir: ./output_dir/bmae_K64
[15:33:28.222327] Epoch: [0]  [  0/195]  eta: 0:04:56  lr: 0.000000  loss: 2.5151 (2.5151)  time: 1.5201  data: 0.3773  max mem: 2091
[15:33:31.739604] Epoch: [0]  [ 20/195]  eta: 0:00:41  lr: 0.000001  loss: 2.4594 (2.4379)  time: 0.1758  data: 0.0002  max mem: 2159
[15:33:36.393414] Epoch: [0]  [ 40/195]  eta: 0:00:36  lr: 0.000003  loss: 1.9374 (2.2089)  time: 0.2326  data: 0.0002  max mem: 2159
[15:33:42.688062] Epoch: [0]  [ 60/195]  eta: 0:00:35  lr: 0.000004  loss: 1.4184 (1.9575)  time: 0.3147  data: 0.0002  max mem: 2159
[15:33:49.684054] Epoch: [0]  [ 80/195]  eta: 0:00:32  lr: 0.000005  loss: 1.1293 (1.7559)  time: 0.3498  data: 0.0002  max mem: 2159
[15:33:56.555533] Epoch: [0]  [100/195]  eta: 0:00:28  lr: 0.000006  loss: 0.9976 (1.6066)  time: 0.3435  data: 0.0002  max mem: 2159
[15:34:03.564653] Epoch: [0]  [120/195]  eta: 0:00:22  lr: 0.000008  loss: 0.9240 (1.4949)  time: 0.3504  data: 0.0002  max mem: 2159
[15:34:10.653524] Epoch: [0]  [140/195]  eta: 0:00:17  lr: 0.000009  loss: 0.8550 (1.4052)  time: 0.3544  data: 0.0003  max mem: 2159
[15:34:17.550201] Epoch: [0]  [160/195]  eta: 0:00:11  lr: 0.000010  loss: 0.8223 (1.3331)  time: 0.3448  data: 0.0003  max mem: 2159
[15:34:24.669542] Epoch: [0]  [180/195]  eta: 0:00:04  lr: 0.000012  loss: 0.7954 (1.2738)  time: 0.3559  data: 0.0010  max mem: 2159
[15:34:29.552661] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.000012  loss: 0.7869 (1.2387)  time: 0.3486  data: 0.0001  max mem: 2159
[15:34:29.596173] Epoch: [0] Total time: 0:01:02 (0.3225 s / it)
[15:34:29.596244] Averaged stats: lr: 0.000012  loss: 0.7869 (1.2387)
[15:34:29.769849] log_dir: ./output_dir/bmae_warmup8
[15:34:18.925064] Test:  [ 0/40]  eta: 0:00:39  loss: 1.6481 (1.6481)  acc1: 43.7500 (43.7500)  acc5: 92.5781 (92.5781)  time: 0.9790  data: 0.5190  max mem: 127
[15:34:23.591543] Test:  [10/40]  eta: 0:00:15  loss: 1.6534 (1.6515)  acc1: 41.4062 (42.6136)  acc5: 92.1875 (91.0511)  time: 0.5132  data: 0.0481  max mem: 127
[15:34:28.289176] Test:  [20/40]  eta: 0:00:09  loss: 1.6344 (1.6392)  acc1: 42.1875 (43.7128)  acc5: 91.7969 (91.2388)  time: 0.4681  data: 0.0006  max mem: 127
[15:34:32.965831] Test:  [30/40]  eta: 0:00:04  loss: 1.6323 (1.6418)  acc1: 44.1406 (43.7374)  acc5: 90.2344 (90.9904)  time: 0.4686  data: 0.0002  max mem: 127
[15:34:36.791322] Test:  [39/40]  eta: 0:00:00  loss: 1.6472 (1.6450)  acc1: 42.1875 (43.2800)  acc5: 90.2344 (90.9200)  time: 0.4485  data: 0.0001  max mem: 127
[15:34:36.833772] Test: Total time: 0:00:18 (0.4722 s / it)
[15:34:36.833824] * Acc@1 43.280 Acc@5 90.920 loss 1.645
[15:34:36.833946] Accuracy of the network on the 10000 test images: 43.3%
[15:34:36.833953] Max accuracy: 43.28%
[15:34:36.835492] log_dir: ./output_dir/mae_mk0.5/linear
[15:33:32.691890] Epoch: [0]  [  0/195]  eta: 0:06:52  lr: 0.000000  loss: 2.5151 (2.5151)  time: 2.1179  data: 0.3600  max mem: 2091
[15:33:37.651994] Epoch: [0]  [ 20/195]  eta: 0:00:58  lr: 0.000001  loss: 2.4594 (2.4379)  time: 0.2480  data: 0.0002  max mem: 2159
[15:33:44.289136] Epoch: [0]  [ 40/195]  eta: 0:00:51  lr: 0.000003  loss: 1.9374 (2.2089)  time: 0.3318  data: 0.0002  max mem: 2159
[15:33:51.277970] Epoch: [0]  [ 60/195]  eta: 0:00:45  lr: 0.000004  loss: 1.4184 (1.9575)  time: 0.3494  data: 0.0002  max mem: 2159
[15:33:58.190648] Epoch: [0]  [ 80/195]  eta: 0:00:39  lr: 0.000005  loss: 1.1293 (1.7559)  time: 0.3456  data: 0.0002  max mem: 2159
[15:34:05.201054] Epoch: [0]  [100/195]  eta: 0:00:32  lr: 0.000006  loss: 0.9976 (1.6066)  time: 0.3505  data: 0.0004  max mem: 2159
[15:34:12.144239] Epoch: [0]  [120/195]  eta: 0:00:25  lr: 0.000008  loss: 0.9240 (1.4949)  time: 0.3471  data: 0.0003  max mem: 2159
[15:34:19.217976] Epoch: [0]  [140/195]  eta: 0:00:18  lr: 0.000009  loss: 0.8550 (1.4052)  time: 0.3536  data: 0.0003  max mem: 2159
[15:34:26.271863] Epoch: [0]  [160/195]  eta: 0:00:12  lr: 0.000010  loss: 0.8223 (1.3331)  time: 0.3526  data: 0.0002  max mem: 2159
[15:34:33.376395] Epoch: [0]  [180/195]  eta: 0:00:05  lr: 0.000012  loss: 0.7954 (1.2738)  time: 0.3552  data: 0.0003  max mem: 2159
[15:34:38.355734] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.000012  loss: 0.7869 (1.2387)  time: 0.3573  data: 0.0002  max mem: 2159
[15:34:38.403376] Epoch: [0] Total time: 0:01:07 (0.3478 s / it)
[15:34:38.403466] Averaged stats: lr: 0.000012  loss: 0.7869 (1.2387)
[15:34:38.593517] log_dir: ./output_dir/bmae_warmup16
[15:33:07.314056] Epoch: [6]  [  0/195]  eta: 0:02:21  lr: 0.060000  loss: 1.6691 (1.6691)  time: 0.7237  data: 0.2318  max mem: 127
[15:33:16.999147] Epoch: [6]  [ 20/195]  eta: 0:01:26  lr: 0.061026  loss: 1.7011 (1.7125)  time: 0.4842  data: 0.0001  max mem: 127
[15:33:26.757536] Epoch: [6]  [ 40/195]  eta: 0:01:16  lr: 0.062051  loss: 1.6911 (1.7057)  time: 0.4879  data: 0.0007  max mem: 127
[15:33:36.492820] Epoch: [6]  [ 60/195]  eta: 0:01:06  lr: 0.063077  loss: 1.7052 (1.7036)  time: 0.4867  data: 0.0001  max mem: 127
[15:33:46.204395] Epoch: [6]  [ 80/195]  eta: 0:00:56  lr: 0.064103  loss: 1.6761 (1.6986)  time: 0.4855  data: 0.0001  max mem: 127
[15:33:55.888829] Epoch: [6]  [100/195]  eta: 0:00:46  lr: 0.065128  loss: 1.6827 (1.6957)  time: 0.4842  data: 0.0001  max mem: 127
[15:34:05.705354] Epoch: [6]  [120/195]  eta: 0:00:36  lr: 0.066154  loss: 1.6705 (1.6931)  time: 0.4908  data: 0.0001  max mem: 127
[15:34:15.437976] Epoch: [6]  [140/195]  eta: 0:00:26  lr: 0.067179  loss: 1.6544 (1.6889)  time: 0.4866  data: 0.0009  max mem: 127
[15:34:25.021953] Epoch: [6]  [160/195]  eta: 0:00:17  lr: 0.068205  loss: 1.6705 (1.6857)  time: 0.4792  data: 0.0001  max mem: 127
[15:34:34.837076] Epoch: [6]  [180/195]  eta: 0:00:07  lr: 0.069231  loss: 1.6637 (1.6828)  time: 0.4907  data: 0.0001  max mem: 127
[15:34:41.573168] Epoch: [6]  [194/195]  eta: 0:00:00  lr: 0.069949  loss: 1.6413 (1.6806)  time: 0.4836  data: 0.0001  max mem: 127
[15:34:41.619232] Epoch: [6] Total time: 0:01:35 (0.4873 s / it)
[15:34:41.619294] Averaged stats: lr: 0.069949  loss: 1.6413 (1.6806)
